%!TEX root = ../Thesis_example_compile_this.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Network Implementation}
\label{ch:04implement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter presents the implementation details of the \gls{NMR} introduced in Chapter \ref{ch:03framework}. The network is the first component of \gls{NMR} simulation workflow and operates in conjunction with \gls{SGRI}, introduced in Chapter \ref{ch:05impute}. Inference of network parameters, $\theta$, is an inverse problem as only the true data values are known. Therefore, a mapping function, $\mathcal{F}_{\theta}$, is required to map the unknown latent space to the known observed space. The function is parameterized such that a pool of unconditional, latent Gaussian factors reproduce the known values and have the desired spatial features when mapped with $\mathcal{F}_{\theta}$. As the latent space is unknown and ultimately a synthetic feature of the \gls{NMR} model, it is free to contain any number of components with any covariance structure. This flexibility permits creativity with latent factor design; combining latent covariance structures in unique ways allows a mixture of univariate Gaussian distributions to possess non-multivariate Gaussian spatial features. Practical implementation details include considerations for the design of the network and the latent Gaussian pool, as well as sensitivities associated with these model parameters. A synthetic example demonstrates latent factor design, network parameter inference and non-uniqueness properties of the \gls{NMR}. This example highlights the network component of the full \gls{NMR} workflow and is carried forward into Chapter \ref{ch:05impute} to illustrate the second imputation component.

\FloatBarrier
\section{Network Design}
\label{sec:04networkd}

The \gls{NMR} is not a true neural network but rather a model of regionalization inspired by neural network structure. The network consists of an input layer where the number of latent factors determines the number of input nodes, a single ``hidden layer'' of the same dimension, and an output layer with a single node. The single output node makes the \gls{NMR} univariate (analogous to the \gls{LMR}); however, there is no reason it could not be extended to the multivariate case (analogous to the \gls{LMC}). However, with geospatial data, one commonly considers extreme values in the univariate context; it is not immediately clear what constitutes a multivariate extreme.

\subsection{Architecture}
\label{subsec:04arch}

The number of latent factors in the Gaussian pool controls network architecture. The current \gls{NMR} implementation restricts the network architecture to an input layer, a single hidden layer and a univariate output layer. The input and hidden layer both contain $M+1$ nodes. Figure \ref{fig:example_nmr} shows an example network configuration with $M=3$ input latent factors, $\{ Y_{0}, \dots, Y_{3}\}$, where the nugget effect is the $0^{th}$ factor by convention. The network's hidden layer is a transformation layer, transforming the latent Gaussian values to arbitrary activation units. The network activation function $\phi(\dots)$ is a \gls{MPL} function; the following subsection presents the details. The network output $Z_{1}$ is a weighted linear combination of the activated latent factors.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, trim={0 1.5cm 0 1.5cm}, clip=true]{./0-Figures/04-Ch4/example_nmr.png}
    \caption{Schematic representation of the \gls{NMR} with $M=3$ latent factors. $\phi(\dots)$ is the \gls{MPL} activation function. }
    \label{fig:example_nmr}
\end{figure}

The following operators define the general forward pass through the network:
\begin{align}
    \label{eq:fpass1}
    \mathbf{x} & = \sum_{m=0}^{M}a_{m} \cdot \phi(\mathbf{y}_{m}, \omega_{m}) \\
    \label{eq:fpass2}
    \mathbf{z} & = G^{-1}\left( F_{X}\left(\mathbf{x} \right)\right)
\end{align}

Where $\mathbf{x}$ is an intermediary activation vector, $a_{m}$ is a weight applied to factor $m$, $\phi(\mathbf{y}_{m}, \omega_{m})$ is the \gls{MPL} activation function with exponent $\omega$ applied to factor $m$, $F_{X}$ is the \gls{CDF} of $\mathbf{x}$ and $G^{-1}$ is the inverse of the Gaussian \gls{CDF}. The weights $\{ a_{0}, \dots, a_{M}\}$ are constrained to be greater than or equal to zero and reflect the relative importance of each factor in the mapping function. Alternatively, Equation \ref{eq:fpass1} can be rewritten in matrix notation as:
\begin{equation}
    \mathbf{x} = \phi\left( \mathbf{Y} \right) \mathbf{a}^{T}
    \label{eq:fpass3}
\end{equation}

Where $\mathbf{a}$ is a row vector of factor weights and $\phi\left(\mathbf{Y} \right)$ is an $ndata$ x $M+1$ matrix of activated latent factors. The raw output of the network is activation units that must be transformed to Gaussian space, necessitating the normal score transform of $\mathbf{x}$ to $\mathbf{z}$. With the given architecture, the \gls{NMR} requires inference of $2 \cdot (M+1)$ parameters: a $1$ x $M+1$ dimensional vector of factor weights and a $1$ x $M+1$ dimensional vector of \gls{MPL} exponents $\omega$. These $2 \cdot (M+1)$ network parameters are inferred through stochastic optimization and discussed in Section \ref{sec:04paraminfer}. The univariate normal score transform from activation units to Gaussian units in Equation \ref{eq:fpass2} requires a representative distribution unique to the network's given state. Each iteration of the stochastic optimization algorithm generates an updated parameter vector, resulting in a new mapping. The iteration-specific reference distribution is created by drawing a $1e^{4}$ x $M+1$ dimensional matrix of independent, standard normal Gaussian values and mapping them to a $1e^{4}$ x $1$ vector of activation values. These activation values are normal score transformed, resulting in a temporary, iteration-specific transformation table. Using this transformation table, the linear interpolation of activation values is the $G^{-1}$ operator in Equation \ref{eq:fpass2}.

\subsection{Activation Function}
\label{subsec:04activation}

The goal of the \gls{NMR} is to parameterize the arbitrary mapping function, $\mathcal{F}_{\theta}$, between the latent and observed spaces. The form of this function is not obvious; hence, a network-based approach is used as a function approximation. Given the complex spatial features we wish to capture in the final models, a polynomial of degree greater than one is useful. Suppose the activation function is linear. As the forward pass through the network is a weighted, linear combination of the inputs, a linear activation function (or simply $c \cdot \mathbf{y}$ where $c$ is a constant) results in a linear output or single-order polynomial \citep{sharma2020activation}. To achieve a non-linear network output, one must introduce a non-linearity as an activation function. Real data commonly contain non-linearly separable features, and a non-linear activation function permits the projection of these features onto a non-linear feature space \citep{dubey2022activation}.

In the traditional \gls{ML} context, the constraint of differentiability is placed on neural network activation functions due to using the back-propagation algorithm \citep{rojas1996backpropagation}. The \gls{NMR} structure is only inspired by a neural network, and is not subject to this same constraint. \Gls{NMR} parameters are ``learned'' through gradient-free stochastic optimization. This gradient-free approach, and the strength of the \gls{NMR}, permits using virtually any activation function, differentiable or not. The \gls{NMR} activation function is a power-law function with the form:
\begin{equation}
    \phi \left( \mathbf{y}, \omega \right) =
    \begin{cases}
        \mathbf{y}^{\omega},           & \text{if $\mathbf{y} > 0$} \\
        \mathbf{y}^{\frac{1}{\omega}}, & \text{if $\mathbf{y} < 0$} \\
    \end{cases}
    \label{eq:power}
\end{equation}


Where $\omega$ is a trainable parameter. The magnitude of $\omega$ allows the activation function to emphasize certain regions of the latent distribution. If $\omega = 1$, the activation is linear and $\phi \left( \mathbf{y}, 1 \right)=\mathbf{y}$. If $\omega$ is less than one, the function takes on a concave shape that emphasizes low values and mutes the influence of high values. When $\omega$ is greater than one, the opposite is true. The function takes on a convex shape that emphasizes high values and mutes the influence of low values. Figure \ref{fig:power_activation} (left) shows the relationship between $\phi \left( \mathbf{y}, \omega \right)$ and $\mathbf{y}$ for various values of $\omega$. As the magnitude of $\omega$ increases, the activation function becomes steeper above zero and flatter below zero. The high values' magnitude increases exponentially, and low values are muted significantly. This non-linear amplification allows the network to embed high-grade features of latent factors in the mapping function $\mathcal{F}_{\theta}$. Low-grade features are embedded oppositely.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/power_activation.png}
    \caption{The \gls{MPL} (left) and scaled \gls{MPL} (right) activation function for various values of $\omega$ and input $\mathbf{y} \in [-5,5]$. The scaled activation uses $\xi = G^{-1}(0.999) \approx 3.09$. }
    \label{fig:power_activation}
\end{figure}

Notably, the \gls{MPL} activation has three inflection points: at -1, 0, and 1. The inflections exist as $\pm 1^{\omega} = \pm 1$ and $0^{\omega} = 0$, regardless of $\omega$. No negative effects have been observed related to these inflections; however, the points at -1 and 1 may be adjusted by introducing a scaling factor, $\xi$, to the \gls{MPL} activation function:
\begin{equation}
    \phi \left( \mathbf{y}, \omega, \xi \right) =
    \begin{cases}
        \xi \cdot \left(\frac{\mathbf{y}}{\xi}\right)^{\omega},           & \text{if $\mathbf{y} > 0$} \\
        \xi \cdot \left(\frac{\mathbf{y}}{\xi}\right)^{\frac{1}{\omega}}, & \text{if $\mathbf{y} < 0$} \\
    \end{cases}
    \label{eq:power_scale}
\end{equation}

The scaling parameter $\xi$ allows setting a predetermined inflection point while retaining the linear nature of the function when $\omega = 1$. Figure \ref{fig:power_activation} (right) shows the scaled \gls{MPL} activation function where $\xi = G^{-1}(0.999) \approx 3.09$ and $G^{-1}$ is the inverse of the Gaussian \gls{CDF}. $\xi$ effectively controls the threshold for dampening or emphasizing a factor. When $\omega > 1$ and $0 < y < \xi$, the \gls{MPL} reduces the value of $y$ slightly, similar to an opportunity-seeking risk perspective \citep{eidsvik2015value}, saying ``we are more interested in values of $y > \xi$.'' When $\omega < 1$ and $0 < y < \xi$, the \gls{MPL} increases the value of $y$ slightly, similar to a risk-averse perspective, saying ``we are more interested in values of $y < \xi$.'' The opposite relationships hold when $y < 0$. It is not immediately clear when the \gls{MPL} should be scaled; however, if the practitioner notices artifacts related to the activation function, $\xi$ can be tuned. $\xi$ could be introduced as a trainable parameter. The range of $\omega$ for each latent factor is an important component of latent factor design, discussed in Section \ref{sec:04factord}. Constraining $\omega_{m} > 1$ embeds higher-grade features of the $m^{th}$ factor in the mapping. Conversely, constraining $\omega_{m} < 1$ embeds lower-grade features of the $m^{th}$ factor in the mapping. Values of $\omega$ are practically within $[0.25, 4.0]$ and reflect the relative influence of each latent factor's high and low values.

% \begin{enumerate}[noitemsep]
%     \item Architecture
%     \item Gaussian transform
%     \item Activation function
%     \item Recall kriging is linear, but results can be highly non-linear
% \end{enumerate}

\FloatBarrier
\section{Latent Factor Design}
\label{sec:04factord}

Latent factor design is critical to the workflow of the \gls{NMR}. The latent factors form the Gaussian ``pool'' from which the \gls{NMR} draws structure. The univariate Gaussian factors are mixed to generate a spatial distribution that is univariate Gaussian but not multivariate Gaussian. The mixture distribution can only contain features, or a combination of features, present in the Gaussian pool. Therefore, any feature required in the final model must be in the pool. The Gaussian pool is similar to the concept of factorial kriging \citep{goovaerts1997geostatistics}, where the nested variogram model is a composition of $L$ basic variogram structures, and the regionalized variable is a composition of $L$ independent, standard normal, spatial components operating at different geologic scales:
\begin{align}
    \label{eq:gamma_comp}
    \gamma(\mathbf{h}) & = \sum_{\ell=1}^{L} \gamma_{\ell}(\mathbf{h})                     \\
    \label{eq:z_comp}
    Z(\mathbf{u})      & = \sum_{\ell=1}^{L}b_{\ell}Y_{\ell}(\mathbf{u}) + \mu(\mathbf{u})
\end{align}

Where the coefficient $b_{\ell}$ is the square root of the variance contribution of $\gamma_{\ell}$. The key difference between the \gls{NMR} and factorial kriging approaches is that the regionalized \gls{NMR} variable is a weighted, linear combination of non-linear, independent spatial components as Equation \ref{eq:fpass1} highlights. With the \gls{LMC} or factorial kriging, the $Y_{\ell}$ independent factors are synthetic features of the model and are not directly observed; only characterized by $\gamma_{\ell}$. The \gls{NMR} requires explicit definition of a pool of independent Gaussian factors. The pool may contain any structures. It is simple to generate realizations of the independent Gaussian factors with any unconditional simulation algorithm such as \gls{SGS} \citep{gomez-hernandez1993joint} or LU simulation \citep{davis1987production}.

The choice of covariance structures for the Gaussian pool must consider the end goal of the spatial mixture. The latent factors must be reasonable in the sense that achieving the objective is possible. For example, if the final goal is short-range features, the pool must contain short-range features. Long-range and short-range structures can produce a final mixture with medium-range features; however, a pool with only long-range structures cannot. Pool considerations include (1) the conceptual geological model, (2) the $L$ nested components of the normal score variogram, (3) the $L$ nested components of each indicator variogram model and their potential asymmetry, (4) the downhole connectivity measures from the observed data and the potential connectivity of extreme values, and (5) the composition of the objective function, discussed in more detail in Section \ref{sec:04paraminfer}. The factorial kriging concept provides a reasonable starting point for the design of the Gaussian pool. An initial pool can be inferred by decomposing all variogram models into their basic components (as in Equation \ref{eq:gamma_comp}). Each basic variogram component is a single licit structure with three orientation parameters, three range parameters, a nugget of zero and a sill of one. By convention, a pure nugget latent factor is added as the $0^{th}$ factor.

Consider a small \gls{2D} synthetic example where the goal of the \gls{NMR} is to generate gridded realizations with strongly asymmetric 0.1 and 0.9 quantile indicator variograms. That is, the low-grade two-point spatial continuity differs drastically from the high-grade. This scenario is challenging for multivariate Gaussian simulation algorithms and has been discussed at length by many practitioners \citep{journel1983nonparametric, gomez-hernandez1998be, journel1993entropy,renard2011conditioning, guthke2013non}. The maximum entropy characteristic of the multivariate Gaussian distribution tends towards disconnected extremes and maximum connectivity of intermediate values. The destructuring of indicators away from the median is symmetric in the multivariate Gaussian case. The \gls{NMR} framework can overcome this challenge using a well-designed Gaussian pool and $\omega$ bounds. Consider two factors with the goal of asymmetric indicator variograms. Factor 1 is a highly anisotropic factor, oriented in a north-south direction, and factor 2 is isotropic with a range off $\approx \frac{1}{2}$ the domain size. The factors are activated using the \gls{MPL} with $\omega_{1}=4$ emphasizing the high values and $\omega_{2}=1/\omega_{1}=0.25$ emphasizing the low values, followed by linear combination (Equation \ref{eq:fpass1}) and normal score transform. The top row of Figure \ref{fig:pool_asymmetry} shows the factors in activated units (left and center) and the normal score transform of the mixture (right). The mixture model is univariate Gaussian but not multivariate Gaussian. Note the difference in activation units between the factors. The bottom row shows the 0.1 (black) and 0.9 (red) indicator variograms of the final \gls{NMR} mixture in the north-south and east-west directions. The longer range, more-isotropic, low-grade continuity is preserved through factor 2 and $\omega < 1$, and the highly-anisotropic, higher-grade continuity is preserved through factor 1 and $\omega > 1$. This small example is illustrative of three key concepts concerning latent factor design:
\begin{enumerate}[noitemsep]
    \item Latent factors should be designed in conjunction with $\omega$ constraints, targeting continuity in specific grade ranges and the final model goals.
    \item The ranges of factors are reduced through mixing. Mixing of factors cannot increase continuity beyond the longest-range structures in the pool. Factor 2 in Figure \ref{fig:pool_asymmetry} (top row, center) has an isotropic variogram range of 64 meters. The 0.1 quantile indicator variogram range is reduced to roughly 40 and 25 meters in the north-south and east-west directions, respectively.
    \item Anisotropy ratios of factors are affected through mixing. The anisotropy ratio of factor 1 is roughly unchanged in Figure \ref{fig:pool_asymmetry} (top row, left); however, the anisotropy ratio of factor two is increased from 1:1 to $\approx$ 1.6:1. In general, isotropic factors will tend to become more anisotropic, and anisotropic factors will become more isotropic.
\end{enumerate}

\begin{figure}[!htb]
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/f1_f2_grid.png}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/asymmetric_ind_vario.png}
    \end{subfigure}
    \caption{The top row shows factor 1 activated with $\omega_{1}=4$ (left), factor 2 activated with $\omega_{2}=1/\omega_{1}$ (center), and the normal score transform the mixture (right). The bottom row shows the gridded indicator variograms for the 0.1 quantile (black) and 0.9 quantile (red) in the north-south (left) and east-west (right) directions. }
    \label{fig:pool_asymmetry}
\end{figure}

The Gaussian pool must be reasonable concerning the optimization targets discussed in Section \ref{sec:04paraminfer}. The inferred mapping function is non-unique, so there is no ``correct'' Gaussian pool; however, some latent factor combinations may be unreasonable. For example, one cannot expect a Gaussian pool of long-range structures to generate short-range features in the \gls{NMR} model, and vice-versa. This is similarly true for factor orientations. If the final model requires spatial features in certain orientations, factor design should address this requirement. Factors aimed at generating high-order connectivity features should consider the major orientation of drilling. Factors with different orientations may be combined; for example, a north-south and an east-west factor can generate north-east striking features in the final model. This base-case requirement of ``reasonableness'' motivates variogram decomposition as the initial approach to latent factor design. There is no definitive recipe for latent factor design, though the following rules of thumb apply:
\begin{enumerate}[noitemsep]
    \item The Decomposition of nested variogram models into their basic components is a reasonable starting point.
    \item Checking objective component reproduction with a traditional simulation algorithm (such as \gls{SGS}) can provide insight into the ``missing'' factors to be added.
    \item Add factors based on the conceptual geologic model and end goals of the \gls{NMR} model.
    \item Prune any redundant factors. That is, the pool is only as complex as required.
    \item Anisotropy of factors is reduced through mixing. The \gls{NMR} model is only as anisotropic as the most anisotropic factor.
    \item The range of factors is reduced through mixing. That is, the range of \gls{NMR} model is only as long as the longest range factor.
\end{enumerate}

The next step is initializing the objective function components after establishing a reasonable Gaussian pool. Optimizing the parameter vector $\theta$ occurs after this initialization. The following section discusses details of the objective function, optimization algorithm and \gls{NMR} parameter inference.


\FloatBarrier
\section{Parameter Inference}
\label{sec:04paraminfer}

The \gls{NMR} approximates the mapping function, $\mathcal{F}_{\theta}$, between latent and observed space. This function is parameterized by the unknown vector $\theta$. The unknown parameters are inferred from known features: (1) the observed data, (2) the specified latent pool, and (3) the objective function components. Features (2) and (3) are specified by the user and guide the parameter optimization process. $\theta$ is a vector of $2 \cdot (M+1)$ real values of factor weights and \gls{MPL} exponents:
\begin{equation}
    \theta = \{ a_{0}, \dots, a_{M}, \omega_{0}, \dots, \omega_{M} \}
    \label{eq:theta}
\end{equation}

and is optimized with the heuristic, genetic algorithm \gls{DE} \citep{price2013differential}. A population of candidate solutions is initialized and then evolved to mimic natural selection. Each member of the population has an associated ``fitness'' value, and the fittest members of the population are carried over to subsequent generations. Through multiple generations of mutation and crossover operations, the candidate solutions converge towards a solution that minimizes the objective function. Parameter inference begins by simulating a set of $L \cdot (M+1)$ unconditional realizations at the input data locations, where $L$ is the number of realizations and $M+1$ is the number of independent factors, including the nugget. These unconditional realizations permit the evaluation of the objective function and evolution of $\theta$.

\subsection{Differential Evolution}
\label{subsec:04de}

Optimization of network weights uses a gradient-free, heuristic genetic algorithm. Gradient-free methods are typically employed when information about the derivative of the objective function is either costly to obtain or unreliable and noisy \citep{conn2009introduction}. \Gls{DE} is a global stochastic search algorithm that is practical for non-linear, non-differentiable objective functions with a necessarily large search space \citep{rios2013derivativefree}. Any objective function is permissible, an advantage of \gls{DE}.

\Gls{DE} is based on natural evolutionary processes where the fittest member of the population survives through a ``natural selection'' process. An initial population of size $NP$ x $D$ is generated by randomly sampling the objective function space within the defined constraints. $NP$ is the number of individuals in the population, and $D$ is the problem's dimensionality. Each vector from the initial population is passed through the objective function to evaluate its evolutionary ``fitness''. The algorithm generates a mutant vector from the population by adding the scaled difference between two randomly selected vectors to a third randomly selected vector \citep{price2013differential}. The algorithm then generates a trial vector by recombining the mutant vector with the initial population's current row vector, considering a user-defined crossover probability. The trial vector's fitness is compared to the current population vector's fitness in an evolutionary sense. If the trial vector's fitness exceeds the current population vector's, it replaces it. Each iteration compares all population vectors to a randomly generated trial vector and accepts the trial vector if its fitness exceeds the current vector. Each algorithm iteration's ``surviving'' vectors become the parent vectors or the next iteration population. The following general steps summarize the \gls{DE} algorithm:
\begin{enumerate}[noitemsep]
    \item Initialize counter $i=0$
    \item Initialize a random population of size $j = 1, \dots, NP$
    \item If $i < i_{MAX}$:
          \begin{enumerate}
              \item i = i + 1
              \item For each member of the population, $\mathbf{x}_{j}$:
                    \begin{enumerate}[noitemsep]
                        \item Select $r$ other individuals where $\mathbf{x}_{r} \neq \mathbf{x}_{j}$ and $r$ is unique
                        \item Generate mutant $\mathbf{v}_{j}$
                        \item Generate trial $\mathbf{t}_{j}$ through crossover between $\mathbf{x}_{j}$ and $\mathbf{v}_{j}$
                        \item If $f(\mathbf{t}_{j}) < f(\mathbf{x}_{j})$ replace $\mathbf{x}_{j}$ with $\mathbf{t}_{j}$
                    \end{enumerate}
              \item Finish if $i = i_{MAX}$
          \end{enumerate}
    \item Return $\text{argmin}_{j} \ f(\mathbf{t}_{j})$
\end{enumerate}

The algorithm begins by randomly initializing a population of candidate vectors (Equation \ref{eq:theta}) and evaluating each member's fitness. A population size of 30-50 is a reasonable balance between sufficient diversity and algorithm runtime, though there is no definitive guideline for population size \citep{piotrowski2017review}. Over the specified number of iterations, each member of the population undergoes mutation. Optimization employs a DE/current-to-best/1 mutation strategy. \cite{georgioudakis2020comparative} describes the mutated vector as:
\begin{equation}
    \mathbf{v}_{j} = \mathbf{x}_{j} + F(\mathbf{x}_{best}-\mathbf{x}_{j}) + F(\mathbf{x}_{r1} - \mathbf{x}_{r2})
    \label{eq:mutation}
\end{equation}

Where $r1 \neq r2 \neq j$, $F$ is the scaling factor controlling the amplification of the difference vectors and $\mathbf{x}_{best}$ is the current best vector in the population. The mutation process learns from the current best vector (local searching of the solution space) while exploring the global search space through the randomly selected difference vector. The parameter $F$ is analogous to the learning rate in machine learning problems. A smaller $F$ value will lead to smaller mutation step sizes, and the algorithm will take longer to converge. Larger $F$ values increase the degree of solution exploration but may lead to divergence. \cite{price2013differential} suggests there is no upper limit for $F$, however effective values are almost always $F < 1.0$.

This new mutated ``genetic information'' is crossed over to other population members based on a crossover probability, $CR$, and a binomial crossover scheme. The trial vector is built from two vectors: the mutant and another member of the population. \cite{price2013differential} describes the crossover as:
\begin{equation}
    \mathbf{t}_{j} =
    \begin{cases}
        \mathbf{v}_{j, k}, & \text{ if }rand_{k}(0,1) \leq CR \text{ or } k=k_{rand} \\
        \mathbf{x}_{j, k}, & \text{ otherwise }
    \end{cases}
    \label{eq:crossover}
\end{equation}

Where $\mathbf{t}_{j}$ is the trial vector, $\mathbf{x}_{j, k}$ is the target vector, $k$ denotes the \gls{1D} vector index of the $j^{th}$ member of the population, $rand_{k}(0,1)$ is a uniform random number $\in [0,1]$, and $k_{rand}$ is a random index. For each element in the target vector, if the uniform random number is less than $CR$, the element is copied from the mutant vector $\mathbf{v}_{j, k}$; otherwise, the target element remains. The random index $k_{rand}$ ensures the target vector is not copied completely; at least one element of the mutant vector passes to the trial vector. $CR$ influences the diversity of the evolving population. Larger $CR$ values introduce more variation in the population, resulting in a greater search of the global solution space, while smaller values may lead to stagnation \citep{georgioudakis2020comparative}. Optimization employs a non-linear crossover scheme following the work of \cite{mohamed2014rdel}:
\begin{equation}
    CR = CR_{hi} + (CR_{lo} - CR_{hi}) \cdot (1-i/N)^{4}, \ \ i=1,\dots, N
    \label{eq:f(cross)}
\end{equation}

The idea behind the non-linear crossover scheme is that the crossover rate is lower when population variance is high in early generations. This crossover scheme prevents extreme diversity or potential divergence in the initial iterations. In subsequent generations, the population variance decreases as the vectors become similar, approaching the solution. In order to thoroughly explore this more local search space, the crossover should be high, encouraging a diverse population of ``good'' solutions.

\subsection{Objective Function}
\label{subsec:04objfunc}

The parameter vector $\theta$ is optimized heuristically through an objective function and \gls{DE}. This approach is highly flexible as the objective function can contain any number of components. The practitioner provides target spatial features of the final mixture from which a loss or objective value can be calculated for the candidate mixture. The objective value is minimized through successive iterations reproducing the target spatial features. The possible objective function components are (1) the continuous variogram, (2) indicator variograms, (3) cumulative run length frequencies, and (4) the $n$-point connectivity function. Components may have multiple sub-components for each indicator threshold. Run length frequencies and the $n$-point connectivity function are higher-order multi-point statistics that better characterize non-Gaussian features than the two-point variogram. The objective function comprises a weighted combination of $C \leq 4$ objective components:
\begin{equation}
    O = \sum_{c=1}^{C} w_{c}O_{c}
    \label{eq:fobj}
\end{equation}

Where $w_{c}$ is the component weight and $O_{c}$ is the component objective value. The objective function quantifies how different the desired feature is from the target feature. Weighting the objective function components is required as each component may exist in widely different units. For example a variogram component is expressed in units of variance squared ($\sum [ \gamma_{\text{target}} - \gamma_{\text{realization}}]^{2}$) while cumulative runs is expressed as the number of runs squared ($\sum [ R_{\text{target}} - R_{\text{realization}}]^{2}$), and may be orders of magnitude different. A weighting scheme to prevent one component dominating follows the work of \cite{deutsch1992annealing}. The goal is to have each component contribute equally to the overall objective value where each weight is inversely proportional to the average change of that objective component \citep{deutsch1992annealing}:
\begin{equation}
    w_{c} = \frac{1}{\bar{|\Delta O_{c}|}}
    \label{eq:fobj_wt}
\end{equation}

The average change of the objective component is approximated numerically by averaging the change of $J=1000$ independent forward passes through the network:
\begin{equation}
    \bar{|\Delta O_{c}|} = \frac{1}{J} \sum_{j=1}^{J} | O^{j}_{c} - O_{c}|, \ \ c=1,\dots, C
    \label{eq:fobj_avg}
\end{equation}

Where $\bar{|\Delta O_{c}|}$ is the average change of component $c$, $O^{j}_{c}$ is the updated objective value for iteration $j$ and $O_{c}$ is the initial objective value of that component. The objective value is the sum of squared errors between the experimental values calculated on the distribution derived from Equation \ref{eq:fpass1} and the initialized target values. The objective function (Equation \ref{eq:fobj}) is evaluated across all simulated realizations and minimized in expectation. Though many components can enter the objective function, one should consider contrasting objectives. For example, one cannot expect to reproduce continuous and indicator variograms closely if the principal directions of continuity are orthogonal. It is less clear how the specification of run frequencies or connectivity functions affect variograms, though there are likely confounding factors.

\subsection{Precedence}
\label{subsec:04precedence}

The application of precedence is an optional component of parameter inference. Precedence allows the spatial features of a certain latent factor to take priority in the mapping function. A particular factor can be given precedence during optimization if one desires its spatial features in certain portions of the grade range. The algorithm enforces precedence by employing a sigmoid weighting function where $s(y) = \frac{1}{1+e^{-y}}$. The weighting function modifies the forward pass (Equation \ref{eq:fpass1}) through the network to:
\begin{align}
    \label{eq:wtpass1}
    \mathbf{x} & = a_{p} \cdot \phi(\mathbf{y}_{p}, \omega_{p}) + \sum_{m=0}^{M-1} \phi(\mathbf{y}_{m}, \omega_{m}) \cdot s(\mathbf{y}_{p} \cdot x), \ \ m \neq p \\
    \label{eq:wtpass2}
    \mathbf{z} & = G^{-1}\left( F_{X}\left(\mathbf{x} \right)\right)
\end{align}

Where $a$ is the weight to the factor, subscript $p$ is the factor index with precedence, $\phi(\dots)$ is the \gls{MPL} activation function, $s(\dots)$ is the sigmoid function, and $x$ is a constant $\in [-10, 10]$. The sign and magnitude of $x$ controls what part of the grade range factor $\mathbf{y}_{p}$ influences. If $x<0$, $\mathbf{y}_{p}$ influences high values and, $x>0$, $\mathbf{y}_{p}$ influences low values. Figure \ref{fig:sigmoid} shows a weighting function where $x=-1.5$. The y-axis is the weight to the remaining $M-1$ factors for the range of values of $\mathbf{y}_{p}$. In this scenario, $\mathbf{y}_{p}$ receives $\approx 95\%$ of the weight when equal to 2.0, $\approx 80\%$ of the weight when equal to 1.0 and so on. As the magnitude of $x$ increases, $s(\dots)$ tends towards a binary step function centered on zero.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/sigmoid.png}
    \caption{Sigmoid weighting function where $x=-1.5$. The x-axis is the range of $\mathbf{y}_{p}$, and the y-axis is the weight to the remaining M-1 factors. }
    \label{fig:sigmoid}
\end{figure}

The sigmoid weighting function allows a certain factor to take precedence where the observed data takes on either high or low values. This increased control is particularly useful for capturing local high- or low-grade continuity with well-designed latent factors.

\subsection{Complete Algorithm}
\label{subsec:04algorithm}

Algorithm \ref{alg:nmropt} summarizes all components of the \gls{NMR} parameter inference workflow. The first step is the unconditional simulation of the latent Gaussian pool at the data locations (lines 2-6). The objective function components are scaled using the unconditional realizations (lines 7-10) prior to heuristically optimizing $\theta$ with \gls{DE} (lines 11-35).

\begin{algorithm}
    \caption{\gls{NMR} parameter inference pseudocode.}\label{alg:nmropt}
    \begin{algorithmic}[1]
        \State simulate unconditional realizations (LU or SGS):
        \For{$\ell = 1, \dots, L$}
        \For{$m = 1, \dots, M$}
        \State simulate $\mathbf{Y^{(\ell)}_m}$
        \EndFor
        \EndFor
        \State scale objective components:
        \For{$c = 1, \dots, C$}
        \State $w_{c} = \frac{1}{\bar{|\Delta O_{c}|}}$
        \EndFor
        \State initialize population: $pop(pool dim, popsize)$ \Comment{candidate solutions}
        \State calculate fitness of population: $fpop$
        \State $best = minloc(fpop)$ \Comment{index of initial best solution}
        \For{i=1,\dots, its} \Comment{begin DE}
        \For{j=1,\dots, popsize}
        \State $mutant = \Call{mutation}{pop(j)}$ \Comment{best-to-current mutation}
        \State $\theta = \Call{crossover}{mutant}$ \Comment{binomial crossover}
        \State $a, \omega = \Call{vector\_to\_matrices}{\theta}$ \Comment{reshape $\theta$ to $a, \omega$ vectors}
        \State $O = 0.0$
        \For{$\ell = 1, \dots, L$}
        \State $z = \Call{network\_forward}{a, \omega}$ \Comment{forward pass through network}
        \State $iz = \Call{indicator\_transform}{z, thresholds}$
        \For{$c = 1, \dots, C$}
        \State $O_{c} = \sum [ \gamma^{\text{target}}_{c} - \gamma^{\text{realization}}_{c}]^{2}$
        \EndFor
        \State $O = \sum_{c=1}^{C}w_{c}\cdot O_{c}$
        \EndFor
        \State $O = O / L$
        \If{$O < fpop(best)$}
        \State $fpop(best) = O$ \Comment{update objective value}
        \State $pop(best) = \theta$ \Comment{retain the trial}
        \State $best = j$ \Comment{track the new best}
        \EndIf
        \EndFor
        \EndFor \Comment{end DE}
    \end{algorithmic}
\end{algorithm}

Inference of $\theta$ is relatively straightforward with only a handful of \gls{DE} parameters. However, care must be given to align the design of the latent pool and objective components with the final goals of the \gls{NMR} model. Depending on the modeling scenario, this is the most challenging portion of the workflow.

\FloatBarrier
\subsection{Checks}
\label{subsec:04checks}

The quality of the \gls{NMR} parameter inference is measured based on reproducing the objective function components. Generally, these components come directly from the observed data, so the inferred parameters reproduce observed geologic features. A small \gls{3D} synthetic example with 746 data is generated to highlight the concepts of parameter checking. The data set is simulated so that the indicator variograms are asymmetric about the median and have connectivity of high-grade values. These features provide a reasonable test case for applying the \gls{NMR} workflow. Imputation concepts are presented using the same data set in Chapter \ref{ch:05impute}. Figure \ref{fig:synthetic_sections} shows a plan-view and long-section through the synthetic data in Gaussian units. Figure \ref{fig:synthetic_varios} shows experimental variogram points and fitted models for the normal score variable (a), the 0.1 indicator (b) and the 0.9 indicator (c). For brevity, the median indicator variogram is not shown; however, the indicator variograms show asymmetric destructuring about the median. Figure \ref{fig:synthetic_npoint} shows the $n$-point connectivity function, which approaches zero at five connected steps ($\approx 50$m). The objective function components are the normal score variogram model, the 0.1 indicator variogram model and the 0.9 indicator variograms (Figure \ref{fig:synthetic_varios} (a), (b), and (c)), and the $n$-point connectivity function (Figure \ref{fig:synthetic_npoint}).

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/synthetic_sections.png}
    \caption{Plan-view (left) and long-section oriented $340\degree$ (right) though the synthetic data set. Both sections have a tolerance of $\pm25$ meters.}
    \label{fig:synthetic_sections}
\end{figure}

\begin{figure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/syn_cont_vario.png}
        \caption{Normal Scores}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/syn_ivario_0_1.png}
        \caption{0.1 Quantile}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/syn_ivario_0_9.png}
        \caption{0.9 Quantile}
    \end{subfigure}
    \caption{Fitted experimental variograms for the normal score variable (a), the 0.1 indicator (b) and 0.9 indicator (c). Note the asymmetric destructuring of the indicator variograms. }
    \label{fig:synthetic_varios}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/synthetic_npoint.png}
    \caption{$N$-point connectivity function above the 0.9 quantile indicator for the synthetic data set.}
    \label{fig:synthetic_npoint}
\end{figure}

The latent factor pool contains two structures: the long-range structure from the 0.1 quantile indicator model and the single structure from the 0.9 quantile indicator model. The goal of the pool is to have the long-range structure spatially control the lower grades and the short-range structure spatially control the highs. Some degree of mixing between the structures permits the reproduction of the normal score variogram model. The omega parameters are constrained such that $0.25 \leq \omega_{1} \leq 1.0$ and $2.0 \leq \omega_{2} \leq 4.0$. No precedence is given to either factor. The algorithm is run for 1500 \gls{DE} iterations. The primary \gls{NMR} parameter check is the reproduction of objective function components. The mapped unconditional realizations, $\mathcal{F}_{\theta}\left(\mathbf{Y}\right)$, must reproduce, on average, the input objective targets. Figure \ref{fig:ex5_ivariogram} shows normal score variogram reproduction for the inferred mapping function. Reproduction is reasonable though there is some deviation at shorter lags. This deviation is attributed to these lags having fewer pairs. Figure \ref{fig:ex5_ivariogram} shows variogram reproduction for the 0.1 (a) and 0.9 (b) quantile indicators. The indicators show a wider band of uncertainty related to data density, particularly at shorter lag distances with fewer pairs. Figure \ref{fig:ex5_npoint} shows $n$-point connectivity reproduction for the 0.9 quantile indicator.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/ex5_variogram.png}
    \caption{Continuous variogram reproduction for 25 realizations. The black line is the variogram model, the red line is the average variogram, and the shaded red area encloses the minimum and maximum variogram values. Left to right are the major, minor and vertical directions, respectively.}
    \label{fig:ex5_variogram}
\end{figure}

\begin{figure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/ex5_ivariogram_0_1.png}
        \caption{0.1 Quantile}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/ex5_ivariogram_0_9.png}
        \caption{0.9 Quantile}
    \end{subfigure}
    \caption{Indicator variogram reproduction for 25 realizations for the 0.1 (a) and 0.9 (b) quantiles. Left to right are the major, minor and vertical directions, respectively. The black line is the variogram model, the red line is the average variogram, and the shaded red area encloses the minimum and maximum variogram values.}
    \label{fig:ex5_ivariogram}
\end{figure}


\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.50, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/ex5_npoint.png}
    \caption{$N$-point connectivity function reproduction for 25 realizations for the 0.1 quantile indicator (left) and 0.9 quantile indicator (right). The black line is the target model, the red line is the average experimental value, and the shaded red area encloses the minimum and maximum experimental values.}
    \label{fig:ex5_npoint}
\end{figure}

At this point in the \gls{NMR} workflow, the inferred parameters reasonably approximate the mapping function between latent and observed space. The unconditional Gaussian realizations reproduce the objective function components on average when mapped through $\mathcal{F}_{\theta}$. Any deviations between the optimization targets and mapped latent factors will manifest in the final \gls{NMR} realizations as latent factor imputation (Chapter \ref{ch:05impute}) is anchored on $\mathcal{F}_{\theta}$. If present, these deviations are transferred to the imputed data realizations that condition the gridded factor realizations. This same synthetic data configuration is used to check imputed realization in Chapter \ref{ch:05impute}.

\FloatBarrier
\section{Implementation Details}
\label{sec:04implementd}

The goal of the \gls{NMR} model is to infer the best possible mapping between latent and observed spaces. Given the data configuration, the design of the latent pool and the objective function components, there may be qualitative and quantitative uncertainty in the parameter vector $\theta$. This section discusses some practical implementation details of \gls{NMR} parameter inference that may contribute to or mitigate parameter uncertainty. Topics in this section include the non-uniqueness of the inferred solution, the number of data, the possibility of conflicting objectives, and overall computational considerations.

\FloatBarrier
\subsection{Non-Uniqueness}
\label{subsec:04nonunique}

Solutions to inverse problems are often non-unique. Experimental variograms in the presence of sparse data are inherently uncertain statistics. Considering these facts, it is reasonable to assume that there are multiple (possibly infinite) network configurations and pools of Gaussian factors that approximate the objective function targets. This non-uniqueness emphasizes the importance of considering the conceptual geological model when designing latent factors. Some latent pools are feasible from a numerical perspective but should also be geologically reasonable. The following section highlights non-uniqueness examples in \gls{NMR} parameter inference. Potential non-unique scenarios are summarized as follows:
\begin{enumerate}[noitemsep]
    \item The \gls{NMR} solution may be an arbitrary mixture of factors even if the variogram objective targets exist within the Gaussian pool.
    \item A mix of short and long-range factors can reproduce a medium-range target.
    \item A mix of only short-range factors cannot reproduce a medium or long-range target, and vice versa.
    \item A mix of factors with NS and EW orientations can reproduce a target orientation of NNE, NNW, SSE, SSW, and so on.
\end{enumerate}

Consider the same data configuration from Section \ref{subsec:04checks}. Synthetic data values are simulated using the \gls{NMR} with a known parameter vector, $\theta$. For clarity of the example, only the reproduction of the continuous variogram is measured. Different pools of latent factors are mixed to highlight the non-uniqueness scenarios mentioned above. The weight to each factor, $a_{m}$, is interpreted as the relative importance of each factor to the fit of $\theta$. This weight can be compared to the known weight used to simulate the data to measure similarity. The solution is non-unique if multiple latent pools and varying parameters can reproduce the target. Another approach to understanding the dependence of each latent factor on $\theta$ is a measure of \gls{PFI} \citep{fisher2019all}. Feature importance can be calculated by permuting the input latent factors and calculating the increase in error in the output of the fitted model. \gls{PFI} is in the same units as the objective function. Features that are pertinent to the network output will show more significant errors when they are permuted. A caveat is that shorter-range structures (more random) will always show less feature importance than longer-range structures (less random) as permutation introduces randomness. Shorter range structures may still contribute to the final fit of the model without showing high permutation feature importance.

Consider the first scenario listed above. The latent pool consists of the long-range structure of the normal score variogram model, the long-range structure of the 0.1 quantile indicator variogram model, the single structure of the 0.9 quantile indicator variogram model, and the nugget effect for $M = 3+1$ latent factors. Intuition suggests that if that variogram model (or its elemental components) exists within the Gaussian pool, the network will filter all irrelevant factors and weight the important components appropriately. In practice, this is only sometimes the case. Table \ref{tab:optwts_c01} shows the $a$ and $\omega$ elements of $\theta$, plus \gls{PFI} for comparison. Figure \ref{fig:nonunq_c01_variogram} shows the corresponding variogram reproduction. The $a$'s show how much weight is given to factors two and three that correspond to the nested structures of the indicator variograms. Only and portion of the weight is given to factor 1. The \gls{PFI} backs up this weighting, showing factors two and three most important. Granted, there is a similarity between the low-grade indicator and normal score variogram models; the resulting variogram of the mixture model may be an arbitrary mixture of latent factors.

\begin{figure}
    \begin{subfigure}{1.\textwidth}
        \centering
        \includegraphics[scale=0.70, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/nonunq_c01_variogram.png}
        \caption{}
        \label{fig:nonunq_c01_variogram}
    \end{subfigure}
    \begin{subtable}{1.\textwidth}
        \centering
        \resizebox{1\width}{!}{\input{0-Tables/optwts_c01.tex}}
        \caption{}
        \label{tab:optwts_c01}
    \end{subtable}
    \caption{Normal score variogram reproduction (a) in the major, minor and vertical directions, respectively, and the components of $\theta$ (b) for the first scenario. PFI=permutation feature importance. }
    \label{}
\end{figure}

The second scenario is intuitive. A mix of short-range and long-range factors can reproduce features with intermediate-range. Again, this scenario is non-unique as one may consider any number of factors and variogram ranges are continuous and essentially unbounded. Consider a pool with two latent structures: one long-range and one short-range. The long-range structure corresponds to the second structure of the 0.1 quantile indicator variogram scaled by a factor of 1.5 in all directions. The short-range structure is the 0.9 quantile indicator model scaled by a factor of 0.7 in all directions. Table \ref{tab:optwts_c02} shows the $a$ and $\omega$ elements of $\theta$, plus \gls{PFI} for comparison. Figure \ref{fig:nonunq_c02_variogram} shows the corresponding variogram reproduction. The mixture of long and short-range structures can reproduce the variogram model of intermediate range with each factor receiving similar weight. This concept is also applicable to the orientation of factors in the pool. For example, the mix of factors with east-west and north-south orientations can generate features with intermediate orientation in the final model.

\begin{figure}
    \begin{subfigure}{1.\textwidth}
        \centering
        \includegraphics[scale=0.70, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/nonunq_c02_variogram.png}
        \caption{}
        \label{fig:nonunq_c02_variogram}
    \end{subfigure}
    \begin{subtable}{1.\textwidth}
        \centering
        \resizebox{1\width}{!}{\input{0-Tables/optwts_c02.tex}}
        \caption{}
        \label{tab:optwts_c02}
    \end{subtable}
    \caption{Normal score variogram reproduction (a) in the major, minor and vertical directions, respectively, and the components of $\theta$ (b) for the second scenario. PFI=permutation feature importance. }
    \label{}
\end{figure}

The third scenario is also straightforward. A mix of short-range factors cannot reproduce a longer-range target as destructive interference occurs when mixing factors. The range of structures in the final model can only be as long as the longest range structure in the latent pool. The opposite of this is also true; a mix of long-range structures cannot reproduce a short-range target. In this scenario, the pool consists of the two nested structures of the normal score variogram model scaled by a factor of 0.3 in all directions. Table \ref{tab:optwts_c02} shows the $a$ and $\omega$ elements of $\theta$, plus \gls{PFI} for comparison. Figure \ref{fig:nonunq_c02_variogram} shows the corresponding variogram reproduction. As expected, the algorithm cannot converge on an acceptable solution due to poor latent feature design. The algorithm attempts to filter the first unnecessary factor by giving it a weight of zero, though the remaining factors do not have sufficient flexibility to reproduce the target.

\begin{figure}
    \begin{subfigure}{1.\textwidth}
        \centering
        \includegraphics[scale=0.70, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/nonunq_c03_variogram.png}
        \caption{}
        \label{fig:nonunq_c03_variogram}
    \end{subfigure}
    \begin{subtable}{1.\textwidth}
        \centering
        \resizebox{1\width}{!}{\input{0-Tables/optwts_c03.tex}}
        \caption{}
        \label{tab:optwts_c03}
    \end{subtable}
    \caption{Normal score variogram reproduction (a) in the major, minor and vertical directions, respectively, and the components of $\theta$ (b) for the third scenario. PFI=permutation feature importance. }
    \label{}
\end{figure}

This section presents scenarios to emphasize the non-uniqueness of the \gls{NMR} solution. Numerous latent pools can reproduce objective targets. The final example presents a scenario where the design of the pool and the objective function have conflicting objectives. The scenario is designed for illustrative purposes and defies any geologic-based logic; however, it emphasizes the practitioner's role in ensuring the latent pool and objective function are sound concerning the conceptual geology. Adding components to the objective function amplifies this issue. The requirement of ``reasonableness'' further promotes the decomposition of all variogram targets into an initial latent Gaussian pool.

\FloatBarrier
\subsection{Number of Data}
\label{subsec:04ndata}

The number of data is an important consideration in the \gls{NMR} workflow. Experimental statistics form the basis of the objective function components and are sensitive to the number of available data. Experimental variograms are an inherently uncertain statistic, particularly in the presence of sparse data \citep{ortiz2002calculation,pardo-iguzquiza2012varboot}. The variogram value at lag vector $\mathbf{h}$ is the mean of the squared differences between data values separated by $\mathbf{h}$. This mean value depends directly on the number of data, $n_{\mathbf{h}}$, entering the calculation, which depends on the data configuration and variogram tolerance parameters. Uncertainty in the experimental variogram points transfers to uncertainty in chosen model parameters, though this is only quantifiable with knowledge of the true variogram model. Sequence objective components are calculated downhole and are more sensitive to the total number of drillholes than the total data. However, the shape of the global distribution of runs or the global $n$-point connectivity function ultimately depends on the number of data.

A synthetic model is simulated to assess the sensitivity of $\theta$ to data spacing or the total number of available data. The model is simulated on a regular 56 x 56 x 56 m \gls{3D} grid with a resolution of 1 m. The grid is sampled with regular, square data configurations ranging from 3 x 3 m to 20 x 20 m, with 1 m spacing in the vertical direction. Table \ref{tab:syn_data} shows the resampled data configurations and the corresponding number of data. The grid is also sampled at a 1 x 1 m spacing, a reference distribution for calculating the ``true'' variograms, distributions of runs, and $n$-point connectivity functions. A parameter vector $\theta$ is inferred for each data spacing, and the unconditional latent factors are mapped to the observed space. The objective function value is the weighted sum of squared errors (Equation \ref{eq:fobj}) between the mapped latent values and the objective components calculated from the reference distribution. Figure \ref{fig:mse_vs_data_plot} shows the relationship between the square data spacings and the objective function value.

\begin{figure}
    \begin{subtable}{.5\textwidth}
        \centering
        \resizebox{1\width}{!}{\input{0-Tables/dstable.tex}}
        \caption{}
        \label{tab:syn_data}
    \end{subtable}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[scale=0.70, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/mse_vs_data.png}
        \caption{}
        \label{fig:mse_vs_data_plot}
    \end{subfigure}
    \caption{Square data spacings and corresponding numbers of data (a) and corresponding minimum sum of squared error values from \gls{NMR} parameter inference(b). The total number of data increases to the right of the plot in (b). }
    \label{fig:mse_vs_data}
\end{figure}

The sum of the squared errors is inversely proportional to the total number of data, which is the anticipated response. The curve decreases quickly to a spacing of 10 x 10 m or approximately 2000 data. Beyond this spacing, the curve is much flatter but does continue to decrease towards the tightest spacing. This relationship suggests that somewhere between 2000 and 5000 data are enough to provide a stable inference of $\theta$. Beyond 5000 data, the error decreases slightly, though these improvements are negligible when considering the increase in computation time.

\FloatBarrier
\subsection{Computational Considerations}
\label{subsec:04comp}

The NMR objective function is a computationally expensive calculation. This expense is primarily due to experimental variogram calculation. As each population vector is an entirely new network, all experimental variogram pairs must be updated on each iteration. Depending on the data configuration, this updating may account for a significant portion of the algorithm run time. Two straightforward approaches to dealing with runtime are:
\begin{enumerate}
    \item \Gls{PDE}
    \item Constraining the maximum number of experimental variogram pairs per lag
\end{enumerate}

Evaluating the objective function for each member of the population in \gls{DE} is an independent task and lends itself to parallelization. \gls{PDE} is a slightly different algorithm than \gls{DE}. In \gls{DE}, the population is updated after evaluating each trial vector (loop on line 15 in Algorithm \ref{alg:nmropt}), so each subsequent mutation includes information from the previous. In \gls{PDE}, the entire population is mutated and evaluated before updating the next generation. This operation introduces new genetic information to the population once per generation rather than after every mutation. Though slightly different algorithms, \gls{DE} and \gls{PDE} produce similar, but different, final results. Figure \ref{fig:nmropt_serial_parallel} shows the objective function value versus iteration for two optimization runs: serial and parallel. Both scenarios use the same number of data, objective components and hyperparameters. \Gls{PDE} achieves a similar objective function value to serial \gls{DE}, but $\approx 6.5$ times quicker.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.50, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/nmropt_serial_parallel.png}
    \caption{Objective function value versus iterations for runs of the \gls{DE} and \gls{PDE} algorithms.}
    \label{fig:nmropt_serial_parallel}
\end{figure}

The second potential speedup comes from setting a maximum number of randomly selected experimental variogram pairs to consider per lag when evaluating the objective function. Figure \ref{fig:vario_stability_dir1} shows that restricting the maximum number of pairs per lag to $\approx 10000$ yields a stable variogram relative to all possible pairs. Restricting the pairs speeds up the objective function calculation, though one should check variogram stability for each data configuration.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.50, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/vario_stability_dir1.png}
    \caption{Sensitivity of experimental variogram points to the total number of pairs per lag.}
    \label{fig:vario_stability_dir1}
\end{figure}

% The number of realizations to simulate may be significant depending on the chosen number of latent factors. However, the simulation implementation is efficient as it only considers data locations.

\FloatBarrier
\section{Discussion}
\label{sec:04discuss}

The \gls{NMR} structure is inspired by neural networks from the field of \gls{ML}. The \gls{NMR} approximates the mapping function, $\mathcal{F}_{\theta}$, from latent to observed space. The parameter vector, $\theta$, contains the weights applied to each latent factor and the corresponding \gls{MPL} activation function exponent. $\theta$ permits the mixture of multivariate Gaussian spatial distributions in ways that reproduce non-Gaussian spatial features. The components of $\theta$, $a$ and $\omega$, are optimized stochastically with the heuristic, genetic global optimization algorithm \gls{DE}. A significant advantage of the genetic algorithm approach is the flexibility of both the activation and the loss or objective function. There is no constraint of differentiability on either the activation or objective function as \Gls{DE} is a gradient-free algorithm. The objective function may contain any component deemed necessary for the modeling scenario. The \gls{MPL} activation function allows the influence of certain latent spatial features on certain portions of the grade range of the final mixture model. This activation function is key to achieving asymmetric spatial continuity between highs and lows. Some latent factors may only affect high and some low grades, conditional on constraints placed on $\omega$. The practitioner has significant flexibility concerning latent factor design in both number and covariance structure. The design of the latent factors must consider the conceptual geologic model and be reasonable in the context of the modeling goals. Poorly designed factors may conflict with objective function components, preventing algorithm convergence; in some cases, reproducing the objective components may be impossible with the chosen factors. Decomposition of the nested variogram structures of the objective function components is a reasonable initial pool, provided one is confident in the variogram interpretation. One should prune redundant factors and add specific factors as required.

This chapter has introduced the first component of the \gls{NMR} simulation workflow: inference of the mapping function $\mathcal{F}_{\theta}$. The following chapter introduces the second workflow component: imputation of latent factors. The ultimate modeling goal is gridded, non-Gaussian realizations for mine planning. Conditioning those realizations requires data. Data realizations are imputed for each latent factor such that they have the same spatial properties as the mixture model when mapped through $\mathcal{F}_{\theta}$. The synthetic example introduced in Section \ref{subsec:04checks} is carried forward into Chapter \ref{ch:05impute}.