%!TEX root = ../Thesis_example_compile_this.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Network Implementation}
\label{ch:04implement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter presents the implementation details of the \gls{NMR} introduced in Chapter \ref{ch:03framework}. The network is the first component of \gls{NMR} simulation workflow and operates in conjunction with \gls{SGRI}, introduced in Chapter \ref{ch:05impute}. Inference of network parameters, $\theta$, is an inverse problem as only the true data values are known. A mapping function, $\mathcal{F}_{\theta}$, is therefore required to map the unknown latent space to the known observed space. The function is parameterized such that a pool of unconditional, latent Gaussian factors reproduce the known values and have the desired spatial features when mapped with $\mathcal{F}_{\theta}$. As the latent space is unknown, and ultimately a synthetic feature of the \gls{NMR} model, it is free to contain any number of components with any covariance structure. This flexibility permits creativity with latent factor design; combining latent covariance structures in unique ways allows a mixture of univariate Gaussian distributions to possess non-multivariate Gaussian spatial features. Practical implementation details include considerations for design of the network and the latent Gaussian pool as well as sensitivities associated with these model parameters. A small synthetic example demonstrates latent factor design, network parameter inference and non-uniqueness properties of the \gls{NMR}. This example highlights the network component of the complete \gls{NMR} workflow and is carried forward into Chapter \ref{ch:05impute} to illustrate the second imputation component.

\FloatBarrier
\section{Network Design}
\label{sec:04networkd}

The \gls{NMR} is not a true neural-network, rather a model of regionalization inspired by neural-network structure. The network consists of an input layer where the number of latent factors determines the number of input nodes, a single ``hidden layer'' of the same dimension, and an output layer with a single node.  The single output node makes the \gls{NMR} univariate (analogous to the \gls{LMR}) however there is no reason it could not be extended to the multivariate case (analogous to the \gls{LMC}). However, with geospatial data, one commonly considers extreme values in the univariate context; it is not immediately clear what constitutes a multivariate extreme.

\subsection{Architecture}
\label{subsec:04arch}

Network architecture is controlled by the number of latent factors in the Gaussian pool. The current \gls{NMR} implementation restricts the architecture of the network to an input layer, a single hidden layer and a univariate output layer. The input and hidden layer both contain $M$ nodes. Figure \ref{fig:example_nmr} shows an example network configuration with $M=3$ input latent factors, $\{ Y_{0}, \dots, Y_{3}\}$. By convention, the $0^{th}$ factor is the nugget effect. The hidden layer of the network if effectively a transformation layer, transforming the latent Gaussian values to unitless activation values. The network activation function $\phi(\dots)$ is a \gls{MPL} function; the following subsection presents the details. The network output $Z_{1}$ is then a weighted linear combination of the activated latent factors.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, trim={0 1.5cm 0 1.5cm}, clip=true]{./0-Figures/04-Ch4/example_nmr.png}
    \caption{Schematic representation of the \gls{NMR} with $M=3$ latent factors. $\phi(\dots)$ is the \gls{MPL} activation function. }
    \label{fig:example_nmr}
\end{figure}

The generalized forward pass through the network is defined as:
\begin{align}
    \label{eq:fpass1}
    \mathbf{x} & = \sum_{m=0}^{M}a_{m} \cdot \phi(\mathbf{y}_{m}, \omega_{m}) \\
    \label{eq:fpass2}
    \mathbf{z} & = G^{-1}\left( F_{X}\left(\mathbf{x} \right)\right)
\end{align}

Where $\mathbf{x}$ is an intermediary activation vector, $a_{m}$ is a weight applied to factor $m$, $\phi(\mathbf{y}_{m}, \omega_{m})$ is the \gls{MPL} activation function with exponent $\omega$ applied to factor $m$, $F_{X}$ is the \gls{CDF} of $\mathbf{x}$ and $G^{-1}$ is the inverse of the Gaussian \gls{CDF}. The weights $\{ a_{0}, \dots, a_{M}\}$ are constrained to be greater than or equal to zero and reflect the relative importance of each factor in the mapping function. Alternatively, Equation \ref{eq:fpass1} can be rewritten in matrix notation as:
\begin{equation}
    \mathbf{x} = \phi\left( \mathbf{Y} \right) \mathbf{a}^{T}
    \label{eq:fpass3}
\end{equation}

Where $\mathbf{a}$ is a row vector of factor weights and $\phi\left(\mathbf{Y} \right)$ is an $ndata$ x $M+1$ matrix of activated latent factors. The raw output of the network is units of activation that must be transformed to Gaussian space, necessitating the normal score transform. With the given architecture, the \gls{NMR} requires inference of $2 \cdot (M+1)$ parameters: a $1$ x $M+1$ dimensional vector of factor weights and a $1$ x $M+1$ dimensional vector of power-law exponents $\omega$. These $2 \cdot (M+1)$ network parameters are inferred through stochastic optimization and discussed in Section \ref{sec:04paraminfer}. The univariate normal score transform from activation units to Gaussian units in Equation \ref{eq:fpass2} requires a representative distribution that is unique to the given state of the network. Each iteration of the stochastic optimization algorithm generates an updated parameter vector resulting in a new mapping. The iteration specific reference distribution is created by drawing a $1e^{4}$ x $M+1$ dimensional matrix of independent, standard normal Gaussian values and mapping them to a $1e^{4}$ x $1$ vector of activation values. These activation values are normal score transformed resulting in a temporary, iteration specific transformation table. The $G^{-1}$ operator in Equation \ref{eq:fpass2} is the linear interpolation of activation values using this transformation table.

\subsection{Activation Function}
\label{subsec:04activation}

The goal of the \gls{NMR} is to parameterize the arbitrary mapping function, $\mathcal{F}_{\theta}$, between the latent and observed spaces. The form of this function is not obvious, hence the use of a network based approach as a function approximation. Given the complex spatial features we wish to capture in the final models, a polynomial of degree greater than one is useful. Suppose a linear activation function is used. As the forward pass through the network is a weighted, linear combination of the inputs, a linear activation function (or simply $c \cdot \mathbf{y}$ where $c$ is a constant) results in a linear output or single order polynomial \citep{sharma2020activation}. To achieve a non-linear network output, one must introduce a non-linearity in the form of an activation function. Real data commonly contain non-linearly separable features and a non-linear activation function permits projection of these features onto a non-linear feature space \citep{dubey2022activation}.

In the traditional \gls{ML} context, the constraint of differentiability is placed on activation functions due to use of the back-propagation algorithm \citep{rojas1996backpropagation}. The \gls{NMR} is only inspired by the structure of a neural-network and its parameters are ``learned'' through gradient-free stochastic optimization. This gradient free approach, and a strength of the \gls{NMR}, permits the use of virtually any activation function, differentiable or not. The \gls{NMR} activation function is a \gls{MPL} function with the form:
\begin{equation}
    \phi \left( \mathbf{y} \right) =
    \begin{cases}
        \mathbf{y}^{\omega},           & \text{if $\mathbf{y} > 0$} \\
        \mathbf{y}^{\frac{1}{\omega}}, & \text{if $\mathbf{y} < 0$} \\
    \end{cases}
    \label{eq:power}
\end{equation}

Where $\omega$ is a trainable parameter. The magnitude of $\omega$ allows the activation function to emphasize certain regions of the latent distribution. If $\omega = 1$, the activation is linear and $\phi \left( \mathbf{y} \right)=\mathbf{y}$. If $\omega$ is less than one the function takes on a concave shape that emphasizes low values and mutes the influence of high values. When $\omega$ is greater than one, the opposite is true. The function takes on a convex shape that emphasizes high values and mutes the influence of low values. Figure \ref{fig:power_activation} (left) shows the relationship between $\phi \left( \mathbf{y} \right)$ and $\mathbf{y}$ for various values of $\omega$. As the magnitude of $\omega$ increases, the activation function becomes steeper above zero, and flatter below zero. The magnitude of the high values are increased exponentially, and low values are muted significantly. This non-linear amplification allows the network to embed high-grade features of latent factors in the mapping function $\mathcal{F}_{\theta}$. Low grade features are embedded in opposite fashion.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/power_activation.png}
    \caption{The \gls{MPL} (left) and scaled \gls{MPL} (right) activation function for various values of $\omega$ and input $\mathbf{y} \in [-5,5]$. The scaled activation uses $\xi = G^{-1}(0.999) \approx 3.09$. }
    \label{fig:power_activation}
\end{figure}

It is notable that the \gls{MPL} activation has three inflection points: at -1, 0, and 1. The inflections exist as $\pm 1^{\omega} = \pm 1$ and $0^{\omega} = 0$, regardless of $\omega$. No negative effects have been observed related to these inflections, however the points at -1 and 1 may be adjusted by introducing a scaling factor, $\xi$, to the \gls{MPL} activation function:
\begin{equation}
    \phi \left( \mathbf{y}, \xi \right) =
    \begin{cases}
        \xi \cdot \left(\frac{\mathbf{y}}{\xi}\right)^{\omega},           & \text{if $\mathbf{y} > 0$} \\
        \xi \cdot \left(\frac{\mathbf{y}}{\xi}\right)^{\frac{1}{\omega}}, & \text{if $\mathbf{y} < 0$} \\
    \end{cases}
    \label{eq:power_scale}
\end{equation}

The scaling parameter $\xi$ allows a predetermined inflection point to be set while retaining the linear nature of the function when $\omega = 1$. Figure \ref{fig:power_activation} (right) shows the scaled \gls{MPL} activation function where $\xi = G^{-1}(0.999) \approx 3.09$ and $G^{-1}$ is the inverse of the Gaussian \gls{CDF}. $\xi$ effectively controls the threshold at which dampening or emphasizing a factor occurs at. When $\omega > 1$ and $0 < y < \xi$, the \gls{MPL} reduces the value of $y$ slightly, similar to an opportunity-seeking risk perspective \citep{eidsvik2015value}, saying ``we are only interested in values of $y > \xi$.'' When $\omega < 1$ and $0 < y < \xi$, the \gls{MPL} increases the value of $y$ slightly, similar to a risk-averse perspective, saying ``we are only interested in values of $y < \xi$.'' The opposite relationships hold true when $y < 0$. It is not immediately clear when the \gls{MPL} should be scaled, however if the practitioner notices artifacts related to the activation function, $\xi$ can be tuned. The range of $\omega$ for each latent factor is an important component of latent factor design, discussed in Section \ref{sec:04factord}. Constraining $\omega_{m} > 1$ enforces higher-grade features of the $m^{th}$ factor in the mapping. Conversely, constraining $\omega_{m} < 1$ enforces lower-grade features of the $m^{th}$ factor in the mapping. Values of $\omega$ are practical within $[0.25, 4.0]$ and reflect the relative influence of on the high and low values of each latent factor.

% \begin{enumerate}[noitemsep]
%     \item Architecture
%     \item Gaussian transform
%     \item Activation function
%     \item Recall kriging is linear, but results can be highly non-linear
% \end{enumerate}

\FloatBarrier
\section{Latent Factor Design}
\label{sec:04factord}

Latent factor design is a critical component of the \gls{NMR} workflow. The latent factors form the pool from which the \gls{NMR} draws structure from. The univariate Gaussian factors are mixed with the goal of generating a spatial distribution that is univariate Gaussian, but not multivariate Gaussian. The mixture distribution can only contain features, or combination of features, present in the Gaussian pool. Therefore, it is important that any feature required in the final model be present in the pool. The Gaussian pool is similar to the concept of factorial kriging \citep{goovaerts1997geostatistics}, where the nested variogram model is a composition of $L$ basic variogram structures, and the regionalized variable is a composition of $L$ independent, standard normal, spatial components operating at different geologic scales:
\begin{align}
    \label{eq:gamma_comp}
    \gamma(\mathbf{h}) & = \sum_{\ell=1}^{L} \gamma_{\ell}(\mathbf{h})                     \\
    \label{eq:z_comp}
    Z(\mathbf{u})      & = \sum_{\ell=1}^{L}b_{\ell}Y_{\ell}(\mathbf{u}) + \mu(\mathbf{u})
\end{align}

Where the coefficient $b_{\ell}$ is the square root of the variance contribution of $\gamma_{\ell}$. The key difference between the \gls{NMR} and factorial kriging approaches, is that the regionalized \gls{NMR} variable is a weighted, non-linear combination of independent spatial component as Equation \ref{eq:fpass1} highlights. With the \gls{LMC} or factorial kriging, the $Y_{\ell}$ independent factors are synthetic features of the model and are not directly observed. The \gls{NMR} requires the explicit definition of a pool of independent Gaussian factors. The pool may contain any number of structures that define the network's input dimension. It is simple to generate realizations of the independent Gaussian factors with any unconditional simulation algorithm such as \gls{SGS} \citep{gomez-hernandez1993joint} or LU simulation \citep{davis1987production}.

The choice of covariance structures for the Gaussian pool must consider the end goal of the spatial mixture. The latent factors must be reasonable in the sense that it is possible to achieve the objective. For example, the pool must contain short-range features if the final goal is short-range features. Long-range and short-range structures can produce a final mixture with medium-range features; however, a pool with only long-range structures cannot. Pool considerations include (1) the conceptual geological model, (2) the $L$ nested components of normal score variogram, (3) the $L$ nested components of each indicator variogram model and their potential asymmetry, (4) the down hole connectivity measures from the observed data and the potential connectivity of extreme values, and (5) the composition of the objective function, discussed in more detail in Section \ref{sec:04paraminfer}. The factorial kriging concept provides a reasonable starting point for the design of the Gaussian pool. An initial pool can be inferred through the decomposition of all variogram models into their basic components (as in Equation \ref{eq:gamma_comp}). Each basic variogram component is a single licit structure with three orientation parameters, three range parameters, a nugget of zero and sill of one. By convention a pure nugget latent factor is added as the $0^{th}$ factor.

Consider a small \gls{2D} synthetic example where the goal of the \gls{NMR} is to generate gridded realizations with strongly asymmetric 0.1 and 0.9 quantile indicator variograms. That is, the low-grade two-point spatial continuity differs drastically from the high-grade. This scenario is challenging for multivariate Gaussian simulation algorithms and has been discussed at length by many practitioners \citep{journel1983nonparametric, gomez-hernandez1998be, journel1993entropy,renard2011conditioning, guthke2013non}. The maximum entropy characteristic of the multivariate Gaussian distribution tends towards disconnected extremes and maximum connectivity of intermediate values. The destructuring of indicators away from the median is symmetric. The \gls{NMR} framework is able to overcome this challenge through the use of a well-designed Gaussian pool and $\omega$ bounds. Factor 1 is a highly anisotropic factor, oriented north-south direction and Factor two is isotropic with a range off $\approx \frac{1}{2}$ the domain size. The factors are activated using the \gls{MPL} with $\omega_{1}=4$ emphasizing the high values and $\omega_{2}=1/\omega_{1}$ emphasizing the low values, followed by linear combination (Equation \ref{eq:fpass1}) and normal score transform. The top row of Figure \ref{fig:pool_asymmetry} shows the factors in activated units (left and center), and the normal score transform of the linear mixture (right). The mixture model is univariate Gaussian but not multivariate Gaussian. Note the difference in activation units. The bottom row shows the 0.1 (black) and 0.9 (red) indicator variograms of the final \gls{NMR} mixture in the north-south and east-west directions. The longer range, more-isotropic, low-grade continuity is preserved through factor two and $\omega < 1$, and the highly-anisotropic, higher-grade continuity is preserved through factor one and $\omega > 1$. This small example is illustrative of three key concepts with respect to latent factor design:
\begin{enumerate}[noitemsep]
    \item Latent factors should be designed in conjunction with $\omega$ bounds, targeting specific ranges of continuity, and the final model goals.
    \item The ranges of factors are reduced through mixing. Mixing of factors cannot increase continuity beyond the longest range structures in the pool. Factor two in Figure \ref{fig:pool_asymmetry} has an isotropic variogram range of 64 meters. The range of the 0.1 quantile indicator variogram is reduced to roughly 40 and 25 meters in the north-south and east-west directions, respectively.
    \item Anisotropy ratios of factors are affected through mixing. The anisotropy ratio of factor one is roughly unchanged in Figure \ref{fig:pool_asymmetry} (4:1), however the anisotropy ratio of factor two is increased from 1:1 to $\approx$ 1.6:1. In general, isotropic factors will tend to become more anisotropic, and anisotropic factors will become more isotropic.
\end{enumerate}

\begin{figure}[!htb]
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/f1_f2_grid.png}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/asymmetric_ind_vario.png}
    \end{subfigure}
    \caption{The top row shows factor one activated with $\omega_{1}=4$ (left), factor two activated with $\omega_{2}=1/\omega_{1}$ (center), and the normal score transform the mixture (right). The bottom row shows the gridded indicator variograms for the 0.1 quantile (black) and 0.9 quantile (red) in the north-south (left) and east-west (right) directions. }
    \label{fig:pool_asymmetry}
\end{figure}

Overall, the Gaussian pool must be reasonable with respect to the optimization targets, discussed in Section \ref{sec:04paraminfer}. The inferred mapping function is non-unique, so there is no ``correct'' Gaussian pool; however, some latent factor combinations may be unreasonable. For example, one cannot expect a Gaussian pool composed entirely of long-range structures to generate short-range features in the \gls{NMR} model, and vice-versa. Similar is true for factor orientations. If certain spatial features are required in certain orientations, factors should be designed to address this. Factors aimed at generating high-order connectivity features should consider the major orientation of drilling. Factors with different orientations may be combined, for example, a north-south and an east-west factor can generate north-east striking features in the final model. This base-case requirement of ``reasonableness'' motivates variogram decomposition as the initial approach to latent factor design. There is no definitive recipe for latent factor design though the following rules of thumb apply:
\begin{enumerate}[noitemsep]
    \item Decomposition of nested variogram models into their basic components   is a reasonable starting point.
    \item Checking feature reproduction with a traditional simulation algorithm (such as \gls{SGS}) can provide insight into the ``missing'' features to be added.
    \item Add factors based on the conceptual geologic model and end goals of the \gls{NMR} model.
    \item Prune any redundant factors. That is, the pool is only as complex as required.
    \item Anisotropy of factors is reduced through mixing. That is, the \gls{NMR} model is only as anisotropic as the most anisotropic factor.
    \item The range of factors is reduced through mixing. That is, the range of \gls{NMR} model is only as long as the longest range factor.
\end{enumerate}

Once a reasonable Gaussian pool is established, the objective function components can be initialized and the parameter vector $\theta$ can be optimized. The following section discusses details of objective function, optimization algorithm and \gls{NMR} parameter inference.


% \begin{enumerate}[noitemsep]
%     \item Conceptual geologic model
%     \item Indicator asymmetry
%     \item Connectivity of extreme values
%     \item nugget
% \end{enumerate}


\FloatBarrier
\section{Parameter Inference}
\label{sec:04paraminfer}

The \gls{NMR} approximates the mapping function, $\mathcal{F}_{\theta}$, between latent and observed space. This function is parameterized by the unknown vector $\theta$. The unknown parameters are inferred from known features: (1) the observed data, (2) the specified latent pool, and (3) the objective function components. Features (2) and (3) are specified by the user and guide the parameter optimization process. $\theta$ is a vector of $2 \cdot (M+1)$ real values of factor weights and \gls{MPL} exponents:
\begin{equation}
    \theta = \{ a_{0}, \dots, a_{M}, \omega_{0}, \dots, \omega_{M} \}
    \label{eq:theta}
\end{equation}

and is optimized with the heuristic, genetic algorithm \gls{DE} \citep{price2013differential}. A population of candidate solutions is initialized and then evolved in a process mimicking natural selection. Each member of the population has an associated ``fitness'' value, and the fittest members of the population are carried over to subsequent generations. Through multiple generations of mutation and crossover operations, the candidate solutions converge towards a solution that minimizes the objective function. Parameter inference begins by simulating a set of $L$ x $M+1$ unconditional realizations at the input data locations, where $L$ is the number of realizations and $M+1$ is the number of independent factors, including the nugget. These unconditional realizations permit the evaluation of the objective function as $\theta$ is evolved.

\subsection{Differential Evolution}
\label{subsec:04de}

Optimization of network weights uses a gradient-free, heuristic genetic algorithm. Gradient-free methods are typically employed when information about the derivative of the objective function is either costly to obtain or unreliable and noisy \citep{conn2009introduction}. \Gls{DE} is a global stochastic search algorithm that is practical for non-linear, non-differentiable objective functions with a necessarily large search space \citep{rios2013derivativefree}. Any objective function is permissible, which is an advantage of \gls{DE}.

\Gls{DE} is based on natural evolutionary processes where the fittest member of the population survives through a ``natural selection'' process. An initial population of size $NP$ x $D$ is generated by randomly sampling the objective function space within the defined constraints. $NP$ is the number of individuals in the population, and $D$ is the dimensionality of the problem. Each vector from the initial population is passed through the objective function to evaluate its evolutionary ``fitness''. A mutant vector is then generated from the population by adding the scaled difference between two randomly selected vectors to a third randomly selected vector \citep{price2013differential}. A trial vector is generated by recombining the mutant vector with the initial population's current row vector considering a user-defined crossover probability. The trial vector's fitness is compared to the current population vector's fitness in an evolutionary sense. If the trial vector's fitness exceeds the current population vector's, it replaces the current population vector. Each iteration of the algorithm compares all population vectors to a randomly generated trial vector and accepts the trial vector if its fitness is greater than the current vector. The ``surviving'' vectors from each algorithm iteration become the parent vectors or the next iteration population. The following general steps summarize the \gls{DE} algorithm:
\begin{enumerate}[noitemsep]
    \item Initialize counter $i=0$
    \item Initialize a random population of size $j = 1, \dots, NP$
    \item If $i < i_{MAX}$:
          \begin{enumerate}
              \item i = i + 1
              \item For each member of the population, $\mathbf{x}_{j}$:
                    \begin{enumerate}[noitemsep]
                        \item Select $r$ other individuals where $\mathbf{x}_{r} \neq \mathbf{x}_{j}$ and $r$ is unique
                        \item Generate mutant $\mathbf{v}_{j}$
                        \item Generate trial $\mathbf{t}_{j}$ through crossover between $\mathbf{x}_{j}$ and $\mathbf{v}_{j}$
                        \item If $f(\mathbf{t}_{j}) < f(\mathbf{x}_{j})$ replace $\mathbf{x}_{j}$ with $\mathbf{t}_{j}$
                    \end{enumerate}
              \item Finish if $i = i_{MAX}$
          \end{enumerate}
    \item Return $\text{argmin}_{j} \ f(\mathbf{t}_{j})$
\end{enumerate}

\Gls{DE} minimizes the objective function after simulation and scaling of the objective components. A population of candidate vectors (Equation \ref{eq:theta}) is randomly initialized, and each member's fitness is evaluated. A population size of 30-50 is a reasonable balance between sufficient diversity and algorithm runtime though there is no definitive guideline for population size \citep{piotrowski2017review}. Over a specified number of iterations, each member of the population is mutated with other members. Optimization employs a DE/current-to-best/1 mutation strategy. The mutated vector is described by \citep{georgioudakis2020comparative}:
\begin{equation}
    \mathbf{v}_{i} = \mathbf{x}_{i} + F(\mathbf{x}_{best}-\mathbf{x}_{i}) + F(\mathbf{x}_{r1} - \mathbf{x}_{r2})
    \label{eq:mutation}
\end{equation}

Where $r1 \neq r2 \neq i$, $F$ is the scaling factor controlling the amplification of the difference vectors and $\mathbf{x}_{best}$ is the current best vector in the population. The mutation process learns from the current best vector (local searching of the solution space) while exploring the global search space through the randomly selected difference vector. The parameter $F$ is analogous to the learning rate in machine learning problems. A smaller $F$ value will lead to smaller mutation step sizes, and the algorithm will take longer to converge. Larger $F$ values increase the degree of solution exploration but may lead to divergence. \cite{price2013differential} suggests there is no upper limit for $F$, however effective values are almost always $F < 1.0$.

This new mutated ``genetic information'' is crossed over to other population members based on a crossover probability, $CR$ and a binomial crossover scheme. The trial vector is built from two vectors: the mutant and another member of the population. The crossover is described by \citep{price2013differential}:
\begin{equation}
    \mathbf{t}_{i} =
    \begin{cases}
        \mathbf{v}_{i, j}, & \text{ if }rand_{j}(0,1) \leq CR \text{ or } j=j_{rand} \\
        \mathbf{x}_{i, j}, & \text{ otherwise }
    \end{cases}
    \label{eq:crossover}
\end{equation}

Where $\mathbf{t}_{i}$ is the trial vector, $\mathbf{x}_{i, j}$ is the target vector, $j$ denotes the index of the $i^{th}$ member of the population, $rand_{j}(0,1)$ is a uniform random number $\in [0,1]$, and $j_{rand}$ is a random index. For each element in the target vector, if the uniform random number is less than $CR$, the element is copied from the mutant vector $\mathbf{v}_{i, j}$; otherwise, the target element remains. The random index $j_{rand}$ ensures the target vector is not copied completely; at least one element of the mutant vector passes to the trial vector. $CR$ influences the diversity of the evolving population. Larger $CR$ values introduce more variation in the population, resulting in a greater search of the global solution space, while smaller values may lead to stagnation \citep{georgioudakis2020comparative}. Optimization employs a non-linear crossover scheme following the work of \cite{mohamed2014rdel}:
\begin{equation}
    CR = CR_{hi} + (CR_{lo} - CR_{hi}) \cdot (1-i/N)^{4}, \ \ i=1,\dots, N
    \label{eq:f(cross)}
\end{equation}

The idea behind the non-linear crossover scheme is that when population variance is high in early generations, the crossover rate is low. This crossover scheme prevents extreme diversity or potential divergence in the initial iterations. In subsequent generations, the variance of the population decreases as the vectors become similar, approaching the solution. In order to thoroughly explore this more local search space, the crossover should be high, encouraging a diverse population of ``good'' solutions.

\subsection{Objective Function}
\label{subsec:04objfunc}

The parameter vector $\theta$ is optimized heuristically through an objective function and \gls{DE}. This approach is highly flexible as the objective function can contain any number of components. The practitioner provides target features of the final mixture from which a loss or objective value can be calculated for the candidate mixture. The objective value is minimized through successive iterations reproducing the target spatial features. The possible objective function components are: (1) the continuous variogram, (2) indicator variograms, (3) cumulative run length frequencies, and (4) the $n$-point connectivity function. Components may have multiple sub-components for each indicator threshold. Run length frequencies and the $n$-point connectivity function are higher-order multi-point statistics that better characterize non-Gaussian features than the two-point variogram. The objective function comprises a weighted combination of $C$ objective components:
\begin{equation}
    O = \sum_{c=1}^{C} w_{c}O_{c}
    \label{eq:fobj}
\end{equation}

Where $w_{c}$ is the component weight and $O_{c}$ is component objective value. The objective function quantifies how different the desired feature is from the target feature. Weighting the objective function components is required as they may be represented in widely different units. For example a variogram component is expressed in units of variance squared ($\sum [ \gamma_{\text{target}} - \gamma_{\text{realization}}]^{2}$) while cumulative runs is expressed as the number of runs squared ($\sum [ R_{\text{target}} - R_{\text{realization}}]^{2}$), and may be orders of magnitude different. A weighting scheme follows the work of \cite{deutsch1992annealing}. The goal is to have each component contribute equally to the overall objective value where each weight is inversely proportional to the average change of that objective component \citep{deutsch1992annealing}:
\begin{equation}
    w_{c} = \frac{1}{\bar{|\Delta O_{c}|}}
    \label{eq:fobj_wt}
\end{equation}

The average change of the objective component is approximated numerically by averaging the change of $J=1000$ independent forward passes through the network:
\begin{equation}
    \bar{|\Delta O_{c}|} = \frac{1}{J} \sum_{j=1}^{J} | O^{j}_{c} - O_{c}|, \ \ c=1,\dots, C
    \label{eq:fobj_avg}
\end{equation}

Where $\bar{|\Delta O_{c}|}$ is the average change of component $c$, $O^{j}_{c}$ is the updated objective value for iteration $j$ and $O_{c}$ is the initial objective value of that component. Though any number of components can enter the objective function, one should consider contrasting objectives. For example, one cannot expect to closely reproduce continuous and indicator variograms if the principal directions of continuity are orthogonal. It is less clear how the specification of run frequencies or connectivity functions affect variograms, though there are likely confounding factors. The objective value is the \gls{MSE} between the experimental values calculated on the distribution derived from Equation \ref{eq:fpass1} and the initialized target values. The objective function (Equation \ref{eq:fobj}) is evaluated across all simulated realizations and minimized in expectation.

\subsection{Precedence}
\label{subsec:04precedence}

Application of precedence is an optional component of parameter inference. Precedence allows the spatial features of certain latent factors to be prioritized in the mapping function. A particular factor can be given precedence during imputation if one desires its spatial features in certain portions of the grade range. Precedence is achieved by employing a sigmoid weighting function where $s(y) = \frac{1}{1+e^{-y}}$. The weighting function modifies the forward pass (Equation \ref{eq:fpass1}) through the network to:
\begin{align}
    \label{eq:wtpass1}
    \mathbf{x} & = a_{p} \cdot \phi(\mathbf{y}_{p}) + \sum_{m=0}^{M-1} \phi(\mathbf{y}_{m}) \cdot s(\mathbf{y}_{p} \cdot x), \ \ m \neq p \\
    \label{eq:wtpass2}
    \mathbf{z} & = G^{-1}\left( F_{X}\left(\mathbf{x} \right)\right)
\end{align}

Where $a$ is the weight to the factor, subscript $p$ is the factor index with precedence, $\phi(\dots)$ is the power law activation function, $s(\dots)$ is the sigmoid function, and $x$ is a constant $\in [-10, 10]$. The sign and magnitude of $x$ controls what part of the grade range factor $\mathbf{y}_{p}$ influences. If $x<0$ $\mathbf{y}_{p}$ influences high values and $x>0$ influences low values. Figure \ref{fig:sigmoid} shows a weighting function where $x=-1.5$. The y-axis is the weight to the remaining factors for the range of values of $\mathbf{y}_{p}$. In this scenario $\mathbf{y}_{p}$ receives $\approx 95\%$ of the weight when equal to 2.0, $\approx 80\%$ of the weight when equal to 1.0 and so on. As the magnitude of $x$ increases, $s(\dots)$ tends towards a binary step function centered on zero.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/sigmoid.png}
    \caption{Sigmoid weighting function where $x=-1.5$. The x-axis is the range of $\mathbf{y}_{p}$ and the y-axis is the weight to the remaining M-1 factors. }
    \label{fig:sigmoid}
\end{figure}

The sigmoid weighting function allows a certain factor to take precedence where the observed data takes on either high or low values. This increased control is particularly useful for capturing local high- or low-grade continuity with well-designed latent factors.

\subsection{Complete Algorithm}
\label{subsec:04algorithm}

Algorithm \ref{alg:nmropt} summarizes all components of the \gls{NMR} parameter inference workflow. Unconditional simulation of the latent Gaussian pool at the data locations is the first step (lines 2-6). The objective function components are scaled using the unconditional realizations (lines 7-10) prior to heuristically optimizing $\theta$ with \gls{DE} (lines 11-35).

\begin{algorithm}
    \caption{\gls{NMR} parameter inference pseudocode.}\label{alg:nmropt}
    \begin{algorithmic}[1]
        \State simulate unconditional realizations (LU or SGS):
        \For{$\ell = 1, \dots, L$}
        \For{$m = 1, \dots, M$}
        \State simulate $\mathbf{Y^{\ell}_m}$
        \EndFor
        \EndFor
        \State scale objective components:
        \For{$c = 1, \dots, C$}
        \State $w_{c} = \frac{1}{\bar{|\Delta O_{c}|}}$
        \EndFor
        \State initialize population: $pop(pool dim, popsize)$ \Comment{candidate solutions}
        \State calculate fitness of population: $fpop$
        \State $best = minloc(fpop)$ \Comment{index of initial best solution}
        \For{i=1,\dots, its} \Comment{begin DE}
        \For{j=1,\dots, popsize}
        \State $mutant = \Call{mutation}{pop(j)}$ \Comment{best-to-current mutation}
        \State $\theta = \Call{crossover}{mutant}$ \Comment{binomial crossover}
        \State $a, \omega = \Call{vector\_to\_matrices}{\theta}$ \Comment{reshape vector to $a, \omega$ vectors}
        \State $O = 0.0$
        \For{$\ell = 1, \dots, L$}
        \State $z = \Call{network\_forward}{a, \omega}$ \Comment{forward pass through network}
        \State $iz = \Call{indicator\_transform}{z, thresholds}$
        \For{$c = 1, \dots, C$}
        \State $O_{c} = \sum [ \gamma^{\text{target}}_{c} - \gamma^{\text{realization}}_{c}]^{2}$
        \EndFor
        \State $O = \sum_{c=1}^{C}w_{c}\cdot O_{c}$
        \EndFor
        \State $O = O / L$
        \If{$O < fpop(best)$}
        \State $fpop(best) = O$ \Comment{update objective value}
        \State $pop(best) = \theta$ \Comment{retain the trial}
        \State $best = j$ \Comment{track the new best}
        \EndIf
        \EndFor
        \EndFor \Comment{end DE}
    \end{algorithmic}
\end{algorithm}

Inference of $\theta$ is relatively straightforward with only a handful of \gls{DE} parameters, though care must be given to align the design of the latent pool and objective components with the final goals of the \gls{NMR} model. Depending on the modeling scenario, this is the most challenging portion of the workflow.

\FloatBarrier
\subsection{Checks}
\label{subsec:04checks}

The quality of the \gls{NMR} parameter inference is measured based on reproduction of the objective function components. Generally, these components come directly from the observed data, so the inferred parameters reproduce observed geologic features. To highlight the concepts of parameter checking a small \gls{3D} synthetic example with 746 data is generated. The data set is simulated such that there is asymmetric destructuring of the indicator variograms away from the median and connectivity of high-grade values. These features provide a reasonable test case of the application of the \gls{NMR} workflow. Imputation concepts are presented using the same data set in Chapter \ref{ch:05impute}. Figure \ref{fig:synthetic_sections} shows a plan-view and long-section through the synthetic data in Gaussian units. Figure \ref{fig:synthetic_varios} shows experimental variogram points and fitted models for the normal score variable (a), the 0.1 indicator (b) and 0.9 indicator (c). For brevity the median indicator variogram is not shown however the indicator variograms show asymmetric destructuring about the median. Figure \ref{fig:synthetic_npoint} shows the $n$-point connectivity function which approaches zero at five connected steps. The objective function components are the normal score variogram model, the 0.1 indicator variogram model and the 0.9 indicator variograms (Figure \ref{fig:synthetic_varios} (a), (b), and (c)), and the $n$-point connectivity function (Figure \ref{fig:synthetic_npoint}).

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/synthetic_sections.png}
    \caption{Plan-view (left) and long-section oriented $340\degree$ (right) though the synthetic data set. Both sections have a tolerance pf $\pm25$ meters.}
    \label{fig:synthetic_sections}
\end{figure}

\begin{figure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/syn_cont_vario.png}
        \caption{Normal Scores}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/syn_ivario_0_1.png}
        \caption{0.1 Quantile}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/syn_ivario_0_9.png}
        \caption{0.9 Quantile}
    \end{subfigure}
    \caption{Fitted experimental variograms for the normal score variable (a), the 0.1 indicator (b) and 0.9 indicator (c). Note the asymmetric destructuring of the indicator variograms. }
    \label{fig:synthetic_varios}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/synthetic_npoint.png}
    \caption{$N$-point connectivity function above the 0.9 quantile indicator for the synthetic data set.}
    \label{fig:synthetic_npoint}
\end{figure}

The latent factor pool contains two structures: the long-range structure from the 0.1 quantile indicator model and the single structure from the 0.9 quantile indicator model. The goal of the pool is to have the long-range structure spatially control the lower grades and have the short range structure spatially control the highs. Some degree of mixing between the structures permits reproduction of the normal score variogram model. The omega parameters are constrained such that $0.25 < \omega_{1} \leq 1.0$ and $2.0 < \omega_{2} \leq 4.0$. No precedence is given to either factor. The algorithm is run for 1500 \gls{DE} iterations. The primary \gls{NMR} parameter check is reproduction of objective function components. The mapped unconditional realizations, $\mathcal{F}_{\theta}\left(\mathbf{Y}\right)$, must reproduce, on average, the input objective targets. Figure \ref{fig:ex5_ivariogram} shows normal score variogram reproduction for the inferred mapping function. Reproduction is reasonable for shorter lags though there is some deviation at lags beyond where the variogram reaches the sill. Figure \ref{fig:ex5_ivariogram} shows variogram reproduction for the 0.1 (a) and 0.9 (b) quantile indicators. The indicators show a wider band of uncertainty, particularly at shorter lag distances, which is related to data density. Figure \ref{fig:ex5_runs_1} shows $n$-point connectivity reproduction for the 0.9 quantile indicator.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/ex5_variogram.png}
    \caption{Continuous variogram reproduction for 25 realizations. The black line is the variogram model, the red line is the average variogram, and the shaded red area encloses the minimum and maximum variogram values. Left to right are the major, minor and vertical directions, respectively.}
    \label{fig:ex5_variogram}
\end{figure}

\begin{figure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/ex5_ivariogram_0_1.png}
        \caption{0.1 Quantile}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/ex5_ivariogram_0_9.png}
        \caption{0.9 Quantile}
    \end{subfigure}
    \caption{Indicator variogram reproduction for 25 realizations for the 0.1 (a), and 0.9 (b) quantiles. Left to right are the major, minor and vertical directions, respectively. The black line is the variogram model, the red line is the average variogram, and the shaded red area encloses the minimum and maximum variogram values.}
    \label{fig:ex5_ivariogram}
\end{figure}


\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.50, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/ex5_runs_1.png}
    \caption{$N$-point connectivity function reproduction for 25 realizations for the 0.9 quantile indicator. The black line is the target model, the red line is the average experimental value, and the shaded red area encloses the minimum and maximum experimental values.}
    \label{fig:ex5_runs_1}
\end{figure}

At this point the inferred parameters are a reasonable approximation of the mapping function between latent and observed space. The unconditional Gaussian realizations reproduce the objective function components on average when mapped through $\mathcal{F}_{\theta}$. Any deviations between the optimization targets and mapped latent factors will manifest in the final \gls{NMR} realizations as latent factor imputation (Chapter \ref{ch:05impute}) uses $\mathcal{F}_{\theta}$. If present, these deviations are transferred to the imputed data realizations that condition the gridded factor realizations. This same synthetic data configuration is used to check imputed realization in Chapter \ref{ch:05impute}.


% \begin{enumerate}[noitemsep]
%     \item Introduce 3D example here?
%     \item Differential evolution with small example? PDE
%     \item Objective function
%     \item Constraints and precedence
%     \item Sigmoid weighting
%     \item Conflicting objectives
%     \item Checks
% \end{enumerate}


\FloatBarrier
\section{Implementation Details}
\label{sec:04implementd}

The goal of the \gls{NMR} model is to infer the best possible mapping between latent and observed spaces. Given the data configuration, the design of the latent pool and the objective function components there may be uncertainty, both qualitative and quantitative, in the parameter vector $\theta$. This section discusses some practical implementation details of \gls{NMR} parameter inference that may contribute to, or mitigate parameter uncertainty. Topics in this section include the non-uniqueness of the inferred solution, the number of data, the possibility of conflicting objectives, and overall computational considerations.

\FloatBarrier
\subsection{Non-Uniqueness}
\label{subsec:04nonunique}

Solutions to inverse problems are often non-unique. Experimental variograms in the presence of sparse data are inherently uncertain statistics. Considering these facts, it is reasonable to assume that there are multiple (possibly infinite) network configurations and pools of Gaussian factors that approximate the objective function targets. This non-uniqueness emphasizes the importance of considering the conceptual geological model when designing latent factors. Some latent pools are feasible from a numerical perspective, but they should also be geologically reasonable. The following section highlights examples of non-uniqueness in \gls{NMR} parameter inference. Potential non-unique scenarios are summarized as follows:
\begin{enumerate}[noitemsep]
    \item The \gls{NMR} solution may be an arbitrary mixture of factors even if the variogram objective targets exist within the Gaussian pool.
    \item A mix of short and long-range factors can reproduce a medium-range target.
    \item A mix of only short-range factors cannot reproduce a medium or long-range target, and vice versa.
    \item A mix of factors with NS and EW orientations can reproduce a target orientation of NNE, NNW, SSE, SSW, and so on.
\end{enumerate}

Consider the same data configuration from Section \ref{subsec:04checks}. Synthetic data values are simulated using the \gls{NMR} with a known parameter vector, $\theta$. For clarity of the example, only reproduction of the continuous variogram is measured. Different pools of latent factors are mixed to highlight the non-uniqueness scenarios mentioned above. The weight to each factor, $a_{m}$, is interpreted as the relative importance of each factor to the fit of $\theta$. This weight can be compared to the known weight used to simulate the data to get a measure of similarity. If multiple latent pools with varying weights can reproduce the target, the solution is non-unique. Another approach to understand the dependence of each latent factor on $\theta$ is a measure of \gls{PFI} \citep{fisher2019all}. Feature importance can be calculated by permuting the input latent factors and calculating the increase in error in the output of the fitted model. \gls{PFI} is in the same units as the objective function. Features that are pertinent to the network output will show more significant errors when they are permuted. A caveat is that shorter-range structures (more random) will always show less feature importance than longer-range structures (less random) as permutation introduces randomness. Shorter range structures may still contribute to the final fit of the model without showing high permutation feature importance.

Consider the first scenario listed above. The latent pool consists of the long-range structure of the normal score variogram model, the long-range structure of the 0.1 quantile indicator variogram model, the single structure of the 0.9 quantile indicator variogram model, and the nugget effect for $M = 3+1$ latent factors.  Intuition suggests that if that variogram model (or its elemental components) exists within the Gaussian pool, the network will filter all irrelevant factors and weight the important components appropriately. In practice, this is not always the case. Table \ref{tab:optwts_c01} shows the $a$ and $\omega$ elements of $\theta$, plus \gls{PFI} for comparison. Figure \ref{fig:nonunq_c01_variogram} shows the corresponding variogram reproduction. Factor weights show much of the weight is given factors two and three which correspond to the nested structures of the indicator variograms. This is further backed up by the \gls{PFI} which shows factors two and three and most important. Granted there is similarity between the low-grade indicator and normal score variogram models, the resulting variogram of the mixture model may be an arbitrary mixture of latent factors.

\begin{figure}
    \begin{subfigure}{1.\textwidth}
        \centering
        \includegraphics[scale=0.70, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/nonunq_c01_variogram.png}
        \caption{}
        \label{fig:nonunq_c01_variogram}
    \end{subfigure}
    \begin{subtable}{1.\textwidth}
        \centering
        \resizebox{1\width}{!}{\input{0-Tables/optwts_c01.tex}}
        \caption{}
        \label{tab:optwts_c01}
    \end{subtable}
    \caption{Normal score variogram reproduction (a) in the major, minor and vertical directions, respectively, and the components of $\theta$ (b) for the first scenario. PFI=permutation feature importance. }
    \label{}
\end{figure}

The second scenario is intuitive. A mix of short-range and long-range factors can reproduce features with intermediate-range. Again, this scenario is non-unique as any number of factors can be considered and variogram ranges are continuous and essentially unbounded. Consider a pool with two latent structures: one long-range and one short-range. The long-range structure corresponds to the second structure 0.1 quantile indicator variogram scaled by a factor of 1.5 in all directions, and the short range structure is the 0.9 quantile indicator variogram model scale by a factor of 0.7 in all directions. Table \ref{tab:optwts_c02} shows the $a$ and $\omega$ elements of $\theta$, plus \gls{PFI} for comparison. Figure \ref{fig:nonunq_c02_variogram} shows the corresponding variogram reproduction. The mixture of long and short-range structures is able to reproduce the variogram model of intermediate range with each factor receiving similar weight. This concept is also applicable to the orientation of factors in the pool. For example, the mix of factors with east-west and north-south orientations can generate features with intermediate orientation in the final model.

\begin{figure}
    \begin{subfigure}{1.\textwidth}
        \centering
        \includegraphics[scale=0.70, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/nonunq_c02_variogram.png}
        \caption{}
        \label{fig:nonunq_c02_variogram}
    \end{subfigure}
    \begin{subtable}{1.\textwidth}
        \centering
        \resizebox{1\width}{!}{\input{0-Tables/optwts_c02.tex}}
        \caption{}
        \label{tab:optwts_c02}
    \end{subtable}
    \caption{Normal score variogram reproduction (a) in the major, minor and vertical directions, respectively, and the components of $\theta$ (b) for the second scenario. PFI=permutation feature importance. }
    \label{}
\end{figure}

The third scenario is also straightforward. A mix of short-range factors cannot reproduce a longer-range target as there is destructive interference between factors when they are mixed. That is, the range of structures in the final model can only be as long as the longest range structure in the latent pool. The opposite of this is also true; a mix of long-range structures cannot reproduce a short-range target. In this scenario the pool consists of the two nested structures of the normal score variogram model scaled by a factor of 0.3 in all directions. Table \ref{tab:optwts_c02} shows the $a$ and $\omega$ elements of $\theta$, plus \gls{PFI} for comparison. Figure \ref{fig:nonunq_c02_variogram} shows the corresponding variogram reproduction. As expected, the algorithm is unable to converge on an acceptable solution due to poor latent feature design. The algorithm attempts to filter the unnecessary first factor though the pool does not have sufficient flexibility to reproduce the target.

\begin{figure}
    \begin{subfigure}{1.\textwidth}
        \centering
        \includegraphics[scale=0.70, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/nonunq_c03_variogram.png}
        \caption{}
        \label{fig:nonunq_c03_variogram}
    \end{subfigure}
    \begin{subtable}{1.\textwidth}
        \centering
        \resizebox{1\width}{!}{\input{0-Tables/optwts_c03.tex}}
        \caption{}
        \label{tab:optwts_c03}
    \end{subtable}
    \caption{Normal score variogram reproduction (a) in the major, minor and vertical directions, respectively, and the components of $\theta$ (b) for the third scenario. PFI=permutation feature importance. }
    \label{}
\end{figure}

This section presents scenarios to emphasize the non-uniqueness of the \gls{NMR} solution. There are numerous latent pools that can reproduce objective targets. The final example presents a scenario where the design of the pool and the objective function have conflicting objectives. The scenario is designed for illustrative purposes and defies any geologic-based logic. However, it emphasizes the practitioner's role to ensure the latent pool and objective function are sound with respect to the conceptual geology. This issue is amplified as more objective function components are added. The requirement of ``reasonableness'' promotes the decomposition of all variogram targets into an initial latent Gaussian pool.

\FloatBarrier
\subsection{Number of Data}
\label{subsec:04ndata}

The number of data is an important consideration in the \gls{NMR} workflow. Experimental statistics form the basis of the objective function components, and are sensitive to the number of available data. Experimental variograms are an inherently uncertain statistic, particularly in the presence of sparse data \citep{ortiz2002calculation,pardo-iguzquiza2012varboot}. The variogram value at lag vector $\mathbf{h}$ is the mean of the squared differences between data values separated by $\mathbf{h}$. This mean value depends directly on the number of data, $n_{\mathbf{h}}$, entering the calculation, which in turn depends on the data configuration and variogram tolerance parameters. Uncertainty in the experimental variogram points transfers to uncertainty in chosen model parameters, though this is not easily quantifiable without knowledge of the true variogram model. Sequence objective components are calculated downhole and are more sensitive to the total number of drillholes rather than the total number of data. However, the shape of the global distribution of runs or the global $n$-point connectivity function ultimately depends on the number of data.

A synthetic model is simulated to assess the sensitivity of $\theta$ to data spacing, or the total number of available data. The model is simulated on a regular 56 x 56 x 56 m \gls{3D} grid with a resolution of 1 m. The grid is sampled with regular, square data configurations ranging from 3 x 3 m to 20 x 20 m, with 1 m spacing in the vertical direction. Table \ref{tab:syn_data} shows the resampled data configurations and the corresponding number of data. The grid is also sampled at a 1 x 1 m spacing which is a reference distribution for calculating the ``true'' variograms, distributions of runs, and $n$-point connectivity functions. For each data spacing a parameter vector $\theta$ is inferred, and the unconditional latent factors are mapped to observed space. The objective function value is the weighted sum of squared errors (Equation \ref{eq:fobj}) between the mapped latent values and the objective components calculated from the reference distribution. Figure \ref{fig:mse_vs_data_plot} shows the relationship between the square data spacings and the objective function value.

\begin{figure}
    \begin{subtable}{.5\textwidth}
        \centering
        \resizebox{1\width}{!}{\input{0-Tables/dstable.tex}}
        \caption{}
        \label{tab:syn_data}
    \end{subtable}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[scale=0.70, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/mse_vs_data.png}
        \caption{}
        \label{fig:mse_vs_data_plot}
    \end{subfigure}
    \caption{Square data spacings and corresponding numbers of data (a) and corresponding minimum \gls{MSE} values from \gls{NMR} parameter inference(b). The total number of data increases to the right in (b). }
    \label{fig:mse_vs_data}
\end{figure}

The sum of the squared errors is inversely proportional to the total number of data which is the anticipated response. The curve decreases quickly to a spacing of 10 x 10 m or approximately 2000 data. Beyond this spacing the curve is much flatter, but does continue to decrease towards the tightest spacing. This relationship suggests that somewhere between 2000 and 5000 data are enough to provide stable inference of $\theta$. Beyond 5000 data does show slight improvements, though these improvements are negligible when considering the increase in computation time.

\FloatBarrier
\subsection{Computational Considerations}
\label{subsec:04comp}

The NMR objective function is a computationally expensive calculation. This expense is primarily due to experimental variogram calculation. As each population vector is an entirely new network, all experimental variogram pairs must be updated on each iteration. Depending on the data configuration, this updating may account for a significant portion of the algorithm run time. Two straightforward approaches to dealing with runtime are:
\begin{enumerate}
    \item \Gls{PDE}
    \item Maximum number of experimental variogram pairs to consider
\end{enumerate}

Evaluating the objective function for each member of the population in \gls{DE} is an independent task and lends itself to parallelization. \gls{PDE} is a slightly different algorithm than \gls{DE}. In \gls{DE}, the population is updated after evaluating each trial vector (loop on line 15 in Algorithm \ref{alg:nmropt}), so each subsequent mutation includes information from the previous. In \gls{PDE}, the entire population is mutated and evaluated prior to updating the next generation. This operation introduces new genetic information to the population once per generation rather than after every mutation. The second potential speedup comes from setting a maximum number of experimental variogram pairs to consider per lag when evaluating the objective function. Figure \ref{fig:vario_stability_dir1} shows that roughly restricting the maximum number of pairs per lag to $\approx 10000$ yields a stable variogram relative to considering all possible pairs. Restricting the pairs speeds up the objective function calculation, though variogram stability should be checked for each data configuration.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.50, max size={\textwidth}{\textheight}]{./0-Figures/04-Ch4/vario_stability_dir1.png}
    \caption{Sensitivity of experimental variogram points to the total number of pairs per lag.}
    \label{fig:vario_stability_dir1}
\end{figure}

\FloatBarrier
\section{Discussion}
\label{sec:04discuss}

