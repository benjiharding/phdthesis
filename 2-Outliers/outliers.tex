%!TEX root = ../Thesis_example_compile_this.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Outlier Management}
\label{ch:02outlier}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter presents a minor deviation from the \gls{NMR} framework, though the generation of non-Gaussian spatial fields and the presence of extreme values are unequivocally linked. A discussion regarding extreme values and outlier management is pertinent to any methodology focusing on features of the tails of a distribution. Research initially focused on developing objective measures to identify spatial extreme values and best practices for explicitly managing these values, though evolved into a simulation framework with applicability beyond just extreme values. A key idea of the \gls{NMR} is that extreme values and outliers do not require explicit management. This contrasts many standard practices in the mining industry; \cite{dutaut2021new} mention that methods that avoid capping are interesting, but rarely applied in mining applications. The presence of extreme values, and correctly characterizing their spatial distribution is of great importance. This importance warrants methodologies beyond the standard graphical approaches \citep{silva2021classification}, and motivates a holistic approach incorporating both the statistical and spatial components of extreme values.

The following sections present the concepts of outliers and extreme values in a mining context. Nomenclature is first defined, delineating the differences between an outlier and extreme value. These terms have similar connotations, though different meanings when rigorously defined. An overview of outlier management practices in the mining industry is given, including commonly employed tools and methodologies, followed by a review of methodologies from a survey of 125 \gls{NI} 43-101 reports published between 2019 and 2021. Though the \gls{NMR} framework does not require explicit management of extreme values, the practice is omnipresent in the mining industry. For this reason a spatial outlier identification algorithm is developed that considers a data point's degree of ``outlierness'' both from a local neighbourhood perspective, and the global \gls{CDF}. Finally and analytical approach for forecasting extreme values is presented.  Though some assumptions regarding the underlying distribution must be made, the ability to predict the frequency of intersecting extreme values is valuable from a data collection, and risk-qualified decision-making perspective.

\FloatBarrier
\section{Outliers and Extreme Values}
\label{sec:02extreme}

"Outlier" is a general term for an observation that is sufficiently dissimilar to other observations that further investigation is warranted \citep{barnett1984outliers}. Outliers may be random fluctuations of the data generation mechanism (noise), true anomalies, or measurement errors. An extreme value is a value in the tails of the distribution that is believed to be real, but occurring rarely. Extreme values are different from outliers in the sense that all extreme values are possible outliers, but the reverse is not always true \citep{aggarwal2016outlier}. A key distinction here is that an outlier is not necessarily restricted to the tails of a distribution, while extreme values are. Consider the \gls{1D} set of values: $\{1,2,2,50,98,98,99\}$. In the extreme value context mentioned above, 1 and 99 could (weakly) be considered extreme values, while 50 (the average of the values), is definitely not an extreme value. However, in the context of an outlier, the value of 50 is distant, or isolated from the remaining values. Distance or density based outlier detection methods would likely classify 50 as an outlier, which is correct given it is sufficiently dissimilar from the remaining values. This simple but illustrative example adapted from \cite{aggarwal2016outlier} highlights the core differences between outliers and extreme values. That being said, the terms are typically synonymous in the mining industry: one is only interested in outliers if they are also extreme values. Throughout this text the term outlier will refer to abnormal, or extreme data values that are assumed to be in either the upper or lower tail of the distribution.

Geostatistical models are often generated using widely spaced data configurations. The cost of data collection prohibits exhaustive sampling and necessitates statistical inference from limited samples. Spatial prediction with widely spaced data in the presence of extreme values is a long-standing issue in the mining industry \citep{leuangthong2015dealing}. Extreme values may have significant local influence leading to overstated resources and the risk of production shortfalls. Practitioners are presented with difficult decisions for limiting extreme value influence and characterizing their spatial continuity.

Inputs to numerical geologic models consist of observed data, a representative histogram and spatial controls on mineralization. Each of these components presents challenges in the presence of extreme values. Extreme values are often under sampled making inference about their probability of occurrence difficult. The influence of extreme values are often limited in practice through grade capping which could have a significant impact on the final resource. The spatial continuity of extreme values is different than that of the barren or mineralized background. Traditional geostatistical methods are limited in capacity to adapt to both extreme values and asymmetric spatial continuity features.

\FloatBarrier
\subsection{Outlier Detection}
\label{subsec:02dectection}

Outlier detection is applicable in virtually all statistical modeling. Measures of ``inlierness'' or ``outlierness'' are typically based on (1) statistics of observations for the rest of the distribution (parametric or non-parametric); (2) distances (euclidean or non-euclidean) between observations with outliers being ``far'' from neighbours and (3) probability density-based measures where outliers have low densities \citep{li2022ecod}. A comprehensive review of outlier detection methods beyond the scope of this chapter; the reader is referred to \cite{aggarwal2016outlier,pang2022deep,hodge2004survey,wang2019progress} for a review of methodology.

\begin{enumerate}
    \item a challenge is defining/thresholding what is in/out
\end{enumerate}



\FloatBarrier
\section{Mining Industry Practices}
\label{sec:02industry}

The mining industry has no definitive consensus regarding outlier management, and many approaches are developed on an ad hoc basis. Should high grades be capped? Should sub-regions be delineated to isolate higher grades? Should we use decile analysis, cumulative probability plots, cutting curves, coefficient of variation, production reconciliation, arbitrary percentiles, metal at risk, indicator correlations, multiple indicator kriging or no capping at all? If we cap, should it be before or after compositing? All of these questions influence the final metal content of resource predictions. The impact of restricting outliers may be significant depending on the distribution of the available data.

Identifying and managing extreme values or ``outliers'' is essential, particularly concerning heavy-tailed mineral deposits. A practitioner forecasting resources should understand both the statistical influence and spatial distribution of extreme values. Though there are numerous tools for these purposes, there is no generally accepted workflow in the mining industry, and professional judgment guides best practice. Assessing the influence of extreme values with a variety of techniques seems reasonable. The Canadian Institute of Mining, Metallurgy and Petroleum (CIM) mineral resource and mineral reserve best practices summarizes outlier management as \citep{cim2019}:

\blockquote{\textit{``Recognition of the spatial extent of outlier values (a component of grade continuity) should be investigated and a procedure devised for incorporating such data appropriately into an estimate. Procedures including domaining, grade capping (also known as top cutting), spatially restricting the influence of high-grade assays, single and multiple indicator kriging, and Monte Carlo simulation methods all compensate in varying ways for potential overestimation. Regardless of the methodology selected, the Practitioners must provide documentation of the approach selected, along with justification and support for the decision, possibly including reconciliation of estimated block model grades with available production information. Comparisons of the outcome of the different approaches can be useful.'' (pg. 18)}}

\cite{leuangthong2015dealing}, echoed by CIM best practices, break the process of outlier management into three categories: (1) choosing appropriate domains, (2) grade capping and (3) limiting the influence of outliers through the estimation process. This short note focuses on the second category. Grade capping or ``top-cutting'' is a common practice in the mining industry. Grades above a given threshold are reset to that threshold. The general idea is that uncapped grades may lead to unrealistic local estimates adjacent to high-grade composites \citep{nowak2013suggestions}. The ``smearing'' may be significant if sparse data are estimated with kriging. Due to the normal score transform, simulation is somewhat more robust in the presence of outliers though capping may still be required in some cases.

\FloatBarrier
\subsection{Tools for Outlier Evaluation}
\label{subsec:02tools}

Probability plots are ubiquitous in resource estimation. The \gls{CDF} of the variable is plotted against the \gls{CDF} of the normal distribution (log scaling results in a log-normal probability plot). Inflection points may indicate the presence of multiple populations, and gaps in the distribution are typically targeted as potential capping limits. A recent survey of 125 43-101 reports with gold as the primary commodity showed that the cumulative probability plot is the most common tool for assessing outliers and establishing outliers. This prevalence is likely due to (1) the simplicity of the technique and the ease of implementation and (2) the historical prevalence of the technique in the mining industry.

\begin{figure}[!htb]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/outlier_ex2_cpp.png}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/outlier_ex2_cpp_tukey.png}
    \end{subfigure}
    \caption{}
    \label{fig:cpp}
\end{figure}

Decile analysis or the Parrish method \citep{parrish1997geologist} assesses the metal content of each decile of the grade distribution. The assay population is sorted and arranged into deciles containing equal samples; the upper decile will likely contain less data than the rest. The length weighted mean and standard deviation of each decile are calculated as well as metal content (grade value x length). The top decile is then split into percentiles, and the same summary statistics are calculated for each. Table \ref{tab:parrish_decile_ex2} shows an example decile table for a synthetic log‐normal‐like distribution. The general rules proposed by \cite{parrish1997geologist} are:
\begin{enumerate}[noitemsep]
    \item If the top decile contains more than 40\% of the metal, capping is warranted
    \item If the top decile contains more than twice the metal of the 80-90\% decile, capping is warranted
    \item If the top percentile (or more) contains more than 10\% of the total metal capping is warranted
\end{enumerate}

\begin{table}[!htb]
    \centering
    \caption{Parrish decile analysis for a log-normal distribution with 334 samples. }
    \resizebox{1\width}{!}{\input{0-Tables/parrish_decile_ex2.tex}}
    \label{tab:parrish_decile_ex2}
\end{table}

If the practitioner decides that capping is appropriate, one could reduce the high values in each percentile exceeding 10\% of the total metal to the maximum value of the next lowest percentile. For example, in Table \ref{tab:parrish_decile_ex2}, all values in P99 could be reset to the maximum value of 33.66 in P98. However, P98 contains 8.3\% of the total metal, suggesting further capping may be required based on practitioner judgement.

Cutting curves \citep{roscoe1996cutting,leuangthong2015dealing} is another simple tool to assess the average capped grade versus the capping limit. The idea is that the average grade will eventually stabilize as the threshold increases, and a reasonable capping limit exists somewhere near the inflection point. Figure \ref{fig:cutting_cv} (left) shows an example of a cutting curve for a synthetic log-normal-like data set.

% \begin{figure}[htb!]
%     \centering
%     \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/outlier_ex2_cutting_curve.png}
%     \caption{Cutting curve showing the relationship between the top cut grade and the mean of the adjusted data.}
%     \label{fig:outlier_ex2_cutting_curve}
% \end{figure}

\begin{figure}[!htb]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/outlier_ex2_cutting_curve.png}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/outlier_ex2_cumcv.png}
    \end{subfigure}
    \caption{Cutting curve showing the relationship between the top cut grade and the mean of the adjusted data (left) and cumulative \gls{CV} versus sample grade for a log-normal like dataset (right). The cumulative \gls{CV} plot shows a rapid increase at approximately CV=1.25.}
    \label{fig:cutting_cv}
\end{figure}

\cite{parker1991statistical} proposed plotting the cumulative \gls{CV} against the grade distribution in order to identify a point where the influence of high values in the upper tail becomes strong. The data are sorted in descending order, and the cumulative \gls{CV} is calculated in ascending order considering all samples with grades less than or equal to the current sample. A rapid increase in cumulative \gls{CV} characterizes this point where the influence of the upper tail is significant. \cite{parker1991statistical} then breaks the distribution into two parts and fits the upper distribution with a truncated log-normal distribution; estimation is performed without capping the distribution. Figure \ref{fig:cutting_cv} (right) shows an example cumulative \gls{CV} plot of a synthetic log-normal-like data set.

% \begin{figure}[htb!]
%     \centering
%     \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/outlier_ex2_cumcv.png}
%     \caption{Cumulative \gls{CV} versus sample grade for a log-normal like dataset. There is a rapid increase at approximately CV=1.25.}
%     \label{fig:outlier_ex2_cumcv}
% \end{figure}

The influence of suspected outlier values can be investigated by assessing uncertainty in the average grade of the domain through the spatial bootstrap \citep{solow1985bootstrapping}. The procedure proposed by \cite{nowak2013suggestions} involves bootstrapping the grade distribution while leaving out some percentage of the highest data (arbitrarily 2\%) and comparing this expected grade to the uncapped grade. The idea is that if the expected grade with the top 2\% of samples removed is significantly lower than the uncapped grade, capping may be warranted within the domain. Figure \ref{fig:outlier_ex2_mean_uncert} shows an example of 1000 bootstrapped means capped at P98 of the data distribution and the declustered uncapped mean (left) and the bootstrapped distributions for reference (right). The difference between the expected and declustered actual grades suggests the grade distribution within the domain is sensitive to high values in the upper tail. If the expected grade and uncapped grade are similar, this suggests that the upper tail of the distribution has little influence, and capping may not be necessary.


\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/outlier_ex2_mean_uncert.png}
    \caption{Distribution of 1000 bootstrapped mean values with the top 2\% of values removed (left), and the \glspl{CDF} of 50 of those realizations (right). }
    \label{fig:outlier_ex2_mean_uncert}
\end{figure}

Metal-at-risk, summarized in \cite{parker2006}, is a procedure that establishes uncertainty related to the amount of high-grade metal within annual or global production volumes. The deposit can be ``re-drilled'' utilizing Monte Carlo simulation where the total number of samples drawn is equal to the annual production tonnage divided by the number of tonnes per assay in the domain. The assay distribution is sampled randomly with replacement under the assumption that high-grade can occur anywhere in the domain. For each resampled realization, the total metal above a given high-grade cutoff is calculated. The P20 value of this distribution is added to the metal content below the cutoff to calculate a ``risk-adjusted'' metal content. \cite{parker2006} states that theoretically, the mine should exceed the risk-adjusted metal in four out of five periods; however, ``there is additional and largely unquantifiable uncertainty related to the representivity of the sample-grade frequency distribution input to the simulation''. Metal-at-risk can be used as a guide to restrict high-grade samples' influence during estimation or calibrate a capping limit that removes this metal content. An advantage to the metal-at-risk approach versus others is that it accounts for data density and the production volume; as data density increases, metal-at-risk decreases and larger production volumes have less risk than small ones.

\begin{enumerate}
    \item Something about high-yield restrictions
    \item p-grams \cite{nowak2019optimal}
    \item adjusting model to mitigate downside risk
    \item grade decomposition \cite{rivoirard2013topcut}
    \item capping based on duplicates \cite{dutaut2021new}
    \item rank-transform of cross-validation results \cite{babakhani2014geostatistical}
\end{enumerate}


\FloatBarrier
\subsection{Current State of the Art}
\label{subsec:02state}


The following section summarizes 125 NI 43-101 reports published by companies traded on Canadian securities exchanges. For each deposit, the decision to cap or not, the capping methodology if so, and the decision to cap either assays or composites is recorded. For producing operations, whether they are underground or open-pit, is also recorded. Numerous reports documented multiple outlier management strategies. Due to many possible combinations of strategies, each is recorded separately. If one project uses cumulative probability plots and decile analysis, it contributes to the total methodology fraction twice. The left panel of Figure~\ref{fig:piechart} shows the results for capping methodology, and the right panel shows a summary of assays versus composites for those who capped. Unknown refers to reports which stated a grade cap was applied though no methodology is provided.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/piechart.png}
    \caption{Summary of capping methodology (left) and if capped, the data support to which the cap is applied to (right) from 125 NI 43-101 reports. CPP - cumulative probability plot; P99 - $99^{th}$ percentile; CV - coefficient of variation; MIK - multiple indicator kriging; Parrish - decile analysis after \cite{parrish1997geologist}.}
    \label{fig:piechart}
\end{figure}

The most common method is the analysis of \glspl{CPP}. Practitioners look for infection points to identify multiple populations or look for a point where the upper tail ``degrades''. Figure~\ref{fig:cpp} shows an example of four \glspl{CPP} from a gold project in Nevada \citep{fiore2021}. The red points are considered outliers. The choice of threshold is necessarily subjective though gaps and infection points are used to guide selection.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.45, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/cpp.png}
    \caption{Cumulative probability plots with (red points) and without outliers for four gold deposits in Nevada. From \cite{fiore2021} (pg. 123)}
    \label{fig:cpp}
\end{figure}

The second most common methodology is decile analysis developed by \cite{parrish1997geologist}. In this method, the samples are sorted by grade and then grouped into ``deciles'' with roughly equal samples. Typically the uppermost decile does not have a full complement of samples. The top decile is then split into percentiles 90-100 in the same fashion. Minimum, maximum, length weighted mean (applicable to assay data) and the fraction of total metal is calculated for each decile and the upper percentiles. Parrish states that if the top decile contains more than 40\% of the total metal capping is warranted. Furthermore, capping is also warranted if the top decile contains more than twice the metal of the previous decile. If the top percentile (99-100) contains more than 10\% of the total metal capping is also recommended. An example of a decile analysis plot from \cite{cartier2020} is shown in Figure~\ref{fig:decile}.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.45, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/decile_analysis.png}
    \caption{Decile analysis plot (left) and cumulative metal content (right) where the upper decile contains more than 40\% of the total metal. From \cite{cartier2020} (pg. 110).}
    \label{fig:decile}
\end{figure}

Multiple indicator kriging (MIK) is a risk-qualified estimation methodology that can manage highly skewed distributions without a need for grade capping \citep{journel1983nonparametric}. The non-linear transform of the original variable to indicators reduces the influence of extreme values in the upper tail, and practitioners suggest this removes the need for explicit grade capping \citep{pretium2020, ngm2020, tristar2021, cardinal2019}. \cite{pretium2020} describe the logic of not applying a grade cap as:
\blockquote{\textit{``The positive tail of the grade distribution does
        not break down (tail decomposition method) until well into the multi-kilogram per tonne range, and even then, the more data that is collected, the higher the value before tail decomposition. Using a percentile-based approach results in an arbitrary and unjustifiable capping of extreme gold grades.'' (pg. 14-11)}}
Though MIK is more robust concerning extreme values than traditional kriging algorithms, \cite{carvalho2017overview} suggest outlier values should still be managed in the usual (industry best practices) way. \cite{artemis2020} is an example of a project where both an explicit grade cap (CPP) and MIK are employed.

A small proportion of the projects report no grade capping \citep{medgold2021,pasofino2020,eldorado2020}. The justification of no grade capping is based primarily on a relatively low coefficient of variation and no ``tail decomposition'' in the CPPs. An example of a histogram and CPP of gold where capping was deemed unnecessary is shown in Figure~\ref{fig:nocap} \citep{medgold2021}.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.50, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/hist_no_cap.png}
    \caption{Histogram and CPP of gold where capping was deemed unnecessary. From \cite{medgold2021} (pg. 77).}
    \label{fig:nocap}
\end{figure}

Indicator correlation as an outlier management method was developed by R.M. Srivastava \citep{leuangthong2015dealing}. This approach considers the degradation of the spatial correlation of grades above a threshold. For many increasing thresholds, the spatial correlation decreases. The choice of capping limit coincides with the threshold where the correlation is approximately zero. This approach was employed at the Cariboo Gold Project \citep{osiko2020} where the threshold corresponding to 99\% of the indicator variogram sill is taken as the capping limit. Figure~\ref{fig:indicators} shows an example of the indicator variograms.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/indicators.png}
    \caption{Indicator variograms for various grade thresholds. From \cite{osiko2020} (pg. 158)}
    \label{fig:indicators}
\end{figure}

The remaining outlier management strategies are relatively straightforward. Some projects utilize an experience-based CV or percentile threshold. The CV threshold, commonly 2.0, chooses a grade cap such that the remaining population has a CV equal to or less than the threshold. A percentile threshold, commonly the $98^{th}$ or $99^{th}$, sets all grade values greater than $F^{-1}(0.99)$ to that value. Production reconciliation involves an iterative estimation process with a range of grade caps. For each capping threshold, the estimated metal content is reconciled to available production data. The threshold that reconciles best is selected. Reconciliation seems like a reasonable approach, but it assumes that past production is indicative of future production.

Another point of general indecision is capping before or after compositing. In this survey, it is more or less a 50-50 split. One approach is not more correct than the other. The author generally believes it is more appropriate to apply a capping grade after assays have been brought to the same support. It seems logical to compare values that are effectively equally weighted. If much of the assay data is the same length, capping could be applied before compositing.

\FloatBarrier
\section{Spatial Outlier Detection}
\label{sec:02spatial}

The majority of outlier management methodologies employed in the mining industry neglect the spatial component of outliers and focus solely on the univariate distribution (quantiles, decile analysis, \glspl{CPP}). Assessing the univariate distribution is considered a density based approach; if a statistical model is fit the empirical distribution, samples in the tails of the distribution have low density relative to the underlying \gls{PDF}. Inliers exist in high density regions and outliers in low density regions \citep{geron2019hands}. While this is a justifiable line of reasoning, one must also consider the local spatial component of outliers. An extreme value located in a neighbourhood of other high values may be an outlier from a density perspective, but not in the context of its local spatial arrangement. The spatial dimension characterizes the relationships with the local neighbourhood while the density dimension characterizes the global relationship with the remaining data. This discrepancy motivates the development of an outlier identification algorithm that considers both the spatial neighbourhood about the data in question, and its position within the global distribution. This section describes the algorithm components, and provides examples of its use first on a synthetic \gls{1D} example, and then on a real \gls{3D} dataset.

Consider a dataset $\{z(\mathbf{u}_{i}), \ i = 1, \dots, n\}$ where $n$ is the total number of data and $\mathbf{u}_{i}$ is a coordinate vector at the $i^{th}$ location. About location $\mathbf{u}_{i}$, there exists a neighbourhood of $k$ nearest locations $NN(\mathbf{u}_{i}; k)$. This neighbourhood could be defined by a fixed search radius and maximum $k$, or a fixed number $k$. The data $\{z(\mathbf{u}_{j}), \ j = 1, \dots, k, \ j \neq i \}$ define the neighbourhood of samples about location $\mathbf{u}_{i}$. Next, consider a function $m(\mathbf{u}_{i})$ that returns a summary statistic, such as the mean or median, for all data values within the neighbourhood $NN(\mathbf{u}_{i}; k)$. The spatial component of the algorithm compares the data value $z(\mathbf{u}_{i})$ to the value returned from $m(\mathbf{u}_{i})$: $h(\mathbf{u}_{i}) = |z(\mathbf{u}_{i}) - m(\mathbf{u}_{i})|$. The function $m(\mathbf{u}_{i})$ is chosen to be the median value of all samples in the neighbourhood (excluding location $\mathbf{u}_{i}$), weighted by distance from $\mathbf{u}_{i}$. The median is chosen as it is a more robust measure of central tendency in the presence of outliers compared to the mean value. The vector $\{h_{1}, h_{2}, \dots, h_{n}\}$ contains the absolute differences between each data value and the median of the surrounding neighbourhood. The $h_{n}$ values are scaled $\in [0,1]$ where values closer to 1 are the most different from their neighbourhood. This comparison accounts for the local spatial relationship between data values. The choice of $k$ is problem specific. If $k$ is too small the median values will be noisy and not representative of the true local variation. If $k$ is too large the median values will be smooth and also not representative of the true local variation.

The spatial neighbourhood approach is similar to the ``Median Algorithm'' proposed by \cite{chen2008detecting}, however there are some key differences. Firstly, the neighbourhood $NN(\mathbf{u}_{i}; k)$ is determined using covariance based distances rather than euclidean distance. The covariance distance comes from the anisotropy ratios of a robust measure of spatial correlation of $z(\mathbf{u})$. As the traditional semi-variogram is sensitive to the presence of outliers, robust measures of correlation such as the correlogram, normal score variogram, or pairwise relative variogram should be used \citep{babakhani2014geostatistical,drumond2019using}. Covariance based distance ensures the local neighbourhoods are aligned with relevant geologic features. Secondly, the function $f(\mathbf{u}_{i})$ is a weighted statistic, incorporating information about the data configuration and sparseness. The weight given to each sample in the neighbourhood is $w(\mathbf{u}_{j})=\frac{1}{d(\mathbf{u}_{i},\mathbf{u}_{j})^{p}}$.

The second component of the algorithm considers the relationship of each data value to the global distribution. This is achieved by fitting a \gls{GMM} to the univariate distribution with the goal of approximating the underlying \gls{PDF}. \Gls{GMM} models are commonly used for outlier or anomaly detection \citep{geron2019hands,qu2021anomaly}, where data values falling in low density regions of the fitted \gls{GMM} are potential outliers. The details of fitting the \gls{GMM} through application of the \gls{EM} algorithm are not given here; the reader is referred to \cite{mclachlan2019finite} for complete details. After the \gls{GMM} is fit to the univariate data, it is straightforward to estimate the density at any location. The log of the \gls{PDF} is calculated as:
\begin{equation}
    \log p(z_{i}) = \log \left( \sum_{j=1}^{J} \pi_{j} \mathcal{N}(z_{i}|\mu_{j}, \Sigma_{j}) \right)
    \label{eq:logprob}
\end{equation}

Where $z_{i}$ is the $i^{th}$ sample of $z$, $J$ is the number of fitted \gls{GMM} components, $\pi_{j}$ is the weight to the $j^{th}$ component, and $\mathcal{N}(z_{i}|\mu_{j}, \Sigma_{j})$ is the \gls{PDF} of a multivariate Gaussian distribution with mean $\mu_{j}$ and covariance $\Sigma_{j}$. Exponentiation of Equation \ref{eq:logprob} results in an estimate of the \gls{PDF} for each sample. The higher the probability density, the more likely the sample belongs to the fitted distribution. As this calculation results in an estimate of the \gls{PDF} and not a true probability, the values are scaled such that they sum to one. The vector $\{p_{1}, p_{2}, \dots, p_{n}\}$ then contains an estimate of the probability that the $i^{th}$ sample belongs to the fitted distribution. Low probability samples come from low density regions are possible outliers. This result clearly depends on the number of components, $J$, which must be chosen. Practice shows that 2-3 components is reasonable for a \gls{1D} distribution and fitting is generally straightforward. The final outlier measure is then a weighted combination of $h_{n}$ and $p_{n}$ for each data value:
\begin{equation}
    g_{i} = w_{h}*h_{i} + w_{p}*(1-p_{i}), \ i = 1, \dots, n
    \label{eq:outlier}
\end{equation}

Where $w_{h}$ and $w_{p}$ weights to the neighbourhood and density components, respectively, and $w_{h}+w_{p}=1.0$. Consider the \gls{1D} synthetic data in Figure \ref{fig:fx}. The data show cyclicity and a trend with increasing $x$, common properties of earth science data. Random noise drawn from a Gaussian distribution with $\sigma=3.0$ is added to ten samples in the sequence to simulate the presence of outliers. The degree of ``outlierness'' varies across the samples. A variogram model is fit to the original data without noise for a robust measure of spatial continuity. This is a luxury that is not possible with real data; alternatively, a correlogram or pairwise relative variogram could be fit to the noise data. A neighbourhood is constructed considering $k=10$ data after scaling the coordinates according to the fitted variogram model.

\begin{figure}[!t]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/fx.png}
        \caption{}
        \label{fig:fx}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/hx.png}
        \caption{}
        \label{fig:hx}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/px.png}
        \caption{}
        \label{fig:px}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/gx.png}
        \caption{}
        \label{fig:gx}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/gx_detrend.png}
        \caption{}
        \label{fig:gx_detrend}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/spatial_outliers.png}
        \caption{}
        \label{fig:spatial_outliers}
    \end{subfigure}
    \caption{}
    \label{fig:outliers_1d}
\end{figure}

Figure \ref{fig:hx} shows the difference between the data values and weighted neighbourhood median values. The red x's indicate the data with added noise. Except for one noise value, the suspected outliers show moderate to significant differences with their neighbourhoods. Figure \ref{fig:px} shows the $J=3$ \gls{GMM} components fit to the univariate distribution $f(x)$. The solid line is the fitted model and the dashed lines are the individual components. The number of components is chosen based on trial and error, with the goal of minimizing false positives. Again, a luxury not possible with real data, but this process allows the exploration of bounds of reasonable parameters. Figure \ref{fig:gx} shows the outlier measure of Equation \ref{eq:outlier} considering $w_{h}=0.5$ and $w_{p}=0.5$. Values closer to one are more likely to be outliers. Compared to Figure \ref{fig:hx}, additional values (near $x=-10$) have been identified as being discordant with the rest of the population. Removing the trend from $g(x)$ using a \gls{MWA} further elucidates the potential division between inliers and outliers. Figure \ref{fig:gx_detrend} shows the detrended distribution $g^{\prime}(x)$. At this point a threshold must be selected to define the division. The green circles in Figure \ref{fig:gx_detrend} identify all data above the threshold of $g^{\prime}(x)=0.08$. Figure \ref{fig:spatial_outliers} shows the final outliers identified by the algorithm (green circles) with the original data configuration.

The algorithm effectively identifies the noisy data points. Two points remain undetected though the magnitude of the noise is small and there are no drastic intra-neighbourhood changes in these areas. In addition to the noisy data, the algorithm also identifies three additional outliers near $x=-10$. It is possible that a traditional univariate method such as visual examination of a \gls{CPP} would identify outliers in the upper and lower tails of the distribution (i.e. the values flagged in the lower left and upper right corners of Figure \ref{fig:spatial_outliers}), however the outliers in the middle of the distribution would go undetected without considering the spatial component.


\begin{enumerate}
    \item need to consider local and global spatial components
    \item combination of univariate and spatial components
    \item robust vario/correlogram \cite{drumond2019using}
    \item can expand to multivariate outliers by considering Mahalanobis distance
    \item not a replacement, but validates other methodologies
    \item use as part of an ensemble
    \item possibly easier to choose a threshold in other arbitrary units
    \item possible unsupervised clustering of $g(x)$ rather than thresholding
    \item capping grade could be inferred from minimum outlier value?
\end{enumerate}


\FloatBarrier
\section{Analytical Extreme Value Models}
\label{sec:02analytical}

The geological processes that led to the precipitation and preservation of the grades under consideration in a particular deposit are complex and defy a simple deterministic assessment. The processes influence our understanding, but we adopt a statistical model since there is no way to understand the initial and boundary conditions of the non-linear and chaotic processes that led to the deposit under consideration. This section describes a trimodal model for mineralization: (1) mineralized - M, (2) high grade - HG, and (3) extreme high grade - EHG. Figure~\ref{fig:ehg} illustrates this. The three populations overlap, mix and are not exclusive, but we could reasonably define a range that represents them, for example, 0.1 to 1.0 g/t for mineralized (M), 5 to 20 g/t for high grade (HG), and 500+ g/t for extreme high grade (EHG).

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.70, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/EHG.png}
    \caption{Trimodal distribution model of mineralized (M), high-grade HG) and extreme high grade (EHG).}
    \label{fig:ehg}
\end{figure}


The concept of three populations is reasonable. The illustration in Figure~\ref{fig:ehg} appears discrete as three populations; however, the data distribution from this model would appear highly skewed. Considering one highly skewed population may be possible; however, a flexible parametric distribution is not available to satisfy observed data, explain outliers, and avoid unrealistically high grades. Considering more than three populations would be possible; however, it seems reasonable to have M, HG and EHG. Additional intermediate populations would complicate the model and could be grouped into one of the three.

An assumption is that the M and HG are more pervasive while EHG is encountered rarely. The EHG, however, is assumed to have some reasonable thickness within the vein structures. A drill hole intersecting M, HG or EHG would be identified as such. We do not expect many EHG intersections.

% \begin{figure}[htb!]
%     \centering
%     \includegraphics[scale=0.25, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/MHGEHG.png}
%     \caption{Illustrations of M, HG and (barely) EHG from Monterde.}
%     \label{fig:MHGEHG}
% \end{figure}

The model is parameterized by the probability of each population ($P_{M}$, $P_{HG}$,and $P_{EHG}$) and three lognormal distributions defined by mean values ($m_{M}$, $m_{HG}$,and $m_{EHG}$) and variance or standard deviation parameters for each ($\sigma_{M}$, $\sigma_{HG}$,and $\sigma_{EHG}$).  The sum of the proportions is unity:
\[
    P_{M} + P_{HG} + P_{EHG}= 1
\]
The overall mean is defined as:
\[
    m_{Overall} = P_{M} \cdot m_{M}  + P_{HG} \cdot m_{HG} + P_{EHG} \cdot m_{EHG}
\]
The variation of each distribution ($M$, $HG$, and $EHG$) is important and relevant, but the metal and resource are defined mainly by the proportions and mean values. There is a great challenge in inferring the parameters of this model. A key parameter is establishing the metal in the $HG$ population versus the $EHG$ population. A straightforward way to parameterize this is to assume that the metal in the $EHG$ population is a fraction of the metal in the $HG$ population. The fraction is essential to understand the probability of encountering extreme high grades and, ultimately, for spatial prediction. The total metal will be calibrated to historical mining or external information. Considering this fractional model leads to:
\[
    P_{EHG} \cdot m_{EHG} = f \cdot P_{HG} \cdot m_{HG}
\]
\[
    P_{EHG} = \frac{f \cdot P_{HG} \cdot m_{HG}}{m_{EHG}}
\]
The precise value of the fractional metal in $EHG$ versus $HG$ (the $f$ parameter in the equations above) will be inferred from the data if there are enough. If there are too few data, then it could be assumed. For example, we could assume $f=1$ as a reasonable value; we could also perform a sensitivity study.

Combining the equations above for the overall mean:
\begin{align}
    m_{Overall} & = P_{M} \cdot m_{M}  + P_{HG} \cdot m_{HG} + P_{EHG} \cdot m_{EHG} \nonumber                                                                                                  \\
                & = \left(1-P_{HG}-\frac{f \cdot P_{HG} \cdot m_{HG}}{m_{EHG}}\right) \cdot m_{M} + P_{HG} \cdot m_{HG} + \frac{f \cdot P_{HG} \cdot m_{HG}}{m_{EHG}} \ \cdot m_{EHG} \nonumber \\
                & = m_{M} + P_{HG} \cdot \left( -m_{M}-\frac{f \cdot m_{M}}{m_{EHG}}  + m_{HG} + f \cdot m_{HG} \right) \nonumber
\end{align}
So, the proportions of the populations are defined in sequence by the following:
\[
    P_{HG} =
    \frac{m_{Overall} - m_M}
    {
        -m_M \cdot \left( 1+\frac{f \cdot m_{HG}}{m_{EHG}}\right) + m_{HG}(1+f)
    }
\]
\[
    P_{EHG} = \frac{f \cdot P_{HG} \cdot m_{HG}}{m_{EHG}}
\]
\[
    P_{M} = 1 - P_{HG} - P_{EHG}
\]
The mean values of the three populations could be estimated with reasonable confidence. The overall mean could be estimated from historical mining. The fraction of metal in the EHG population relative to the HG population (the $f$ value) is a model parameter that could be inferred from available drilling if enough intersections are available. Given the mean values and $f$ we could infer the proportions of the populations and the contribution to metal from each population.

\FloatBarrier
\subsection{Probability of Drilling EHG}
\label{subsec:02probehg}

The probability of drilling $n$ successive drill holes without encountering EHG could be computed by:
\[
    \left( 1 - P_{EHG} \right)^n
\]
This approach assumes the drill holes are independent, which is reasonable given the spacing of the drill holes. It also assumes the extreme high grade (EHG) will be seen in a drill hole with a significant thickness, that is, the EHG is not sprinkled around in very small nuggets. This assumption is reasonable since if the EHG were at a very small scale, it would be composited with other rock and end up as mineralized (M) or high grade (HG).

Consider $m_{Overall} = 10 g/t$, $m_{M} = 0.1 g/t$, $m_{HG} = 10 g/t$, $m_{EHG} = 1000 g/t$, and $f=1$, that is, there is the same metal in the HG and the EHG.  These numbers appear reasonable given the intersections encountered at epithermal veins systems. Following the calculations described above:
\[
    P_{HG} =
    \frac{10 - 0.1}
    {
        -0.1 \cdot \left( 1+\frac{1 \cdot 10}{1000}\right) + 10(1+1)
    }
    = 0.4975
\]
\[
    P_{EHG} = \frac{1 \cdot 0.4975 \cdot 10}{1000} = 0.0050
\]
\[
    P_{M} = 1 - 0.4975 - 0.0050 = 0.4975
\]
The overall mean of this model could be checked: $m_{Overall} = 0.4975 \cdot 0.1 + 0.4975 \cdot 10 + 0.0050 \cdot 1000 = 10$ as it must.

So, for thirty ($n=30$) drill holes there is an $0.995^{30}=0.86=86\%$ chance of {\em not} intersecting EHG.  To get to a 50\% chance of encountering an EHG drill hole $log(0.5)/log(0.995)=138$ drill holes would be required.


\FloatBarrier
\subsection{Application}
\label{subsec:02application}

The following demonstrates the conceptual trimodal model for extreme high grades applied to an actual data set. The data consists of 61,027 channel samples across a gold-bearing Witwatersrand reef structure with cumulative grade (cmg/t) and thickness (cm) measurements. Gold values (g/t) are back-calculated from the other two measurements. A significant proportion of the samples are considered ``high grade''. The data set is sufficiently dense such that some \emph{valid} extreme values are likely observed, and the parameters of the extreme high-grade model can be inferred with reasonable confidence.

Distributions of cmg/t and Au are shown in Figure~\ref{fig:wits_hist}. The Au histogram is weighted by sample length. The overall distribution is high grade with multiple oz/t assays. Log probability plots are a useful tool for identifying multiple populations and visualizing the upper tail of the distribution. Figure~\ref{fig:wits_log_prob} shows log probability plots for the same two distributions. The Au plot shows a distinct inflection point near 0.1 g/t and tail decomposition around 30 g/t. These may be reasonable thresholds for the initial assessment of mineralized, high grade and extreme high grade.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/wits_hist.png}
    \caption{Histogram of cmg/t (left) and Au g/t (right).}
    \label{fig:wits_hist}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/wits_log_prob.png}
    \caption{Log probability plots of cmg/t (left) and Au g/t (right). Tukey's fences are shown for reference.}
    \label{fig:wits_log_prob}
\end{figure}

Parameterization of the extreme high-grade model requires four mean values (overall, mineralized, high grade and extreme high grade) and the fraction of extreme high-grade metal contributing to the overall high-grade metal. Thresholds must be selected to delineate sub-populations. A sub-sample of the full data set is used to calculate the proportions of each population. 80\% of the data is with held and the distribution of the remaining 20\% is used for inference to simulate a sparse sampling regime. Distribution quantiles are used to determine the thresholds. Mineralized is defined by 0.1-0.9, high grade from 0.95-0.9995 and extreme high grade 0.9999 and above. Though subjective, threshold selection is informed by log probability plots. The 0.1-0.9 quantile range roughly defines the ``inlier'' range based on Tukey's fences. 0.95 roughly defines the break between ``outlier'' and ``far outlier''. 0.9999 roughly defines the point of tail decomposition suggesting extreme values. Thresholds also consider the number of samples within each distribution; we do not expect to encounter many extreme values. Figure~\ref{fig:wits_ehg_loc} shows a location plot of all samples with the corresponding EHG locations highlighted in red.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/wits_ehg_loc.png}
    \caption{Location plot of all samples with EHG locations highlighted in red.}
    \label{fig:wits_ehg_loc}
\end{figure}

A sub-sample of the full data set is used to calculate the proportions of each population, summarized in Table~\ref{tab:ehg_model}. Table~\ref{tab:ehg_metal_prop} summarizes the proportion of metal contributed by each population. The overall mean of the model can be checked:
\begin{align*}
    m_{Overall} & = P_{M} \cdot m_{M}  + P_{HG} \cdot m_{HG} + P_{EHG} \cdot m_{EHG} \\
    1.45        & = 0.9601 \cdot 1.15 + 0.0398 \cdot 8.55 + 0.0001 \cdot 48.35       \\
    1.45        & = 1.45                                                             \\
\end{align*}
\input{./0-Tables/ehg_model.tex}
\input{./0-Tables/ehg_metal_prop.tex}

In this scenario, the fractional EHG component, $f$, is quite small due to a significant number of samples in the HG population. The channel samples are narrow (tens of cm) and likely only sample mineralized material. If the samples were drill core, one would expect more internal dilution, overall lower grades, and a higher $f$ value. Given the proportions, we can calculate the probability of sampling extreme high grade with $n$ successive channel samples or the required number of samples for a $P_n$ probability of sampling EHG. For example, the probability of \emph{not} sampling EHG in 100 channel samples is $(1-0.0001)^{100} = 98.9\%$. In order to get a 50\% chance of observing an EHG sample, $log(0.5)/log(1-0.0001) = 6301$ additional channels are required.

The reasonableness of this model can be checked with a simple simulation study. For a given probability, say 0.1, 0.5 or 0.9, the number of additional samples required to intersect an EHG value with that probability is calculated analytically as above. High resolution simulated realizations of gold using all available data are considered the truth. If we randomly sample the realizations with the calculated number of samples for some number of trials, we can directly observe how many EHG intersections occur. Ten realizations are used for numerical stability. Each realization is sampled 1000 times with calculated number of samples. Table~\ref{tab:sim_props} summarizes the predicted number of samples required to have a 0.1, 0.5 and 0.9 probability of intersecting EHG and the corresponding expected probabilities from resampling. The simulation results closely reproduce the analytical predictions.

\input{./0-Tables/sim_props.tex}

Access to a high resolution ``true'' model is rarely possible in practice. Often when data is sufficiently dense to be considered the truth, mining has already occurred. One does not have the luxury of calibrating their analytical model. Though the application of the proposed analytical model can never be verified in a practical scenario, the example presented highlights that the analytical model \emph{could} be reasonable. The simulation study shows that the model can predict the probability of intersecting EHG with good accuracy. Determining how many data are required to infer model parameters is a topic of future research.

\FloatBarrier
\section{Discussion}
\label{sec:02discuss}

\begin{enumerate}
    \item Considering the context of an outlier is important.
\end{enumerate}