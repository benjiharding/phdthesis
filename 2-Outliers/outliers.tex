%!TEX root = ../Thesis_example_compile_this.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Outlier Management}
\label{ch:02outlier}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter presents some underlying motivation the \gls{NMR} framework. The generation of non-Gaussian spatial fields and the presence of extreme values are linked. Research initially focused on developing objective measures to identify spatial extreme values and best practices for explicitly managing these values. Then it evolved into a simulation framework with applicability beyond extreme values. A key idea of the \gls{NMR} is that extreme values and outliers do not require explicit management. This idea contrasts many standard practices in the mining industry; \cite{dutaut2021new} mention that methods that avoid capping are interesting but rarely applied in mining applications. The presence of extreme values and the correct characterization of their spatial distribution are important. This importance warrants methodologies beyond the standard graphical approaches \citep{silva2021classification} and motivates a holistic approach incorporating both the statistical and spatial components of extreme values.

The following sections present the concepts of outliers and extreme values in a mining context. Nomenclature is first defined, delineating the differences between an outlier and an extreme value. These terms have similar connotations, though different meanings when rigorously defined. An overview of outlier management practices in the mining industry is given, including commonly employed tools and methodologies, followed by a review of methodologies from a survey of 125 \gls{NI} 43-101 reports published between 2019 and 2021. Though the \gls{NMR} framework does not require explicit management of extreme values, the practice is ubiquitous in the mining industry. For this reason, a spatial outlier identification algorithm is developed that considers a data point's degree of ``outlierness'' from a local neighbourhood perspective and the global \gls{CDF}. The final section presents an analytical approach for forecasting extreme values. Though one must make some assumptions regarding the underlying distribution, predicting the frequency of intersecting extreme values is valuable from a data collection and risk-qualified decision-making perspective.

\FloatBarrier
\section{Outliers and Extreme Values}
\label{sec:02extreme}

``Outlier'' is a general term for an observation sufficiently dissimilar to other observations that further investigation is warranted \citep{barnett1984outliers}. Outliers may be random fluctuations of the data generation mechanism (noise), true anomalies, or measurement errors. An extreme value is a value in the tails of the distribution that is believed to be real but occurs rarely. Extreme values are different from outliers in that all extreme values are possible outliers, but the reverse is not always true \citep{aggarwal2016outlier}. A key distinction here is that an outlier is not necessarily restricted to the tails of a distribution, while extreme values are. Consider the \gls{1D} set of values: $\{1,2,2,50,98,98,99\}$. In the extreme value context mentioned above, 1 and 99 could (weakly) be considered extreme values, while 50 (the average) is not an extreme value. However, in the context of an outlier, the value of 50 is distant or isolated from the remaining values. Distance- or density-based outlier detection methods would likely classify 50 as an outlier, which is correct given that it is sufficiently dissimilar from the remaining values. This simple but illustrative example adapted from \cite{aggarwal2016outlier} highlights the core differences between outliers and extreme values. The terms are typically synonymous in the mining industry: one is only interested in outliers if they are also extreme values. Throughout this text, the term outlier will refer to abnormal or extreme data values assumed to be in either the upper or lower tail of the distribution.

Practitioners often generate geostatistical models using widely spaced data configurations. Data collection costs prohibit exhaustive sampling and necessitate statistical inference from limited samples. Spatial prediction with widely spaced data in the presence of extreme values is a long-standing issue in the mining industry \citep{leuangthong2015dealing}. Extreme values may have significant local influence, leading to overstated resources and the risk of production shortfalls. Practitioners face difficult decisions when limiting extreme value influence and characterizing their spatial continuity. Inputs to numerical geologic models consist of observed data, a representative histogram and spatial controls on mineralization. Each of these components presents challenges in the presence of extreme values. Extreme values are often under-sampled, making inferences about their probability of occurrence difficult. The influence of extreme values is often limited in practice through grade capping, which could significantly impact the final resource. The spatial continuity of extreme values differs from that of the barren or mineralized background. Traditional geostatistical methods are limited in capacity to adapt to both extreme values and asymmetric spatial continuity features.

\subsection{Outlier Detection}
\label{subsec:02dectection}

Outlier detection is applicable in virtually all statistical modeling. Measures of ``inlierness'' or ``outlierness'' are typically based on (1) statistics of observations for the rest of the distribution (parametric or non-parametric); (2) distances (euclidean or non-euclidean) between observations with outliers being ``far'' from neighbours and (3) probability density-based measures where outliers have low densities \citep{li2022ecod}. A comprehensive review of outlier detection methods is beyond the scope of this chapter; the reader is referred to \cite{aggarwal2016outlier,pang2022deep,hodge2004survey,wang2019progress,nowak2019optimal,leuangthong2015dealing} for a review of methodology in the mining industry and beyond.

A challenge of many outlier detection techniques is the choice of a threshold to delineate an abnormal measurement. Whether the technique is statistical or proximity-based, a threshold must be chosen to classify samples based on the measure of ``outlierness'' or the outlier score. If the threshold is too restrictive, the algorithm may not identify true outliers, and if it is too relaxed, it will lead to false positives. For simple methodologies, selecting a threshold may have physical meaning, such as a grade distribution. For spatial (multivariate) outlier detection, selecting a proximity-based threshold in a possibly non-euclidean space is non-trivial. The following section details outlier detection methodologies specific to the mining industry; generally, these approaches are univariate and do not consider the spatial or proximity component of outliers. Threshold selection in these cases is subjective but straightforward.

\FloatBarrier
\section{Mining Industry Practices}
\label{sec:02industry}

Identifying and managing extreme values is essential, particularly concerning heavy-tailed mineral deposits and smooth deterministic estimators. A practitioner forecasting resources should understand extreme values' statistical influence and spatial distribution. Though numerous tools exist for these purposes, there is no generally accepted workflow in the mining industry, and professional judgment guides best practices. Assessing the influence of extreme values with a variety of techniques seems reasonable. The Canadian Institute of Mining, Metallurgy and Petroleum (CIM) mineral resource and mineral reserve best practices summarizes outlier management as \citep{cim2019}:

\blockquote{\textit{``Recognition of the spatial extent of outlier values (a component of grade continuity) should be investigated and a procedure devised for incorporating such data appropriately into an estimate. Procedures including domaining, grade capping (also known as top cutting), spatially restricting the influence of high-grade assays, single and multiple indicator kriging, and Monte Carlo simulation methods all compensate in varying ways for potential overestimation. Regardless of the methodology selected, the Practitioners must provide documentation of the approach selected, along with justification and support for the decision, possibly including reconciliation of estimated block model grades with available production information. Comparisons of the outcome of the different approaches can be useful.'' (pg. 18)}}

\cite{leuangthong2015dealing}, echoed by CIM best practices, break the process of outlier management into three categories: (1) choosing appropriate domains, (2) grade capping, and (3) limiting the influence of outliers through the estimation process. Grade capping or ``top-cutting'' is common in the mining industry. Grades above a given threshold are reset to that threshold. The general idea is that uncapped grades may lead to unrealistic local estimates adjacent to high-grade composites \citep{nowak2013suggestions}. The ``smearing'' may be significant if sparse data are estimated with kriging. Due to the normal score transform, simulation is more robust in the presence of outliers, though some cases may still require capping. As point scale realizations reproduce the input \gls{CDF}, directly capping the realizations avoids iterative re-simulation \citep{harding2023probabilistic}.

\subsection{Tools for Outlier Management}
\label{subsec:02tools}

This section discusses commonly used tools for outlier management in the mining industry. These tools are largely qualitative, provide general guidance, and require subjective decision-making from the practitioner. Statistical methods attempt to guide the selection of a capping limit by exploring characteristics of the distribution tails or the relationship between contained metal and high-grade restrictions. Some methodologies, such as the p-gram \citep{nowak2019optimal}, attempt to characterize the spatial continuity of high grades to support restrictions on their area of influence. Simulation-based methods such as metal-at-risk \citep{parker2006} or mean uncertainty \citep{nowak2013suggestions} ``resample'' the deposit to characterize the uncertainty in metal over some number of realizations. This section does not intend to be an exhaustive list of the available tools; however, it presents some of the most frequently used methodologies.

Probability plots are ubiquitous in resource estimation. The variable value is plotted on the x-axis against the corresponding cumulative probability of the normal distribution on the y-axis. Log scaling of the x-axis results in a lognormal probability plot. Inflection points may indicate the presence of multiple populations, and gaps in the distribution are typically targeted as potential capping limits. A survey of 125 43-101 reports with gold as the primary commodity (Section \ref{subsec:02state}) shows that the \gls{CPP} is the most common tool for assessing and classifying outliers. This prevalence is likely due to (1) the simplicity of the technique and the ease of implementation and (2) the historical prevalence of the technique in the mining industry. Figure \ref{fig:cpp} illustrates a log-probability plot for a synthetic positively skewed distribution with some tail decomposition. A proposed capping limit is selected where the upper tail begins to break down. Tukey's fences are a simple non-parametric method to identify outliers where the fence intervals are defined by \citep{tukey1977exploratory}:
\begin{equation}
    [Q_{1}-k(Q_{3}-Q_{1})), Q_{3}+k(Q_{3}-Q_{1}))]
    \label{eq:tukey}
\end{equation}

\lowercase{Where} $Q_{1}$ and $Q_{3}$ are the first and third quartiles, respectively. Generally, $k=1.5$ defines an ``outlier'' while $k=3$ indicates a ``far outlier''.

\begin{figure}[!htb]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/outlier_ex2_cpp.png}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/outlier_ex2_cpp_tukey.png}
    \end{subfigure}
    \caption{\gls{CPP} showing a possible capping threshold based on upper tail decomposition (left), and the same \gls{CPP} showing outlier and far outlier thresholds based on Tukey's fences \citep{tukey1977exploratory}.}
    \label{fig:cpp}
\end{figure}

Decile analysis or the Parrish method \citep{parrish1997geologist} assesses the metal content of each decile of the grade distribution. The assay population is sorted and arranged into deciles containing equal samples; the upper decile will likely contain less data than the rest. The length-weighted mean and standard deviation of each decile are calculated, as well as metal content (grade value x length). The top decile is split into percentiles, and the same summary statistics are calculated for each. Table \ref{tab:parrish_decile_ex2} shows an example decile table for a synthetic positively skewed distribution. The general rules proposed by \cite{parrish1997geologist} are:
\begin{enumerate}[noitemsep]
    \item If the top decile contains more than 40\% of the metal, capping is warranted.
    \item If the top decile contains more than twice the metal of the 80-90\% decile, capping is warranted.
    \item If the top percentile (or more) contains more than 10\% of the total metal capping is warranted.
\end{enumerate}

\begin{table}[!htb]
    \centering
    \caption{Parrish decile analysis for a positively skewed distribution with 334 samples. The data is binned by deciles (0-9) and the upper decile is further split into percentiles (90-99). The Parrish methodology suggests a capping limit of 33.66.}
    \resizebox{1\width}{!}{\input{0-Tables/parrish_decile_ex2.tex}}
    \label{tab:parrish_decile_ex2}
\end{table}


If the practitioner decides that capping is appropriate, one could reduce the high values in each percentile exceeding 10\% of the total metal to the maximum value of the next lowest percentile. For example, in Table \ref{tab:parrish_decile_ex2}, all values in P99 could be reset to the maximum value of 33.66 in P98. However, P98 contains 8.3\% of the total metal, suggesting further capping may be required based on practitioner judgment.

Cutting curves \citep{roscoe1996cutting,leuangthong2015dealing} is another simple tool to assess the average capped grade versus the capping limit. The idea is that the average grade will eventually stabilize as the threshold increases and a reasonable capping limit exists near the inflection point. Figure \ref{fig:cutting_cv} (left) shows an example of a cutting curve for a synthetic positively skewed data set. The proposed capping limit is from the decile analysis.

\begin{figure}[!htb]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/outlier_ex2_cutting_curve.png}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/outlier_ex2_cumcv.png}
    \end{subfigure}
    \caption{Cutting curve showing the relationship between the top cut grade and the mean of the adjusted data (left) and cumulative \gls{CV} versus sample grade for a lognormal-like dataset (right). The cumulative \gls{CV} plot shows a rapid increase at approximately CV=1.25. The capping limit corresponds to the value determined with the Parrish method.}
    \label{fig:cutting_cv}
\end{figure}

\cite{parker1991statistical} proposed plotting the cumulative \gls{CV} against the grade distribution to identify a point where the influence of high values in the upper tail becomes strong. The data are sorted in descending order, and the cumulative \gls{CV} is calculated considering all samples with grades less than or equal to the current sample. A rapid increase in cumulative \gls{CV} characterizes this point where the influence of the upper tail is significant. \cite{parker1991statistical} then breaks the distribution into two parts and fits the upper distribution with a truncated lognormal distribution; estimation is performed without capping the distribution. Figure \ref{fig:cutting_cv} (right) shows an example cumulative \gls{CV} plot of a synthetic log-normal-like data set.

The influence of suspected outlier values can be investigated by assessing uncertainty in the average grade of the domain through the spatial bootstrap \citep{solow1985bootstrapping}. The procedure proposed by \cite{nowak2013suggestions} involves bootstrapping the grade distribution while leaving out some percentage of the highest data (arbitrarily 2\%) and comparing the expected mean to the uncapped mean. If the expected mean with the top 2\% of samples removed is significantly lower than the uncapped mean, capping may be warranted within the domain. Figure \ref{fig:outlier_ex2_mean_uncert} shows an example of 1000 bootstrapped means capped at P98 of the data distribution and the declustered uncapped mean (left) and the bootstrapped distributions for reference (right). The difference between the expected and declustered actual mean suggests that the grade distribution within the domain is sensitive to high values in the upper tail. If the expected and uncapped means are similar, this suggests that the upper tail of the distribution has little influence, and capping may not be necessary.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/outlier_ex2_mean_uncert.png}
    \caption{Distribution of 1000 bootstrapped mean values with the top 2\% of values removed (left), and the \glspl{CDF} of 50 of those realizations (right). }
    \label{fig:outlier_ex2_mean_uncert}
\end{figure}

Metal-at-risk, summarized in \cite{parker2006}, is a procedure that establishes uncertainty related to the amount of high-grade metal within annual or global production volumes. The deposit can be ``re-drilled'' utilizing Monte Carlo simulation, where the total number of samples drawn equals the annual production tonnage divided by the number of tonnes per assay in the domain. The assay distribution is sampled randomly with replacement, assuming that high-grade can occur anywhere in the domain. The total metal above a given high-grade cutoff is calculated for each resampled realization. The P20 value of this distribution is added to the metal content below the cutoff to calculate a ``risk-adjusted'' metal content. \cite{parker2006} states that theoretically, the mine should exceed the risk-adjusted metal in four out of five periods; however, ``there is additional and largely unquantifiable uncertainty related to the representivity of the sample-grade frequency distribution input to the simulation''. Metal-at-risk can be used as a guide to restrict the influence of high-grade samples during estimation or to calibrate a capping limit that removes this metal content. An advantage of the metal-at-risk approach versus others is that it accounts for data density and production volume; as data density increases, metal-at-risk decreases, and larger production volumes have less risk than small ones.

P-grams are a spatial statistic to characterize the continuity of high grades: the average probability that the tail and head of a lag vector $\mathbf{h}$ are both above a cutoff grade \citep{leuangthong2015dealing, nowak2019optimal}. The p-gram value for each lag is the ratio of the number of pairs where both ends of $\mathbf{h}$ are above the cutoff to the number where only the tail is above the cutoff. This average probability decreases as the lag distance increases. The p-gram can highlight a range of continuity for a specified cutoff value; this range guides restrictions to the search parameters for high-grade samples during estimation.

% this paragraph was added to lit review
% Many methodologies have been proposed to circumvent the practice of capping \citep{rivoirard2013topcut, fourie2019limiting,maleki2014capping,costa2003reducing,hawkins1984robust,machado2012field,journel1983nonparametric,parker1991statistical} though they are not frequently employed in the mining industry. More recently, \cite{silva2021classification} proposed methodology for adjusting outlier grades based on Bayesian updating, and \cite{dutaut2021new} propose using an error-free \gls{CV} calculated from coarse duplicate correlation to determine a capping limit. 

Regardless of the methodology, restricting extreme values aims to mitigate downside risk in resource estimates. The following section presents a survey of outlier management strategies published in publicly available technical reports. The survey provides an overview of current practices in the mining industry.

\subsection{Current State of the Art}
\label{subsec:02state}

This section summarizes 125 \gls{NI} 43-101 reports published by companies traded on Canadian securities exchanges between May 2019 and May 2021. For each deposit, the decision to cap or not, the capping methodology if so, and the decision to cap either assays or composites is recorded. For producing operations, whether underground or open-pit, is also recorded. Numerous reports documented multiple outlier management strategies. Due to many possible combinations of strategies, each is recorded separately. If one project uses cumulative probability plots and decile analysis, it contributes to the total methodology fraction twice. The left panel of Figure~\ref{fig:piechart} shows the results for capping methodology, and the right panel shows a summary of assays versus composites for those who capped. Unknown refers to reports that stated a grade cap was applied, though it provides no methodology.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/piechart.png}
    \caption{Summary of capping methodology (left) and, if capped, the data support to which the cap is applied (right) from 125 \gls{NI} 43-101 reports. CPP - cumulative probability plot; P99 - $99^{th}$ percentile; CV - coefficient of variation; MIK - multiple indicator kriging; Parrish - decile analysis after \cite{parrish1997geologist}.}
    \label{fig:piechart}
\end{figure}

The most common method is the analysis of \glspl{CPP}. Practitioners look for infection points to identify multiple populations or look for a point where the upper tail ``degrades''. The choice of threshold is necessarily subjective, though gaps and infection points guide selection. The second most common methodology is decile analysis developed by \cite{parrish1997geologist}.

% Figure~\ref{fig:cpp} shows an example of four \glspl{CPP} from a gold project in Nevada \citep{fiore2021}. The red points are considered outliers. 
% \begin{figure}[htb!]
%     \centering
%     \includegraphics[scale=0.45, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/cpp.png}
%     \caption{Cumulative probability plots with (red points) and without outliers for four gold deposits in Nevada. From \cite{fiore2021} (pg. 123)}
%     \label{fig:cpp}
% \end{figure}

% The second most common methodology is decile analysis developed by \cite{parrish1997geologist}. Parrish states that capping is warranted if the top decile contains more than 40\% of the total metal. Furthermore, capping is also warranted if the top decile contains more than twice the metal of the previous decile. If the top percentile (99-100) contains more than 10\% of the total metal, capping is also recommended.

% In this method, the samples are sorted by grade and then grouped into ``deciles'' with roughly equal samples. Typically, the uppermost decile has a partial complement of samples. The top decile is then split into percentiles 90-100 in the same fashion. Minimum, maximum, length-weighted mean (applicable to assay data), and the fraction of total metal are calculated for each decile and the upper percentiles.

% An example of a decile analysis plot from \cite{cartier2020} is shown in Figure~\ref{fig:decile}.
% \begin{figure}[htb!]
%     \centering
%     \includegraphics[scale=0.45, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/decile_analysis.png}
%     \caption{Decile analysis plot (left) and cumulative metal content (right) where the upper decile contains more than 40\% of the total metal. From \cite{cartier2020} (pg. 110).}
%     \label{fig:decile}
% \end{figure}

\Gls{MIK} is a risk-qualified estimation methodology that can manage highly skewed distributions without needing grade capping \citep{journel1983nonparametric}. The non-linear transform of the original variable to indicators reduces the influence of extreme values in the upper tail, and practitioners suggest this removes the need for explicit grade capping \citep{pretium2020, ngm2020, tristar2021, cardinal2019}. \cite{pretium2020} describe the logic of not applying a grade cap as:

\blockquote{\textit{``The positive tail of the grade distribution does
        not break down (tail decomposition method) until well into the multi-kilogram per tonne range, and even then, the more data that is collected, the higher the value before tail decomposition. Using a percentile-based approach results in an arbitrary and unjustifiable capping of extreme gold grades.'' (pg. 14-11)}}

Though \gls{MIK} is more robust concerning extreme values than traditional kriging algorithms, \cite{carvalho2017overview} suggests outlier values should still be managed in the usual (industry best practices) way. \cite{artemis2020} is an example of a project where both an explicit grade cap (from \gls{CPP} analysis) and \gls{MIK} are employed.

A small proportion of the projects report no grade capping \citep{medgold2021,pasofino2020,eldorado2020}. The justification of no grade capping is based primarily on a relatively low coefficient of variation and no ``tail decomposition'' in the \glspl{CPP}.

% An example of a histogram and CPP of gold where capping was deemed unnecessary is shown in Figure~\ref{fig:nocap} \citep{medgold2021}.
% \begin{figure}[htb!]
%     \centering
%     \includegraphics[scale=0.50, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/hist_no_cap.png}
%     \caption{Histogram and CPP of gold where capping was deemed unnecessary. From \cite{medgold2021} (pg. 77).}
%     \label{fig:nocap}
% \end{figure}

Indicator correlation (analogous to p-grams) is another commonly employed practice. This approach considers the degradation of the spatial correlation of grades above a threshold. For many increasing thresholds, the spatial correlation decreases. The sill of the indicator variogram identifies the range of high-grade influence. This approach was employed at the Cariboo Gold Project \citep{osiko2020} where the range corresponding to 99\% of the indicator variogram sill is the maximum range of influence.

% Figure~\ref{fig:indicators} shows an example of the indicator variograms.
% \begin{figure}[htb!]
%     \centering
%     \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/indicators.png}
%     \caption{Indicator variograms for various grade thresholds. From \cite{osiko2020} (pg. 158)}
%     \label{fig:indicators}
% \end{figure}

The remaining outlier management strategies are relatively straightforward. Some projects utilize an experience-based \gls{CV} or percentile threshold. The \gls{CV} threshold, commonly 2.0, selects a grade cap such that the remaining population has a \gls{CV} equal to or less than the threshold. A percentile threshold, commonly the $98^{th}$ or $99^{th}$, sets all grade values greater than $F^{-1}(0.99)$ to that value. Production reconciliation involves an iterative estimation process with a range of grade caps. The estimated metal content is reconciled to available production data for each capping threshold. The threshold that reconciles best is selected. Reconciliation is a reasonable approach, but it assumes that past production is characteristic of future production.

Another point of general indecision is whether to cap before or after compositing. In this survey, it is more or less a 50-50 split. One approach is not more correct than the other. The author generally believes it is more appropriate to apply a capping grade after assays have been brought to the same support. It seems logical to compare values that are effectively equally weighted. If much of the assay data is the same length, one could cap before compositing.

\FloatBarrier
\section{Spatial Outlier Detection}
\label{sec:02spatial}

Most outlier detection methodologies employed in the mining industry neglect the spatial component of outliers and focus solely on the univariate distribution (quantiles, decile analysis, \glspl{CPP}). Assessing the univariate distribution is considered a density-based approach; if a statistical model is fit to the empirical distribution, samples in the tails of the distribution have low density relative to the underlying \gls{PDF}. Inliers exist in high-density regions and outliers in low-density regions \citep{geron2019hands}. While this is a justifiable line of reasoning, one must also consider the local spatial component of outliers. An extreme value located in a neighbourhood of other high values may be an outlier from a density perspective but not in the context of its local spatial arrangement.

Additionally, one must understand the volume of influence of a potential outlier. Isolated extreme values, or ones with large areas of influence, pose a risk of overestimation \citep{leuangthong2015dealing}. The spatial context of a sample motivates the development of an outlier identification algorithm that considers both the spatial neighbourhood of the sample and its position within the global distribution. The spatial dimension characterizes the relationships within the local neighbourhood, while the density dimension characterizes the global relationship with the remaining data. The following sections describe the proposed algorithm components and provides examples of its use first on a synthetic \gls{1D} example and then on a real \gls{2D} dataset.

\subsection{Methodology}
\label{subsec:02methodology}

Consider a dataset $\{z(\mathbf{u}_{i}), \ i = 1, \dots, n\}$ where $n$ is the total number of data and $\mathbf{u}_{i}$ is a coordinate vector at the $i^{th}$ location. About location $\mathbf{u}_{i}$, there exists a neighbourhood of $k$ nearest locations, $NN(\mathbf{u}_{i}; k)$. This neighbourhood could be defined by a fixed search radius and maximum $k$ or a fixed number $k$. The data $\{z(\mathbf{u}_{j}), \ j = 1, \dots, k, \ j \neq i \}$ define the neighbourhood of samples about location $\mathbf{u}_{i}$. Next, consider a function $m(\mathbf{u}_{i})$ that returns a summary statistic, such as the mean or median, for all data values within the neighbourhood $NN(\mathbf{u}_{i}; k)$. The spatial component of the algorithm compares the data value $z(\mathbf{u}_{i})$ to the value returned from $m(\mathbf{u}_{i})$: $h(\mathbf{u}_{i}) = |z(\mathbf{u}_{i}) - m(\mathbf{u}_{i})|$. The function $m(\mathbf{u}_{i})$ is chosen to be the median value of all samples in the neighbourhood (excluding location $\mathbf{u}_{i}$), weighted by distance from $\mathbf{u}_{i}$. The median is chosen as it is a more robust measure of central tendency in the presence of outliers than the mean. The vector $\{h_{1}, h_{2}, \dots, h_{n}\}$ contains the absolute differences between each data value and the median of the surrounding neighbourhood. The $h_{n}$ values are scaled $\in [0,1]$ where values closer to 1 differ most from their neighbourhood. This comparison accounts for the local spatial relationship between data values. The choice of $k$ is problem-specific. If $k$ is too small, the median values will be noisy and not represent the true local variation. If $k$ is too large, the median values will be smooth and not representative of the true local variation.

The neighbourhood $NN(\mathbf{u}_{i}; k)$ accounts for the area of influence of each sample. The area (in \acrshort{2D}) or volume (in \acrshort{3D}) of influence is calculated using the maximum distance to any sample in the neighbourhood ($r_{max}$ in Figure \ref{fig:spatial_outlier_sketch}):
\begin{align}
    A_{2D}(\mathbf{u}_{i}) & = \pi \cdot r_{max}(\mathbf{u}_{i})^{2}         \\
    A_{3D}(\mathbf{u}_{i}) & = 4 \cdot \pi \cdot r_{max}(\mathbf{u}_{i})^{2}
    \label{eq:aoi}
\end{align}

This measure is only applicable if $NN(\mathbf{u}_{i}; k)$ considers a fixed number $k$ rather than a fixed search radius. A fixed search radius amounts to equal area weighting of each sample. The area directly accounts for the sparseness of the data configuration within the neighbourhood. The anisotropic area values are scaled $\in [0,1]$. Samples located in sparse regions have values closer to one. A sample being geographically isolated does not constitute an outlier, and thus, one must consider the area of influence in conjunction with $h(\mathbf{u}_{i})$.

The spatial neighbourhood approach is similar to the ``Median Algorithm'' proposed by \cite{chen2008detecting}; however, some key differences exist. Firstly, the neighbourhood $NN(\mathbf{u}_{i}; k)$ is determined using covariance-based distances rather than Euclidean distance. The covariance distance comes from the anisotropy ratios of a robust measure of spatial correlation. As the traditional semi-variogram is sensitive to the presence of outliers, one should use robust measures of correlation such as the correlogram, normal score variogram, or pairwise relative variogram \citep{babakhani2014geostatistical,drumond2019using}. Covariance-based distance ensures that the local neighbourhoods align with relevant geologic features. Secondly, the function $f(\mathbf{u}_{i})$ is a weighted statistic, incorporating information about the data configuration and sparseness. The weight given to each sample in the neighbourhood is $w(\mathbf{u}_{j})=\frac{1}{d(\mathbf{u}_{i},\mathbf{u}_{j})^{p}}$ ($d(i,j)$ in Figure \ref{fig:spatial_outlier_sketch}). Accounting for the area of influence further incorporates spatial information.

The second component of the algorithm considers the relationship of each data value within the global distribution. This relationship is quantified by fitting a \gls{GMM} to the univariate distribution to approximate the underlying \gls{PDF}. \Gls{GMM} models are commonly used for outlier or anomaly detection \citep{geron2019hands,qu2021anomaly}, where data values falling in low-density regions of the fitted \gls{GMM} are potential outliers. The details of fitting the \gls{GMM} with the \gls{EM} algorithm are not given here; the reader is referred to \cite{mclachlan2019finite} for more details. After the \gls{GMM} is fit to the univariate data, estimating the density at any location is straightforward. The log of the \gls{PDF} is calculated as:
\begin{equation}
    \log p(z_{i}) = \log \left( \sum_{j=1}^{J} \pi_{j} \mathcal{N}(z_{i}|\mu_{j}, \Sigma_{j}) \right)
    \label{eq:logprob}
\end{equation}

\lowercase{Where} $z_{i}$ is the $i^{th}$ sample of $z$, $J$ is the number of fitted \gls{GMM} components, $\pi_{j}$ is the weight to the $j^{th}$ component, and $\mathcal{N}(z_{i}|\mu_{j}, \Sigma_{j})$ is the \gls{PDF} of a multivariate Gaussian distribution with mean $\mu_{j}$ and covariance $\Sigma_{j}$. Exponentiation of Equation \ref{eq:logprob} results in an estimate of the \gls{PDF} for each sample. The higher the probability density, the more likely the sample belongs to the fitted distribution. As this calculation results in an estimate of the \gls{PDF} and not a true probability, the values are scaled to sum to one. The vector $\{p_{1}, p_{2}, \dots, p_{n}\}$ then contains an estimate of the probability that the $i^{th}$ sample belongs to the fitted distribution. Low-probability samples that come from low-density regions are possible outliers. This result depends on the number of components, $J$, which must be chosen. Practice shows that 2-3 components are reasonable for a \gls{1D} distribution, and fitting is generally straightforward. The final outlier measure is then a weighted combination of $h_{i}$, $A_{i}$, and $p_{i}$ for each data value:
\begin{equation}
    g_{i} = w_{h}*h_{i} + w_{A}*A_{i} + w_{p}*(1-p_{i}), \ i = 1, \dots, n
    \label{eq:outlier}
\end{equation}

\lowercase{Where} $w_{h}$, $w_{A}$, and $w_{p}$ weights to the neighbourhood, radius and density components, respectively, and $w_{h}+w_{A}+w_{p}=1.0$. Figure \ref{fig:spatial_outlier_sketch} shows a sketch of the local and global components to calculate a final outlier measure. The first two components of $g_{i}$ come from the spatial neighbourhood (1) in Figure \ref{fig:spatial_outlier_sketch} while the third component comes from the fitted probability density (2). Thresholding the final outlier measure in Equation \ref{eq:outlier} results in the binary classification of each sample as an inlier or outlier.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/spatial_outlier_sketch.png}
    \caption{A sketch illustrating the spatial and density components of the spatial outlier algorithm. (1) shows an example search neighbourhood with $k=5$ where $d(i,j)$ is the distance between location $\mathbf{u}_{i}$ and $\mathbf{u}_{j}$, and $r_{max}$ is the distance to the furthest neighbour. (2) shows the \gls{PDF} of all samples fitted by a \gls{GMM} with $J=2$ components. (3) shows an example domain with samples classified as inliers (black) and outliers (red); thresholding the outlier measure in Equation \ref{eq:outlier} results in the binary classification of each sample.}
    \label{fig:spatial_outlier_sketch}
\end{figure}

Consider the \gls{1D} synthetic data in Figure \ref{fig:fx}. The data show cyclicity and a trend with increasing $x$, which are common properties of earth science data. Random noise drawn from a Gaussian distribution ($\mu=1.0$, $\sigma=3.0$) is added to ten samples in the sequence to simulate the presence of outliers. The degree of ``outlierness'' varies across the samples. A variogram model is fit to the original data without noise for a robust measure of spatial continuity. Access to the true data values is not possible with real data; alternatively, a correlogram or pairwise relative variogram could be fit to the noisy data. The spatial neighbourhood considers $k=10$ data after scaling the coordinates according to the fitted variogram model.

Figure \ref{fig:hx} shows the difference between the data and weighted neighbourhood median values. The red x's indicate the data with added noise. Except for one noisy value, the suspected outliers show moderate to significant differences with their neighbourhoods. Figure \ref{fig:px} shows the $J=3$ \gls{GMM} components fit to the univariate distribution $f(x)$. The solid line is the fitted model, and the dashed lines are the individual components. The number of components is chosen based on trial and error to minimize false positives. Again, this is not possible with real data, but this process allows exploration of the bounds of reasonable parameters. Figure \ref{fig:gx} shows the outlier measure of Equation \ref{eq:outlier} considering $w_{h}=0.5$, $w_{A}=0.0$ and $w_{p}=0.5$. $w_{A}=0.0$ as all samples are equidistant and thus equally weighted. Values closer to one are more likely to be outliers. Compared to Figure \ref{fig:hx}, the algorithm identifies additional values (near $x=-10$) as discordant with the rest of the population. Removing the trend from $g(x)$ using a \gls{MWA} further elucidates the potential division between inliers and outliers. Figure \ref{fig:gx_detrend} shows the detrended distribution $g^{\prime}(x)$. At this point, a threshold must be selected to define the division. The green circles in Figure \ref{fig:gx_detrend} identify all data above the threshold of $g^{\prime}(x)=0.08$. Figure \ref{fig:spatial_outliers} shows the final outliers identified by the algorithm (green circles) with the original data configuration.

\begin{figure}[!t]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/fx.png}
        \caption{}
        \label{fig:fx}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/hx.png}
        \caption{}
        \label{fig:hx}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/px.png}
        \caption{}
        \label{fig:px}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/gx.png}
        \caption{}
        \label{fig:gx}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/gx_detrend.png}
        \caption{}
        \label{fig:gx_detrend}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/spatial_outliers.png}
        \caption{}
        \label{fig:spatial_outliers}
    \end{subfigure}
    \caption{\textbf{(a)} \gls{1D} function of the coordinate position $x$; \textbf{(b)} the difference between each sample the neighbourhood median, $h(x)$; \textbf{(c)} fitted \gls{GMM} model with $J=3$ components; \textbf{(d)} the combined outlier measure $g(x)$; \textbf{(e)} detrended $g(x)$ measure with flagged outliers; \textbf{(f)} the \gls{1D} function from \textbf{(a)} with outliers identified. The red x's denote the samples with added noise, and the green circles are the samples classified as outliers by the algorithm.}
    \label{fig:outliers_1d}
\end{figure}

The algorithm effectively identifies the noisy data points. Two points remain undetected, though the magnitude of the noise is small, and there are no drastic intra-neighbourhood changes in these areas. In addition to the noisy data, the algorithm identifies three additional outliers near $x=-10$. It is possible that a traditional univariate method such as visual examination of a \gls{CPP} would identify outliers in the upper and lower tails of the distribution (i.e. the values flagged in the lower left and upper right corners of Figure \ref{fig:spatial_outliers}); however, the outliers in the middle of the distribution would go undetected without considering the spatial component.

\subsection{Application}
\label{subsec:02application}

Consider the \gls{2D} spatial distribution of \gls{PGE} in Figure \ref{fig:pge}. There are scattered high values throughout the northwest portion of the domain where the data density is highest; there are also isolated high values in the more sparse regions to the south. These samples are potential outliers in a spatial context. Figure \ref{fig:pge_probplot} shows the \gls{CDF} and \gls{CPP} for the \gls{PGE} distribution. The empirical \gls{CDF} shows a degree of tail decomposition for high values - this is evident in the \gls{CPP}. Tukey's fences \citep{tukey1977exploratory} identify potential outliers in both distribution tails.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/pge.png}
    \caption{Location map of \gls{PGE} sample locations. }
    \label{fig:pge}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/pge_probplot.png}
    \caption{\Gls{CDF} plot and \gls{CPP} for the \gls{PGE} distribution. The green shaded area in the \gls{CPP} are Tukey's fences \citep{tukey1977exploratory}, identifying potential outliers. }
    \label{fig:pge_probplot}
\end{figure}

The methodology from Section \ref{subsec:02methodology} is employed to identify spatial outliers. An experimental correlogram is calculated and fitted with an exponential model to characterize the spatial relationship between sample locations. The neighbourhood $NN(\mathbf{u}_{i}; k)$ is characterized by $k=25$ neighbours and an inverse distance weighting exponent of $p=1.0$. The area of influence for each sample is calculated in \gls{2D} (Equation \ref{eq:aoi}). The \gls{GMM} is fitted to the histogram using $J=2$ components. The final outlier measure is calculated using $w_{h}=0.4$, $w_{A}=0.2$, and $w_{p}=0.4$. The weight to the area of influence is less than the other measures to not overly weight all sparse samples, or samples near the edges of the domain.

The top row of Figure \ref{fig:measures_pge} shows the absolute difference between each sample value and its corresponding neighbourhood. The left column shows values along the east-west direction, and the right column shows the north-south direction. There are some clear outlying samples, though $h(\mathbf{u}_{i})$ does not show a clear delineation between two populations. The second row of Figure \ref{fig:measures_pge} shows each sample's area of influence measure. Row three shows $1-p(\mathbf{u}_{i})$, where values closer to one are in lower-density regions of the distribution. The fourth row shows the final outlier measure. The division between inliers and outliers is not immediately clear, but one could argue there is a grouping of points above the $g(\mathbf{u}_{i})=0.6$ threshold, highlighted by the red dashed lines.

Figure \ref{fig:pge_outliers} shows the identified outliers in the spatial context. The algorithm identifies high and low values: ten above the mean and three below. Potential outliers must be aligned with geologic intuition. There is a clustering of extreme value outliers in the northern part of the domain, where neighbouring samples are medium-grade. An extreme low-grade sample adjacent to an extreme high-grade sample is also flagged. Given the position of each sample in the \gls{CDF} and spatial arrangement, these samples appear to be appropriate candidates for outliers. Three high-grade samples in the sparsely sampled southern portion of the domain are also flagged as outliers. These samples are also appropriate outlier candidates due to their high grades relative to adjacent samples and their potential influence area for each sample. In an estimation paradigm, these samples could influence the grades in many blocks and lead to overestimation. One could derive a capping limit as the minimum grade of the high-grade samples flagged as outliers.

\begin{figure}[t!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/measures_pge.png}
    \caption{Absolute difference between the sample value and corresponding neighbourhood ($1^{st}$ row); area of influence for each sample ($2^{nd}$ row); density measure ($3^{rd}$ row); final outlier measure ($4^{th}$ row) with flagged outliers above the $g(\mathbf{u}_{i})=0.6$ threshold. The left column shows values in the east-west direction, and the right column shows the north-south direction. }
    \label{fig:measures_pge}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/pge_outliers.png}
    \caption{Location map of \gls{PGE} sample locations with flagged outliers.}
    \label{fig:pge_outliers}
\end{figure}

The spatial outlier detection algorithm effectively identifies potential outliers. Measures of ``outierness'' in spatially correlated data must consider both the univariate distribution and the spatial context of the data. The proposed algorithm directly incorporates features of the grade distribution, robust measures of geologic distance, and features of the data configuration, including sparseness. The final outlier measure is practical in the sense that it includes multiple sources of relevant information, being a weighted blend of a proximity-based algorithm and a parametric statistical model (\gls{GMM}) \citep{li2022ecod}. The algorithm could be extended to the multivariate context by considering the Mahalanobis distance between a $K$ dimensional vector of medians and the center of the data \citep{chen2008detecting}. A challenge of the outlier detection algorithm is that an arbitrary threshold must be chosen. The final measure may not have a clear demarcation between inlier and outlier. Using additional outlier detection measures, such as \gls{CPP} analysis, as an ensemble may aid threshold selection. For instance, the outliers identified by Tukey's fences in Figure \ref{fig:pge_probplot} roughly correspond to the spatial outliers, suggesting appropriate parameters. The identified outliers also appear ``reasonable'' from a geologic perspective. Immediately adjacent extreme high and low values warrant further investigation. One could employ an unsupervised clustering method on the final outlier measure to remove subjectivity from the choice of threshold. Outlier detection methodologies should generally form an ensemble, with higher confidence given to samples identified by multiple methods \citep{zimek2018there}. The spatial outlier detection algorithm is another tool in conjunction with those in Section \ref{subsec:02tools} and provides further justification for the choice of capping limit or outlier management strategy.

% \begin{enumerate}
%     \item need to consider local and global spatial components
%     \item combination of univariate and spatial components
%     \item robust vario/correlogram \cite{drumond2019using}
%     \item can expand to multivariate outliers by considering Mahalanobis distance
%     \item not a replacement, but validates other methodologies
%     \item use as part of an ensemble
%     \item possibly easier to choose a threshold in other arbitrary units
%     \item possible unsupervised clustering of $g(x)$ rather than thresholding
%     \item capping grade could be inferred from minimum outlier value?
% \end{enumerate}


\FloatBarrier
\section{Analytical Extreme Value Models}
\label{sec:02analytical}

Many techniques have evolved to restrict the influence of outliers or extreme high grades. No statistical or geostatistical model exists to understand and manage the resource contributions of \gls{EHG}. The validity of such a model could only be established based on many data or bulk mining. The idea is to develop a simple and intuitive model that accommodates the resource contributions of \gls{EHG}. This model could be applied to understand and explain historical mining and to project the possibility of \gls{EHG} mineralization in unmined areas.

\subsection{Methodology}
\label{subsec:02methodehg}

The geological processes that led to the precipitation and preservation of the grades under consideration in a particular deposit are complex and defy a simple deterministic assessment. The processes influence our understanding, but we adopt a statistical model since there is no way to understand the initial and boundary conditions of the non-linear and chaotic processes that led to the deposit under consideration. This section describes a trimodal model for mineralization: (1) \acrfull{M}, (2) \acrfull{HG}, and (3) \acrfull{EHG}. Figure~\ref{fig:ehg} illustrates this. The three populations overlap, mix and are not exclusive, but we could reasonably define a range that represents them, for example, 0.1 to 1.0 g/t for \gls{M}, 5 to 20 g/t for \gls{HG}, and 500+ g/t for \gls{EHG}.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.70, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/EHG.png}
    \caption{Trimodal distribution model of \acrfull{M}, \acrfull{HG} and \acrfull{EHG}.}
    \label{fig:ehg}
\end{figure}

The concept of three populations is reasonable. The illustration in Figure~\ref{fig:ehg} appears discrete as three populations; however, the data distribution from this model would appear highly skewed. Considering one highly skewed population may be possible; however, a flexible parametric distribution is not available to satisfy observed data, explain outliers, and avoid unrealistically high grades. Considering more than three populations would be possible; however, it seems reasonable to have \gls{M}, \gls{HG} and \gls{EHG}. Additional intermediate populations would complicate the model and could be grouped into one of the three.

An assumption is that the \gls{M} and \gls{HG} are more pervasive while \gls{EHG} is encountered rarely. However, the \gls{EHG} is assumed to have some reasonable thickness within geologic structures. A drill hole intersecting \gls{M}, \gls{HG} or \gls{EHG} would be identified as such. We do not expect many \gls{EHG} intersections. The model is parameterized by the probability of each population ($P_{M}$, $P_{HG}$, and $P_{EHG}$) and three lognormal distributions defined by mean values ($m_{M}$, $m_{HG}$, and $m_{EHG}$) and variance or standard deviation parameters for each ($\sigma_{M}$, $\sigma_{HG}$, and $\sigma_{EHG}$). The sum of the proportions is one:
\[
    P_{M} + P_{HG} + P_{EHG}= 1
\]
The overall mean is defined as:
\[
    m_{Overall} = P_{M} \cdot m_{M}  + P_{HG} \cdot m_{HG} + P_{EHG} \cdot m_{EHG}
\]
The variation of each distribution ($M$, $HG$, and $EHG$) is important and relevant, but the metal and resource are defined mainly by the proportions and mean values. There is a great challenge in inferring the parameters of this model. A key parameter is establishing the metal in the $HG$ population versus the $EHG$ population. A straightforward way to parameterize this is to assume that the metal in the $EHG$ population is a fraction of that in the $HG$ population. The fraction is essential to understand the probability of encountering extreme high grades and, ultimately, for spatial prediction. Historical mining or external information guides the fraction of the total metal from $EHG$. Considering this fractional model leads to the following:
\[
    P_{EHG} \cdot m_{EHG} = f \cdot P_{HG} \cdot m_{HG}
\]
\[
    P_{EHG} = \frac{f \cdot P_{HG} \cdot m_{HG}}{m_{EHG}}
\]
If there is enough data, the precise value of the fractional metal in $EHG$ versus $HG$ (the $f$ parameter in the equations above) will be inferred from the data. If there are too few data, it could be assumed. For example, assuming $f=1$ as a reasonable value, a sensitivity study could be performed.

Combining the equations above for the overall mean:
\begin{align}
    m_{Overall} & = P_{M} \cdot m_{M}  + P_{HG} \cdot m_{HG} + P_{EHG} \cdot m_{EHG} \nonumber                                                                                                  \\
                & = \left(1-P_{HG}-\frac{f \cdot P_{HG} \cdot m_{HG}}{m_{EHG}}\right) \cdot m_{M} + P_{HG} \cdot m_{HG} + \frac{f \cdot P_{HG} \cdot m_{HG}}{m_{EHG}} \ \cdot m_{EHG} \nonumber \\
                & = m_{M} + P_{HG} \cdot \left( -m_{M}-\frac{f \cdot m_{M}}{m_{EHG}}  + m_{HG} + f \cdot m_{HG} \right) \nonumber
\end{align}
The proportions of the populations are defined in sequence by the following:
\[
    P_{HG} =
    \frac{m_{Overall} - m_M}
    {
        -m_M \cdot \left( 1+\frac{f \cdot m_{HG}}{m_{EHG}}\right) + m_{HG}(1+f)
    }
\]
\[
    P_{EHG} = \frac{f \cdot P_{HG} \cdot m_{HG}}{m_{EHG}}
\]
\[
    P_{M} = 1 - P_{HG} - P_{EHG}
\]
The mean values of the three populations could be estimated with reasonable confidence. The overall mean could be estimated from historical mining. The fraction of metal in the \gls{EHG} population relative to the \gls{HG} population (the $f$ value) is a model parameter that could be inferred from available drilling if enough intersections are available. Given the mean values and $f$, we could infer the proportions of the populations and the contribution to metal from each population.

\subsection{Probability of Drilling EHG}
\label{subsec:02probehg}

The probability of drilling $n$ successive drill holes without encountering \gls{EHG} could be computed by:
\[
    \left( 1 - P_{EHG} \right)^n
\]
This approach assumes the drill holes are independent, which may or may not be reasonable, given drillhole spacing. It also assumes the \gls{EHG} will be seen in a drill hole with a significant thickness; that is, the \gls{EHG} is not distributed in very small nuggets. This assumption is reasonable since if the \gls{EHG} were at a very small scale, it would be composited with other rock and end up as mineralized (M) or high grade (HG).

Consider $m_{Overall} = 10 g/t$, $m_{M} = 0.1 g/t$, $m_{HG} = 10 g/t$, $m_{EHG} = 1000 g/t$, and $f=1$ that is, there is the same metal in the HG and the \gls{EHG}. These numbers appear reasonable given the intersections encountered at epithermal vein systems. Following the calculations described above:
\[
    P_{HG} =
    \frac{10 - 0.1}
    {
        -0.1 \cdot \left( 1+\frac{1 \cdot 10}{1000}\right) + 10(1+1)
    }
    = 0.4975
\]
\[
    P_{EHG} = \frac{1 \cdot 0.4975 \cdot 10}{1000} = 0.0050
\]
\[
    P_{M} = 1 - 0.4975 - 0.0050 = 0.4975
\]
The overall mean of the model is checked: $m_{Overall} = 0.4975 \cdot 0.1 + 0.4975 \cdot 10 + 0.0050 \cdot 1000 = 10$ as it must. So, for thirty ($n=30$) drill holes there is an $0.995^{30}=0.86=86\%$ chance of {\em not} intersecting \gls{EHG}. To get to a 50\% chance of encountering an \gls{EHG} drill hole $log(0.5)/log(0.995)=138$ drill holes would be required.

\subsection{Application}
\label{subsec:02applicationehg}

The following demonstrates the conceptual trimodal model for extreme high grades applied to an actual data set. The data comprises 61,027 channel samples across a gold-bearing Witwatersrand reef structure with cumulative grade (cmg/t) and thickness (cm) measurements. Gold values (g/t) are back-calculated from the other two measurements. A significant proportion of the samples are considered ``high grade''. The data set is sufficiently dense such that some \emph{valid} extreme values are likely observed, and we can infer the parameters of the extreme high-grade model with reasonable confidence.

Figure~\ref{fig:wits_au_cdf} (left) shows the Au distribution. The overall distribution is high grade with multiple oz/t assays. Log probability plots are a useful tool for identifying multiple populations and visualizing the upper tail of the distribution. Figure~\ref{fig:wits_au_cdf} (right) shows the log probability plots for Au. Au plot shows a distinct inflection point near 0.1 g/t and tail decomposition around 30 g/t. These may be reasonable thresholds for the initial assessment of mineralized, high grade and extreme high grade.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/wits_au_cdf.png}
    \caption{Au \gls{CDF} with log scaling (left) and log probability plot with Tukey's fences \citep{tukey1977exploratory} (right).}
    \label{fig:wits_au_cdf}
\end{figure}

% \begin{figure}[htb!]
%     \centering
%     \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/wits_log_prob.png}
%     \caption{Log probability plots of cmg/t (left) and Au g/t (right). Tukey's fences are shown for reference.}
%     \label{fig:wits_log_prob}
% \end{figure}

Parameterization of the extreme high-grade model requires four mean values (overall, mineralized, high grade and extreme high grade) and the fraction of extreme high-grade metal contributing to the overall high-grade metal. Thresholds must be selected to delineate sub-populations. A sub-sample of the full data set is used to calculate the proportions of each population. 80\% of the data is withheld, and the distribution of the remaining 20\% is used for inference to simulate a sparse sampling regime. Thresholds are defined based on distribution quantiles. Mineralized is defined by the 0.1-0.9 interquantile range, high grade from 0.95-0.9995, and extreme high grade 0.9999 and above. Though subjective, threshold selection is informed by log probability plots. The 0.1-0.9 quantile range roughly defines the ``inlier'' range based on Tukey's fences \citep{tukey1977exploratory}. 0.95 roughly defines the break between ``outlier'' and ``far outlier''. 0.9999 roughly defines the point of tail decomposition, suggesting extreme values. The thresholds consider the number of samples within each distribution; we do not expect to encounter many extreme values. Figure~\ref{fig:wits_ehg_loc} shows a location plot of all samples with the corresponding \gls{EHG} locations highlighted in red.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/02-Ch2/wits_ehg_loc.png}
    \caption{Location plot of all samples with \gls{EHG} locations highlighted in red.}
    \label{fig:wits_ehg_loc}
\end{figure}

A sub-sample of the full data set is used to calculate the proportions of each population, summarized in Table~\ref{tab:ehg_model}. Table~\ref{tab:ehg_metal_prop} summarizes the proportion of metal contributed by each population. The overall mean of the model can be checked:
\begin{align*}
    m_{Overall} & = P_{M} \cdot m_{M}  + P_{HG} \cdot m_{HG} + P_{EHG} \cdot m_{EHG} \\
    1.45        & = 0.9601 \cdot 1.15 + 0.0398 \cdot 8.55 + 0.0001 \cdot 48.35       \\
    1.45        & = 1.45                                                             \\
\end{align*}
In this scenario, the fractional \gls{EHG} component, $f$, is quite small due to a significant number of samples in the HG population. The channel samples are narrow (tens of cm) and likely only sample mineralized material. If the samples were drill core, one would expect more internal dilution, lower grades, and a higher $f$ value. Given the proportions, we can calculate the probability of sampling extreme high grade with $n$ successive channel samples or the required number of samples for a $P_n$ probability of sampling \gls{EHG}. For example, the probability of \emph{not} sampling \gls{EHG} in 100 channel samples is $(1-0.0001)^{100} = 98.9\%$. In order to get a 50\% chance of observing an \gls{EHG} sample, $log(0.5)/log(1-0.0001) = 6301$ additional channels are required.

\input{./0-Tables/ehg_model.tex}
\input{./0-Tables/ehg_metal_prop.tex}

The reasonableness of this model can be checked with a simple simulation study. For a given probability, say 0.1, 0.5 or 0.9, the number of additional samples required to intersect an \gls{EHG} value with that probability is calculated analytically as above. High-resolution simulated realizations of gold using all available data are considered the truth. If we randomly sample the realizations with the calculated number of samples for some number of trials, we can directly observe how many \gls{EHG} intersections occur. Ten realizations are used for numerical stability. Each realization is sampled 1000 times with the calculated number of samples. Table~\ref{tab:sim_props} summarizes the predicted number of samples required to have a 0.1, 0.5 and 0.9 probability of intersecting \gls{EHG} and the corresponding expected probabilities from resampling. The simulation results closely reproduce the analytical predictions.

\input{./0-Tables/sim_props.tex}

Access to a high-resolution ``true'' model is rarely possible in practice. Often, when data is sufficiently dense to be considered the truth, mining has already occurred. One does not have the luxury of calibrating their analytical model. Though applying the proposed analytical model is unverifiable in a practical scenario, the example presented highlights that the analytical model \emph{could} be reasonable. The simulation study shows that the model can accurately predict the probability of intersecting \gls{EHG}. Determining how much data is required to infer model parameters is a topic of future research.

\FloatBarrier
\section{Discussion}
\label{sec:02discuss}

Outlier management is an important component of the traditional resource estimation paradigm, particularly with precious metals. Using smooth estimators, such as kriging, in the presence of unadjusted extreme values poses a risk of overestimation. High grades in sparse data configurations exacerbate this risk. The selection of grade caps is a long-standing issue; high-grade values are important from an economic perspective, though they are problematic to resource estimation. There is no definitive recipe for outlier management. There is a necessarily subjective threshold choice if one decides to cap. The capping strategies outlined in this chapter provide general guidance; however, no definitive metric exists. One must first assess that the extreme values are valid. If possible, one should address high grades with domain boundaries. A capping strategy may be considered if there is little continuity between high grades. Best practice suggests considering multiple strategies as an ensemble and determining a consistent grade cap across multiple methods.

Many outlier detection methodologies do not consider the spatial context of the samples. To overcome this, a spatial outlier detection algorithm is proposed, which considers the spatial configuration and the probability density of each sample in the distribution. The idea that outliers fall in low-density regions is only sometimes true. The context of each sample within its local neighbourhood should influence the decision to classify an outlier. If an extreme value is discordant within a geologically-driven neighbourhood, it should be flagged as an outlier. An extreme value near other high values is likely a true feature of the underlying distribution. High-grade samples with large areas of influence should also be flagged. The algorithm effectively identifies outliers in a real \gls{2D} example with variable data density across the domain. Multiple features of the sample distribution are combined and projected into a feature space where the threshold selection may be more intuitive than directly selecting a grade value. High-grade samples flagged as outliers permit the inference of a capping limit: the minimum grade of the high-grade outliers. The algorithm provides the practitioner with another tool in the ensemble to guide and detect outliers.

An analytical extreme value model is developed to understand the contributions of \gls{EHG} on model resources. The effect of extreme values is significant from an economic perspective. The \gls{EHG} model allows the practitioner to predict the frequency of intersecting extreme values, which is powerful in the context of drillhole planning or designing data collection schemes. The analytical approach is limited in that historic mining or dense drilling is required to parameterize the model. However, a simulation study with the Witwatersrand data set shows the model can correctly predict the occurrence of \gls{EHG}.

The remaining chapters in this thesis are dedicated to developing the \gls{NMR} framework. The \gls{NMR} explicitly accounts for the spatial features of extreme values through high-order connectivity metrics. If the high-order features are correct, the data does not require explicit capping or outlier management. For this reason, outlier management is not discussed beyond this chapter.


% \begin{enumerate}
%     \item Considering the context of an outlier is important.
%     \item need to consider local and global spatial components
%     \item combination of univariate and spatial components
%     \item robust vario/correlogram \cite{drumond2019using}
%     \item can expand to multivariate outliers by considering Mahalanobis distance
%     \item not a replacement, but validates other methodologies
%     \item use as part of an ensemble
%     \item possibly easier to choose a threshold in other arbitrary units
%     \item possible unsupervised clustering of $g(x)$ rather than thresholding
%     \item capping grade could be inferred from minimum outlier value?
% \end{enumerate}