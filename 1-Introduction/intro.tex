%!TEX root = ../Thesis_example_compile_this.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{ch:01intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The field of geostatistics encompasses the problem of spatial prediction and the characterization of uncertainty within geological systems \citep{deutsch1992geostatistical}. We are concerned with describing the spatial aspects of mineralization and its variability or dispersion. Geostatistics was developed based on the need to forecast the recoverable resources at unsampled locations in mineral deposits \citep{matheron1963principles}. Though initially developed in a mining context, geostatistics has found practical uses in many fields concerned with spatially correlated data, such as petroleum, hydrogeology, environmental science, remote sensing and others \citep{goovaerts1997geostatistics}.

Geostatistics utilizes observed categorical and continuous properties to generate exhaustive numerical models of the subsurface. These models are either deterministic or probabilistic, where equiprobable realizations are generated through stochastic simulation \citep{chiles2012geostatistics}. These realizations honour the spatial and multivariate characteristics of the input data with statistical fluctuations and provide a measure of joint uncertainty within the region of interest \citep{rossi2013mineral}. Characterizing and quantifying geologic uncertainty gives engineers and decision-makers practical tools for optimizing orebody extraction.

Mineral deposits, particularly precious metals, often exhibit strongly positively skewed grade distributions. These distributions pose challenges for spatial prediction as there is usually limited data characterizing the upper tail. Some components of the high values in the upper tail characterize ``outliers'' based on a subjective threshold. There is a risk of local overestimation with smooth kriging estimators if sparse, high-value data are left unmanaged \citep{leuangthong2015dealing}. Standard practice in mining is to cap high values to a maximum to avoid local conditional bias, but those high values may have tremendous economic value. Appreciating the potential and the upside of such values in a quantitative and repeatable manner is of great practical interest. The term ``extreme value'' is not regularly used in the mining industry; rather, ``outlier'' is used. Some mineral distributions with high coefficients of variation likely do contain extreme values in the classic statistical sense. A mineral deposit is an extreme value in the context of regional geology.


% Extreme value theory (EVT) is a statistical foundation for quantifying probability distributions and magnitudes of atypically high or low events. Extreme values are forecast using currently observed extremes \citep{gumbel1958statistics}. Upper or lower tails of the underlying distribution govern the properties of extreme values and their statistics. Classical EVT attempts to characterize the underlying extreme value distribution based on extreme order statistics or isolating values above some high threshold. Extreme value distributions like the generalized extreme value and generalized Pareto distributions apply to domains dealing with time-series data such as meteorology, hydrology, economics and insurance \citep{reiss2007statistical}.


\FloatBarrier
\section{Problem Setting}
\label{sec:01problem}

Mitigating the impact of extreme values on resource estimation is a long-standing issue. Extreme values present unique challenges because there are few samples, and understanding their spatial distribution is difficult. This lack of data is coupled with the potential of significant economic risk if mismanaged. High grades are typically capped in practice to mitigate the risk of overestimation. Numerous problems remain outstanding concerning (1) objectively defining what an extreme value is, (2) explicit approaches for limiting extreme value influence, (3) characterizing the statistical or distribution component of extreme values, (4) characterizing the spatial component of extreme values and (5) develop practical advice to combine the statistical and spatial components.

The mining industry has not established a consensus regarding outlier management, and many approaches are developed case by case. Should high grades be capped? Should sub-regions be delineated to isolate higher grades? Should we use decile analysis, cumulative probability plots, cutting curves, coefficient of variation, production reconciliation, arbitrary percentiles, metal at risk, indicator correlations, multiple indicator kriging or no capping? If we cap, should it be before or after compositing? Answers to these questions influence the final metal content of resource estimates. The impact of restricting outliers may be significant depending on the distribution of the available data.

High-grade values are generally sparse in mining data sets. Though sparse, these high-grade values may contribute significantly to the project's economics. For example, at the Brucejack deposit in Northwest British Columbia, the top 1\% of drill core samples contain 83\% of the total deposit metal (Au) \citep{pretium2020}. Anecdotal evidence from producing gold mines suggests this scenario poses an economic risk if samples in the upper tail are not explicitly managed. The use of smooth deterministic estimators such as kriging potentially exacerbates this issue. Many strongly positively skewed distributions also exhibit non-Gaussian spatial characteristics \citep{journel1989nongaussian}. There is a need for a simulation framework that can correctly characterize the non-Gaussian spatial features related to extreme values without arbitrarily restricting their influence.

The spatial continuity of extreme values in the upper tail may differ from low values. The multivariate Gaussian assumption underlying many geostatistical algorithms does not allow for spatial connectivity of extreme values, nor does it allow for asymmetry in the loss of correlation away from the median \citep{journel1989nongaussian}. \Gls{MIK} \citep{journel1983nonparametric} was conceived for this purpose; however, it has proved difficult to implement effectively in practice, and simpler Gaussian techniques can outperform \gls{MIK} \citep{vincent2021mik}. The challenges associated with \gls{MIK} and traditional extreme value management motivate the need for a new spatial model to characterize the non-Gaussian spatial continuity of extreme values.

\FloatBarrier
\subsection{Thesis Statement}
\label{subsec:01thesis}

A framework for the simulation of continuous variables in the presence of extreme values is proposed to address these challenges. The framework constructs numerical geologic models with explicit consideration for extreme values' presence and spatial structure. These models can better characterize high-grade geologic features by representing a regionalized variable by a non-linear combination of underlying latent Gaussian factors.

The framework consists of a non-linear \acrfull{NMR}, which is an expansion of the \acrfull{LMR} concept \citep{journel1974geostatistics}. Rather than a positive linear combination of latent factors, the \gls{NMR} introduces non-linear activations and a hidden layer to form a network structure. Non-linearity allows the spatial model to capture complex, high-order features and better control the known indicator asymmetry between low and high grades with non-Gaussian distributions \citep{journel1989nongaussian}. High-order measures of connectivity are shown to characterize non-Gaussianity. These connectivity measures are extracted from drill strings and incorporated into the spatial models. Capturing richer spatial structures beyond what is possible with two-point statistics improves the prediction of high-grade in-situ resources. The \gls{NMR} framework is particularly advantageous for strongly positively skewed distributions such as precious metals, uranium or diamonds.

\vspace{\baselineskip}
\begin{tcolorbox}[]
    \textbf{Thesis Statement:} \textit{The breakdown of regionalized variables into fundamental latent components coupled with a non-linear network model of regionalization permits improved probabilistic modeling of strongly positively skewed grade distributions.}
\end{tcolorbox}
\vspace{\baselineskip}

The key contributions of this thesis are the development of:
\begin{enumerate}[noitemsep]
    \item The \gls{NMR} framework for the simulation of high-order spatial features improves the modeling of continuous variables in the presence of extreme values. The framework includes:
          \begin{enumerate}[noitemsep]
              \item Methodology for the parameterization of the network, permitting mapping between latent and observed spaces. This inverse problem is approached through stochastic optimization.
              \item Methodology for stable imputation of latent factors that (1) reproduce the correct spatial statistics and (2) reproduce the observed data values.
              \item A novel activation function to impose spatial features in the tails of the continuous distribution.
          \end{enumerate}
    \item Tools for calculating high-order connectivity measures from drillhole sequences; these connectivity features are a proxy for non-Gaussianity.
    \item An algorithm for identification of outliers in a spatial context. The algorithm considers the spatial arrangement and shape of the global empirical distribution to assign an outlier score.
\end{enumerate}

Little research has been done on the continuous simulation of high-order spatial features without training images. In the \gls{NMR} framework, all high-order features are extracted directly from the observed data with no assumptions made regarding the geological system or underlying physical processes. These \gls{1D} patterns are restricted to the drill strings; however, the network parameters enforce connectivity away from the data. Another key difference is the introduction of the latent Gaussian space. The local conditional \glspl{CDF} are not approximated by a combination of high-order statistical moments but rather calculated directly under a multivariate Gaussian assumption. The non-Gaussian spatial model is constructed as a mixture of Gaussians; the transform from latent to observed space captures the high-order features.


\FloatBarrier
\section{Geostatistical Background}
\label{sec:01geostatreview}

The proposed research covers a wide range of subjects involving modeling continuous, positively skewed variables with geostatistical simulation. The following section reviews the relevant geostatistical concepts. This section does not intend to be exhaustive but rather provide a review of concepts related to the \gls{NMR} framework.

\FloatBarrier
\subsection{Overview}
\label{subsec:01overview}

Geostatistics is a field of applied statistics concerned with characterizing and modeling spatially correlated variables. A variable dispersed in space and exhibiting spatial structure is said to be regionalized \citep{matheron2019matheron}. A foundation of geostatistics is the concept of random variables where unknown values, $z$, at an unsampled location are modeled as outcomes of a random variable $Z$ \citep{deutsch1992geostatistical}. A random function represents a collection of spatially correlated, location-dependent random variables $Z(\mathbf{u})$ for every location $\mathbf{u}$ within the study region \citep{goovaerts1997geostatistics}.

The decision of stationarity is one to group or pool relevant data together. As no data replicates are available at location $\mathbf{u}$ to infer the random function $Z(\mathbf{u})$, geologically similar data must be pooled, permitting reliable inference of population statistics. This decision of stationarity allows the trade of unavailable replicates for data at other locations for statistical inference \citep{deutsch1992geostatistical}. Pooling too little data may lead to unreliable statistics, and too much data may lead to the masking of important geological features. \textit{Stationarity} is a property related to the underlying random function model and cannot be checked or validated with data \citep{goovaerts1997geostatistics}.

Geostatistical estimation and simulation algorithms require inference of the random function's first and second-order moments (mean and covariance). When divided into a sub-region $\mathcal{A}$, the variable of interest is considered first-order stationary if the expected value is constant within $\mathcal{A}$. The variable is second-order stationary if the covariance depends only on the separation vector $\mathbf{h}$ within $\mathcal{A}$. A random function $Z(\mathbf{u})$ is second-order stationary when:
\begin{align*}
    E\{Z(\mathbf{u})\}                                        & = m                                                           \\
    E\{Z(\mathbf{u}) - m^2\}                                  & = C(0) = \sigma^2                                             \\
    E\{Z(\mathbf{u}) \cdot Z(\mathbf{u} + \mathbf{h})\} - m^2 & = C(\mathbf{u}, \mathbf{u}+\mathbf{h}) = C(\mathbf{h})        \\
                                                              & \forall \ \mathbf{u}, \mathbf{u}+\mathbf{h} \ \in \mathcal{A}
\end{align*}

Where $m$, $\sigma^2$ and $C(\mathbf{h})$ are the mean, variance and covariance, respectively, and do not depend on location. The invariance of the random function parameters to a location within $\mathcal{A}$ allows the relation $C(\mathbf{h}) = \sigma^2 - \gamma(\mathbf{h})$ which is the foundation of variogram interpretation \citep{pyrcz2014geostatistical}.

The linear model of regionalization (LMR) is used to fit experimental variograms with models that ensure positive definite covariance matrices. As not all combinations of variogram models may lead to a permissible model, the LMR constructs a random function $Z(\mathbf{u})$ to be the linear combination of $L+1$ independent, standard random functions $\{Y_{\ell}(\mathbf{u}), \ell=0,\dots,L\}$, each with its permissible variogram function \citep{goovaerts1997geostatistics}, plus the stationary mean:
\begin{equation*}
    Z(\mathbf{u}) = \sum_{\ell=0}^{L} a^{\ell} Y^{\ell}(\mathbf{u}) + m(\mathbf{u})
\end{equation*}

Where $L$ is the number of nested structures in the model. By convention, the isotropic nugget effect is the $0^{th}$ structure. The variogram model of can $Z(\mathbf{u})$ then be expressed as the sum of the variograms for each of the factors:
\begin{equation*}
    \gamma_z(\mathbf{h}) = \sum_{\ell=0}^{L} b^{\ell} \Gamma^{\ell}(\mathbf{h}), \ \ \ b^{\ell} = (a^{\ell})^{2}
\end{equation*}

Where $\Gamma_{\ell}(\mathbf{h})$ is the variogram of $Y_{\ell}$ and $b_{\ell}$ represents the variance contribution of each $\ell=0,\dots,L$ factors.

The LMR extends to the multivariate case with $k=1,\dots,K$ coregionalized variables. With the liner model of coregionalization (LMC), each coregionalized random function, $Z_{k}(\mathbf{u})$, is also the sum of the standard, uncorrelated factors:
\begin{equation*}
    Z_{k}(\mathbf{u}) = \sum_{\ell=0}^{L} a^{\ell}_{k} Y^{\ell}(\mathbf{u}) + m_{k}(\mathbf{u})
\end{equation*}

Where $a^{\ell}_{k}$ is the contribution of the $\ell^{th}$ factor the $k^{th}$ variable. The direct and cross variograms can be expressed as:
\begin{equation*}
    \gamma_{k,k^{\prime}}(\mathbf{h}) = \sum_{\ell=0}^{L} a^{\ell}_{k} a^{\ell}_{k^{\prime}} \Gamma^{\ell}(\mathbf{h}), \ \ \ k,k^{\prime} = 1,\dots,K
\end{equation*}
The LMC is commonly modeled in the cokriging paradigm for multivariate covariance inference.

\FloatBarrier
\subsection{Factorial Kriging}
\label{subsec:01factorial}

As demonstrated with the LMR notation, the regionalized variable is characterized by $m(\mathbf{u})$, the $L + 1$ $a_{\ell}$ values and the $L + 1$ variograms $\Gamma_{\ell}(\mathbf{h})$. Factorial kriging aims to model each nested spatial structure present in the LMR for filtering or feature extraction. The idea is that each regionalized factor has a correlation structure responsible for a different scale of continuity, and they can be estimated independently \citep{matheron1982factorial}. The factors are estimated as linear combinations of the data values \cite{deutsch2007recall}:
\begin{equation*}
    z^{\ell*}(\mathbf{u}) = a^{\ell} Y^{\ell}(\mathbf{u}) = \sum_{i=1}^{n} \lambda_{i}^{\ell} z(\mathbf{u}_{i})
\end{equation*}

% \begin{equation*}
%      m^{*}(\mathbf{u}) = \sum_{i=1}^{n} \lambda_{m,i} z(\mathbf{u}_{i})
%  \end{equation*} 

By convention, ordinary kriging is used, though there is no reason simple kriging cannot be used \citep{hong2007improved}. The estimation weights for the $\ell^{th}$ factor are obtained by minimizing the estimation variance, leading to the factorial kriging equations:
\begin{equation*}
    \begin{cases}
        % \sum_{i=1}^{n} \lambda_{m,i} C(\mathbf{u}_{i}, \mathbf{u}_{j}) - \mu_{m} = 0, \ \ i=1,\dots,n  \\
        % \sum_{i=1}^{n} \lambda_{m,i} = 1 \\
        \sum_{j=1}^{n} \lambda_{j}^{\ell} C(\mathbf{u}_{i}, \mathbf{u}_{j}) + \mu^{\ell} = C^{\ell}(\mathbf{u}_{i}, \mathbf{u}), \ \ \ i=1,\dots,n \\
        \sum_{j=1}^{n} \lambda_{j}^{\ell} = 0, \ \ \ \ell = 0,\dots,L
    \end{cases}
\end{equation*}

The right hand side covariances are the covariances corresponding to the particular structure $\ell$ being estimated. The sum of the estimated factors returns the original ordinary kriging estimate:
\begin{equation*}
    z^{*}(\mathbf{u}) = \sum_{\ell=0}^{L} z^{\ell*}(\mathbf{u}) + m^{*}(\mathbf{u})
\end{equation*}

Filtering properties of factorial kriging may be helpful if one is interested in removing a particular factor from estimated maps. For example, high-frequency variation from the nugget effect could be filtered by only considering the $\ell=1,\dots,L$ factors. Factorial kriging extends to the multivariate context by considering the LMC fitted to the direct and cross variograms where each coregionalized variable $\{Z_{k}(\mathbf{u}), k=1,\dots,K\}$ is a linear combination of the standard, independent factors $\{Y_{v}^{\ell}(\mathbf{u}), v=1,\dots,K; \ell=0,\dots,L\}$ \citep{wackernagel1988geostatistical}:
\begin{equation*}
    z^{\ell*}_{k}(\mathbf{u}) = \sum_{v=1}^{K} a_{k,v}^{\ell} Y^{\ell}_{v}(\mathbf{u}) = \sum_{v=1}^{K}\sum_{i=1}^{n} \lambda_{v,i} z_{v}(\mathbf{u}_{i})
\end{equation*}

% \begin{equation*}
%      m^{*}_{k}(\mathbf{u}) = \sum_{v=1}^{K} \sum_{i=1}^{n} \lambda_{v,i} z_{v}(\mathbf{u}_{i})
%  \end{equation*}

The cokriging equations for a particular spatial component are then:
\begin{equation*}
    \begin{cases}
        \sum_{k^{\prime}=1}^{K} \sum_{j=1}^{n} \lambda_{k^{\prime},j} C_{k,k^{\prime}}(\mathbf{u}_{i}, \mathbf{u}_{j}) + \mu_{k} = C_{k,k}^{\ell}(\mathbf{u}_{i},\mathbf{u}), \ \ \ k=1,\dots,K; i=1,\dots,n \\
        \sum_{j=1}^{n} \lambda_{k,j} = 0  \ \ \ k=1,\dots,K
    \end{cases}
\end{equation*}

Again, the sum of the estimated factors returns the original regionalized variable:
\begin{equation*}
    z^{*}_{k}(\mathbf{u}) = \sum_{\ell=0}^{L} z^{\ell*}_{k}(\mathbf{u}) + m^{*}_{k}(\mathbf{u})
\end{equation*}

Multivariate factorial kriging is a technique for characterizing the regionalized factors $Y_{k}^{\ell}$ from observations of $Z_{k}$. This technique is advantageous if the correlation between variables depends on scale and one would like to extract or filter a particular spatial structure.

Simple factorial kriging equations are equivalent to the simple kriging equations except for the right hand side covariance is the covariance of the nested structure being estimated \citep{hong2007improved}:
\begin{equation*}
    \begin{cases}
        \sum_{j=1}^{n} \lambda_{j}^{\ell} C(\mathbf{u}_{i}, \mathbf{u}_{j}) = C^{\ell}(\mathbf{u}_{i}, \mathbf{u}) \\
        i=1,\dots,n \ \ \ \ell=1,\dots,L                                                                           \\
    \end{cases}
\end{equation*}

\begin{equation*}
    z^{*}(\mathbf{u}) = \sum_{\ell=0}^{L} z^{\ell*}(\mathbf{u}) + m
\end{equation*}

The sum of the estimated factors plus the mean equals the simple kriging estimate.

\FloatBarrier
\subsection{Simulating Continuous Variables}
\label{subsec:03simulate}

Kriging and its variants generate smooth, deterministic estimates that do not reproduce the true variability of the data \citep{deutsch1992geostatistical}. The smoothness of this estimate is not desirable if the transfer function is sensitive to extreme values; under-representing the variability may be consequential. Simulation methods reproduce the input variables' covariance structure and spatial variability while honouring data values at their locations \citep{goovaerts1997geostatistics}. Generating a set of equiprobable realizations captures uncertainty in the \gls{RV}. Conventional simulation algorithms rely on the assumption that the input variables are multivariate Gaussian after a univariate normal score transform. Under the multivariate Gaussian assumption, all conditional distributions are defined by the normal equations and thus are linear combinations of the conditioning data \citep{chiles2012geostatistics}. Many algorithms are available for conditional simulation of Gaussian \glspl{RF}. \cite{pinto2020independent} presents a comprehensive overview of the most common algorithms and best practices for selecting an algorithm given the problem context.

Gaussian simulation algorithms are prevalent in the mining industry, with \gls{SGS} likely being the most common \citep{rossi2013mineral}. \Gls{SGS} is a \gls{MCS} technique for simulation of Gaussian \glspl{RF} \citep{gomez-hernandez1993joint,isaaks1990application,goovaerts1997geostatistics,gomez-hernandez2021one}. \Gls{SGS} requires (1) parameterizing a multivariate conditional \gls{CDF}, and (2) drawing realizations. Given the dimensionality of the problems faced in mining, (1) is only possible if a parametric, multivariate Gaussian distribution is adopted \citep{leuangthong2008solved}. The \textit{curse of dimensionality} \citep{bellman1961adaptive}, plus the simplicity and tractability of the Gaussian distribution, precludes the use of other distributions. The following steps generalize the process of generating a realization with \gls{SGS}:
\begin{enumerate}[noitemsep]
    \item Define a random path through the grid nodes.
    \item At each location, calculate the first and second-order moments of the conditional \gls{CDF} using the normal equations.
    \item Randomly draw a simulated value from the conditional \gls{CDF}.
    \item Add the simulated value to the conditioning data.
    \item Visit the next grid node in the path.
\end{enumerate}

As the simulation progresses, the amount of conditioning data increases. In practice, the calculation of the conditional moments is restricted to a local neighbourhood about the location being simulated to prevent unreasonably large systems of equations. Multiple realizations are generated by varying the random path and draws from the conditional \glspl{CDF}.



% Factorial simulation utilizes a conditioning by simple factorial kriging scheme. Conditioning by kriging involves adding a simulated kriging error component to a kriging estimate \citep{chiles2012geostatistics}.
% \begin{equation*}
%     z_{cs}(\mathbf{u}) = z^{*}(\mathbf{u}) + [z_{us}(\mathbf{u}) - z_{us}^{*}(\mathbf{u})]
% \end{equation*}

% Where $z_{cs}(\mathbf{u})$ is the conditioned value, $z^{*}(\mathbf{u})$ is the simple kriging estimate, $z_{us}(\mathbf{u})$ is the unconditionally simulated value and $z_{us}^{*}(\mathbf{u})$ is the simple kriging estimate of the unconditional values at the data locations. This linear combination can be done in a factorial sense as each term can be generated for the given factor \citep{hong2007improved}:
% \begin{equation*}
%     z_{cs}^{\ell}(\mathbf{u}) = z^{\ell*}(\mathbf{u}) + [z_{us}^{\ell}(\mathbf{u}) - z_{us}^{\ell*}(\mathbf{u})]
% \end{equation*}

% Due to Kriging's exactitude, the sum of the conditionally simulated values at the data locations equals the sum of the simple Kriging factors.

\FloatBarrier
\subsection{Multiple Point Statistics}
\label{subsec:01mps}

Two-point statistics summarize the relationship between points separated by a lag vector $\mathbf{h}$. Two-point statistics, such as the variogram or correlogram, are measures of linear continuity. \Gls{MPS} are measures of continuity between multiple spatial arrangements of points with the possibility of reproducing curvilinear or ordering patterns \citep{boisvert2007multiplepoint}. There are numerous \gls{MPS} including the $n$-point connectivity function \citep{journel1989nongaussian}, the distribution of runs \citep{ortiz2003characterization} and the \gls{MPDF} \citep{boisvert2007multiplepoint}.

The foundation of multiple-point simulation algorithms \citep{guardiano1993multivariate,strebelle2002conditional} is the replacement of inference of two-point statistics with the \gls{MPDF}, where the \gls{MPDF} describes the frequency of occurrence of a particular pattern. The local conditional probabilities are derived from multiple-point configurations, allowing for the reproduction of non-linear features \citep{silva2014guide}. A challenge of simulation with \gls{MPS} is that inference of the \gls{MPDF} with limited data. This challenge is overcome by extracting the \gls{MPDF} from a \gls{TI} \citep{journel2005covariance}. The \gls{TI} acts as a substitute for a \gls{RF} model and is an exhaustive image at the same support of realizations. The \gls{TI} should have the expected geologic variability of the final models \citep{gomez-hernandez2021one}. Though the \gls{TI} allows for inference of \gls{MPS} not available from the data, one faces the challenge of selecting an appropriate \gls{TI}. \cite{boisvert2007multiplepoint} describes the choice of \gls{TI} being analogous to variogram modeling in the two-point paradigm, and is of first-order importance. In general, the TI should represent the physics of the underlying geological process and be characteristic of the conceptual geology. \Glspl{TI} can be generated from outcrop data, object-based models, or process-based models \citep{tahmasebi2018multiple}. More recently, \cite{minniakhmetov2022highorder} present methodology for high-order simulation of categorical variables that does not rely on a training image.

Traditionally, \gls{MPS} simulations focus on categorical modeling of stratigraphic deposits where object-based or process-mimicking models are applicable across geologic environments \citep{mariethoz2014multiplepoint}. Multi-point simulation of continuous variables is approached with high-order spatial cumulants, discussed in the next section.


\FloatBarrier
\subsection{High-Order Simulation}
\label{subsec:03hosim}

High-order simulation methods are similar in concept to the multi-point simulation framework, with applications for both continuous and categorical variables. These methodologies are predominantly data-driven, complemented by a \gls{TI}. Rather than inferring high-order conditional probabilities exclusively from a \gls{TI}, they are approximated by spatial cumulants calculated from the data. \cite{dimitrakopoulos2009highorder} and \cite{mustapha2010highorder,mustapha2011hosim} propose high-order simulation based on spatial cumulants. A \textit{cumulant} is defined as the logarithm of the moment-generating function of a \gls{RF}. The idea is that spatial cumulants can generalize the covariance to orders beyond two; \cite{dimitrakopoulos2009highorder} shows that the first and second order cumulants are the mean and variance, respectively. The high-order cumulants, similar to \gls{MPS}, can characterize complex non-linear and non-Gaussian geologic features. By incorporating spatial cumulants up to order five, \cite{mustapha2010highorder} show the ability to capture multi-point periodicity, connectivity of extreme values and complex geometric characteristics. The key idea in the \textit{HOSIM} approach \citep{mustapha2011hosim} is that the local conditional \glspl{CDF} take the form Legendre polynomial expansions, where spatial cumulants approximate the polynomial coefficients \citep{mustapha2010highorder}. Both a \gls{TI} and available data are used to infer the Legendre polynomial coefficients; third- and fourth-order statistics are estimated from the data, while higher-order features come from the \gls{TI} \citep{minniakhmetov2022highorder}. \cite{minniakhmetov2017joint} extends the high-order simulation framework to the multivariate context. \cite{minniakhmetov2018highorder}, \cite{yao2020highorder} and \cite{yao2021learning} present further refinements to the polynomial approximations.

Though largely data-driven, a drawback of these methodologies is that a \gls{TI} is required. The inference of the spatial cumulants draws from both the \gls{TI} and available data; if the high-order multi-point replicates are not available in the data, they are incorporated from the \gls{TI} \citep{mustapha2010highorder,yao2021training}. Selecting or generating a \gls{TI} for continuous variables in mining problems is challenging without dense sampling; \cite{minniakhmetov2018highorder} and \cite{decarvalho2019highorder} use blast hole samples for \gls{TI} construction. \cite{yao2021training} presents methodology for \gls{TI} free simulation with aggregated kernel statistics.

\FloatBarrier
\subsection{Imputation}
\label{subsec:01impute}

Imputation is a key component of probabilistic modeling of continuous heterotopic data \citep{barnett2015multivariate,silva2018multivariate,hadavand2023spatial} and in truncated Gaussian categorical modeling techniques \citep{silva2018enhanced,arroyo2020iterative,madani2021enhanced,armstrong2011plurigaussian}. Simulation often considers a multiple imputation framework \citep{little2019statistical} where one generates realizations of missing values to transfer imputation uncertainty to the final models correctly.

The goal of imputation in the heterotopic data context is to fill in missing values. Modern geostatistical workflows necessitate the use of multivariate transforms like \gls{PPMT}, \gls{SCT}, and \gls{PCA} which require homotopic data. Any heterotopic data observations must be either excluded or imputed. Geologic data is often ``missing not at random'' and simply excluding heterotopic observations can lead to biases in the final model \citep{dasilva2019treatment}. The goal of any imputation algorithm is to define the distribution of the missing values conditional to the observed values. \cite{barnett2015multivariate} proposed a non-parametric methodology for imputing continuous variables based on Bayesian updating and Gibbs sampling. A \gls{SK} mean, and variance is merged with a collocated conditional distribution estimated with a multivariate \gls{KDE}. The parameters calculated with \gls{SK} account for the univariate spatial component, while the \gls{KDE} accounts for the collocated multivariate component. Imputed values are drawn from the merged distribution. This methodology is computationally expensive with many data due to \gls{KDE} calculations and Gibbs sampler iterations. \cite{silva2018multivariate} proposed a non-parametric imputation algorithm based on \glspl{GMM} to relieve the burden of \gls{KDE} calculations. \Gls{SK} defines the local conditional spatial distribution, however the collocated multivariate density is sampled from a fitted \gls{GMM}. \cite{hadavand2023spatial} proposed another non-parametric methodology where deep learning characterizes the multivariate relationships, rather than a \gls{GMM}. Two neural networks are trained to quantify the moments of the conditional missing value distribution; one for the mean and another for the second, third, and fourth-order moments. A lambda distribution is fit given the conditional moments that characterize the collocated multivariate relationship.

Latent imputation is a special scenario where all values are missing \citep{little2019statistical}. Truncated Gaussian modeling techniques utilize the idea that categorical observations are generated by truncating underlying latent variables \citep{matheron1987conditional}. These latent variables are not observed and are a synthetic feature of the model. Imputation of latent variables subject to categorical observations is commonly approached with a Gibbs sampler \citep{geman1984stochastic}, where directly sampling the multivariate truncated Gaussian distribution is not possible, but sampling the marginal conditional distributions is possible \citep{silva2018enhanced,arroyo2020iterative,madani2021enhanced}. Though the Gibbs approach is common, \cite{emery2014simulating} and \cite{silva2018enhanced} note convergence issues with spatially correlated variables. More recently \cite{lauzon2020calibration,lauzon2020sequential,lauzon2023joint} proposed the sequential spectral turning band simulator as an alternative for Gibbs sampling, where Gaussian \glspl{RF} are constructed by addition of cosine functions. The proposed spectral approach begins with Gaussian \glspl{RF} that meets the inequality constraints. Then, the spatial component is introduced by sampling the spectral density. In contrast, the Gibbs sampler approach begins with Gaussian \glspl{RF} with the correct spatial structure and gradually introduces constraints through Gibbs iterations. The authors show that the spectral approach is a valid alternative to the Gibbs sampler with stable convergence of many data, multiple rock types, and complex truncation rules.


% \begin{enumerate}
%     \item Gibbs:
%           \begin{enumerate}
%               \item \cite{barnett2015multivariate,silva2018multivariate,silva2018enhanced,arroyo2020iterative,madani2021enhanced}
%           \end{enumerate}

%     \item Lambda distribution:
%           \begin{enumerate}
%               \item \cite{hadavand2023spatial}
%           \end{enumerate}

%     \item Spectral:
%           \begin{enumerate}
%               \item \cite{lauzon2020calibration,lauzon2020sequential,lauzon2023joint}
%           \end{enumerate}
% \end{enumerate}


\FloatBarrier
\section{Extreme Value Background}
\label{sec:01evtreview}

The following section provides background literature on extreme values, outliers, and their significance in geospatial and mining-related problems. Though the \gls{NMR} framework does not directly incorporate \gls{EVT}, the statistical foundation is presented for completeness.

\FloatBarrier
\subsection{Outlier Detection}
\label{subsec:01outlier}

Outlier detection is relevant to all statistical modeling, and the literature is vast \citep{zimek2018there}. \cite{hodge2004survey} note that authors may refer to outlier detection as novelty, noise, anomaly or deviation detection. In all cases, however, an outlier is an observation sufficiently dissimilar to other observations \citep{barnett1984outliers}. \cite{wang2019progress} group outlier detection methodologies into (1) statistical methods, (2) distance-based methods, (3) density-based methods, (4) clustering-based methods, and (5) ensemble-based methods. Statistical methods, either parametric or non-parametric, compare the relationship of potential outliers with the remaining distribution. Distance-based methods (euclidean or non-euclidean) compare the distance between observations where potential outliers are ``far'' from other observations. Density-based methods consider outliers in low-density regions of a \gls{PDF}. Clustering-based methods classify each observation, and potential outliers are not within or near dense clusters. Finally, ensemble methods are combinations of dissimilar methodologies to create a more robust outlier detection model. \cite{hodge2004survey,wang2019progress,pimentel2014review,boukerche2021outlier} provide comprehensive reviews of outlier detection methodologies with applications to fraud detection, cybersecurity, sensor networks, image processing, time series and data streams, medical diagnostics and industrial monitoring. \cite{pang2022deep} presents a comprehensive review of outlier detection with deep learning, though the concepts are largely beyond the scope of this thesis.

\FloatBarrier
\subsection{Geospatial Outlier Detection}
\label{subsec:01miningoutlier}

Outlier detection in the mining industry is based largely on graphical methods \citep{leuangthong2015dealing, silva2021classification}. These methods are necessarily subjective as the practitioner must interpret a plot and select a threshold to define an outlier. \cite{leuangthong2015dealing}, \cite{nowak2013suggestions}, and \cite{rossi2013mineral} provide practical advice on threshold selection for \glspl{CPP} where breaks in the upper tail may represent outlier populations. \cite{babakhani2014geostatistical} proposed a spatial bootstrap-based methodology to characterize the relationship between the naive and capped mean. The methodology aims to identify the values that cause higher mean values as potential outliers.

Practitioners often omit the spatial characteristics of sample values when identifying outliers. The spatial context of the samples is likely relevant; an extreme value surrounded by other high values may not be an outlier. The correlation structure of the variable(s) is relevant to understanding the spatial context \citep{filzmoser2014identification}. \cite{babakhani2014geostatistical} proposed a methodology for identifying spatial outliers based on the rank transform of cross-validation estimates. Outliers identified by the rank transform consider the spatial neighbourhood and are different from simply considering the univariate distribution. Many authors \citep{filzmoser2020multivariate,filzmoser2014identification,ernst2017comparison, leung2021sample,harris2014multivariate,chen2008detecting} have proposed methodology for spatial multivariate geochemical outlier detection using the \gls{MCD} estimator of \cite{rousseeuw1999fast}. The Mahalanobis distance \citep{mahalanobis2018generalized} is a common distance metric in multivariate space. However, it is sensitive to outliers \citep{filzmoser2014identification}. The \gls{MCD} is a robust measure of the global correlation structure in the presence of outliers. Given the \gls{MCD} estimate, various measures of local and global multivariate distances are calculated to determine an outlier score. \cite{chen2008detecting} calculate an outlying distance based on the differences between each sample value and the median value over its neighbours. \cite{filzmoser2014identification} calculate a degree of isolation for each observation based on the robust Mahalanobis distance (\gls{MCD} estimate) globally, as well as within a local neighbourhood; values above a defined threshold are considered potential multivariate outliers.

\FloatBarrier
\subsection{Geospatial Outlier Management}
\label{subsec:01outliermanage}

Once one identifies outliers, the practitioner must decide on a management strategy. Managing extreme values and outliers prior to resource estimation is a key component, particularly for heavy-tailed mineral deposits. The general idea is that unadjusted grades may lead to unrealistic local estimates adjacent to high-grade composites \citep{nowak2013suggestions}. \cite{leuangthong2015dealing} provide an overview of outlier management practices in the context of mineral resource estimation. The practices are grouped into three general categories: (1) choosing appropriate domains, (2) grade capping, and (3) limiting the influence of outliers through the estimation process. The following sections touch on categories (2) and (3)—trade-craft and practitioner experience is the foundation of many suggested practices.

There are numerous practical methods to manage the influence of high-grade samples explicitly. Traditional methods published in mining technical reports largely fall into the grade-capping category. Techniques for selecting a capping threshold include decile analysis \citep{parrish1997geologist}, cumulative probability plots \citep{rossi2013mineral}, cutting curves \citep{roscoe1996cutting}, coefficient of variation thresholds \citep{parker1991statistical}, production reconciliation, arbitrary quantiles, metal-at-risk \citep{parker2006}, and indicator correlations \citep{nowak2019optimal}. More recently, \cite{dutaut2021new} proposed using an error-free \gls{CV} calculated from coarse duplicate correlation to determine a capping limit.

\cite{babakhani2014geostatistical} proposed a less conventional method of projecting outliers to an extra dimension to reduce the local influence. This method requires specifying a distance $d$ to project the outlier; the distance could be determined by calibrating kriging results to an expected value from simulation or selecting a specific quantile. \cite{babakhani2014geostatistical} also proposed calibrating a capping limit based on an expected value from simulation. This approach is similar to the previous; however, instead of projecting an outlier some distance, $d$, away, the outlier value is reduced. Within some local volume of influence, the outlier grade is reduced until the kriged grade matches the expected value of 100 simulated realizations. The idea is that simulation is more robust in the presence of outliers and this resistance is exploited to calibrate a capping grade. \cite{rivoirard2013topcut} and \cite{maleki2014capping} propose the selection of an optimal capping limit through analysis of the ratios of direct and cross-indicator variograms. The goal is to identify the range $[z_{min}, z_{max}]$ in where the capping limit should fall. The minimum value is defined by the first threshold where the ratio of the cross and direct indicator variogram values are constant, independent of the lag vector. The maximum value is the threshold where the residual variogram is pure nugget.

Many methodologies have been proposed to circumvent the practice of capping. Some techniques, such as \gls{MIK} \citep{journel1983nonparametric}, directly treat outliers through the specification of upper-class means and do not require explicit capping \citep{rossi2013mineral}. \cite{costa2003reducing} proposed a variant of robust kriging \citep{hawkins1984robust} where the weight to outlier samples differs from that of inliers. The ``robust-edited'' values are adjusted based on the difference between the sample value and the weighted median at the same location. \cite{rivoirard2013topcut} proposed the decomposition of the grade value into a truncated grade, a weighted indicator above the top cut grade, and a residual. The residual is uncorrelated with the truncated grade and the indicator if the cutoff is sufficiently high. The final estimate is a kriged estimate of the residual plus a cokriged estimate of the indicator and truncated grade. \cite{maleki2014capping} proposed a similar decomposition approach where they suggest spatial prediction is improved by omitting outlier values from variogram calculations. \cite{fourie2019limiting} proposed a methodology that post-processes kriging weights to generate realistic estimates without grade smearing. Kriging weights are adjusted based on the ratio of the frequency of outlier samples to median samples. The methodology requires a subjective selection of bin widths after the normal score transform of the variable. More recently, \cite{silva2021classification} proposed a methodology for adjusting outlier grades based on Bayesian updating of the data distribution with a cross-validation error distribution.

Restricting the spatial range of influence of outlier values during estimation is another practical approach to circumvent capping. This restriction allows using extreme values or outliers in estimation without explicit capping. Restricting the search around extreme values allows them to influence within that range but not beyond it. There is no theory to support this restriction, though the idea is that the search should be within the range of high-grade continuity. The restricted search ellipsoid dimensions could come from understood mineralization controls, high-grade indicator variogram continuity analysis, or the p-gram \citep{leuangthong2015dealing}. \cite{leuangthong2015dealing} provide an estimation example with a restitched search radii placed on high-grade samples in a South American gold deposit.

\subsection{Extreme Value Theory (EVT)}
\label{subsec:01evt}

\Acrfull{EVT} is a statistical foundation for quantifying probability distributions and magnitudes of atypically high or low events. Early works by \cite{frechet1927loi}, \cite{fisher1928limiting}, \cite{mises1936distribution} and later by \cite{gnedenko1943distribution}  systematically developed the theory and formalized the asymptotic distribution of extremes for block maximums. \cite{fisher1928limiting} show that for a sequence of \gls{iid} random variables with a common distribution function $F$, the so-called ``block maximum'', $M_n = max\{Z_1,...,Z_n\}$, can only converge to one of three distributions:

\begin{equation}
    G_{I}(z) = exp\left(-exp\left(-\left(\frac{z-\mu}{\sigma}\right)\right)\right), \ \ -\infty < z < \infty
    \label{eq:1}
\end{equation}

\begin{equation}
    G_{II}(z) =
    \begin{cases}
        0,                                                           & z \leq \mu \\
        exp\left(-\left( \frac{z-\mu}{\sigma} \right)^{-\xi}\right), & z > \mu
    \end{cases}
    \label{eq:2}
\end{equation}

\begin{equation}
    G_{III}(z) =
    \begin{cases}
        exp\left(-\left( -\left(\frac{z-\mu}{\sigma} \right)^{\xi} \right) \right), & z < \mu    \\
        1,                                                                          & z \geq \mu
    \end{cases}
    \label{eq:3}
\end{equation}

To avoid degeneracy, the variable $M_n$ is normalized to $M_n^* = a_n^{-1}(M_n - b_n)$ for a sequence of ``normalizing'' constants $a_n > 0$ and $b_n$. \cite{fisher1928limiting} show that if these sequences of real numbers can be chosen such that $M_n^*$ has a non-degenerate limiting distribution, it must be one of type I, II or III. These are the only possible limits for the distributions of $M_n^*$ regardless of the population distribution $F$. This Extremal Types Theorem is analogous to the Central Limit Theorem for extreme values \citep{coles2001introduction}.

These collectively are termed extreme value distributions with Equation \ref{eq:1} being Gumbel-type, Equation \ref{eq:2} being Fr\'echet-type and Equation \ref{eq:3} being Weibull-type. Each distribution has a location $\mu$, scale $\sigma$ and Equation \ref{eq:2} and Equation \ref{eq:3} have shape parameter $\xi$. The extreme value distributions are obtained as limiting distributions of $M_n$ as $n \to \infty$. The limit distributions for block maximum can be grouped into a single family termed the \gls{GEV} distribution \citep{dehaan2007extreme}:
\begin{equation}
    G(z) = exp\left(-\left(1+\xi\left(\frac{z-\mu}{\sigma}\right)^{\frac{-1}{\xi}}\right)\right)
    \label{eq:GEV}
\end{equation}

The block maximum methodology may be inefficient as it ignores all but the maximum value in a given block \citep{davison2015statistics}. The second approach to identifying extremes is the so-called ``peak-over-threshold'' (POT) method. Consider a random variable $Z$ with distribution function $F$. The CDF of the excess over some threshold $u$, is defined by:
\begin{equation}
    \begin{aligned}
        F_u(y) & = P(Z - u \leq y | Z > u)                               \\
               & = \frac{F(u+y) - F(u)}{1-F(u)}, \ \ 0 \leq y \leq z_F-u
    \end{aligned}
    \label{eq:exceed}
\end{equation}

Where $y = z - u$ and $z_F$ is the right endpoint of $F$ \citep{gilli2006application}. \cite{pickands1975statistical} states that if $u$ is large, the conditional distribution of $Z$ given $Z$ is much larger than $u$ is well approximated by the \gls{GPD}:
\begin{equation}
    G(y) =
    \begin{cases}
        1 - \left(1 + \frac{\xi y}{\sigma}\right)^{\frac{-1}{\xi }}, & \text{ if } \xi  \neq 0 \\
        1-exp\left(\frac{-y}{\sigma} \right),                        & \text{ if } \xi  = 1
    \end{cases}
    \label{eq:GPD}
\end{equation}

The conditional distribution of the exceedances (Equation~\ref{eq:exceed}) can be modeled asymptotically with the \gls{GPD} by estimating the scale ($\sigma$) and shape ($\xi$) parameters.

In a geoscience context, \cite{caers1999statistics,caers1999statisticsa} use the \gls{GPD} to model earthquake magnitudes, size distributions of diamonds and the size distributions of impact craters. \cite{deligne2010recurrence} use a Poisson process to model the recurrence rate of explosive volcanic eruptions, while \cite{nguyen2023dynamic} use the \gls{GPD} to forecast volcanic eruptions. \cite{miniussi2020metastatistical} model the frequency of flooding events across the United States based on stream gauge measurements. \cite{lee2021temporal} use a Gumbel distribution to predict the exceedance probability of extreme rainfall-induced landslides.

\FloatBarrier
\subsection{Spatial Extreme Value Theory}
\label{subsec:01spevt}

The assumption of \acrfull{iid} observations underlies classical \gls{EVT}. In many real-world applications, one must account for correlation in space or time and the multivariate nature of regionalized variables. Spatial extreme value theory represents an intersection between classical \gls{EVT} and geostatistics \citep{neves2015geostatistical}. The primary difference between spatial \gls{EVT} and geostatistics is that in the geostatistical framework, there are no observed replicates at $Z(\mathbf{u})$. Fitting of a \gls{GEV} or \gls{GPD} in the classical \gls{EVT} sense requires multiple realizations of $Z_i(\mathbf{u})$ for parameter inference.

Spatial \gls{EVT} builds on the concepts of max-stable distributions extending to the max-stable process. A max-stable process is the infinite dimension generalization of the max-stable distribution where all lower-order marginal distributions are \gls{GEV} distributions \citep{schlather2003dependence}. If there exists normalizing constants $a_n(\mathbf{u}) > 0$ and $b_n(\mathbf{u})$ such that $a_n^{-1}(\mathbf{u})\{\max_{i=1,...,\infty} Z_i(\mathbf{u}) - b_n(\mathbf{u})\} = Y(\mathbf{u})$ then $Y$ is a max-stable process \citep{dehaan2007extreme}. The max-stable process applies to maximums as stable Gaussian processes with finite variance apply to the average \citep{chiles2012geostatistics}. All marginal distributions of a max-stable process are \gls{GEV} distributions defined by Equation~\ref{eq:GEV}.

Unlike a Gaussian \gls{RF}, which is fully defined by its correlogram $\rho(\mathbf{h})$, there is no unique model for max-stable processes \citep{chiles2012geostatistics}. Many models are found in the literature. First introduced by \cite{smith1990maxstable} and later modified by \cite{schlather2002models}, Gaussian storm and extremal Gaussian processes are commonly used for modeling spatial extremes. The Brown-Resnick processes \citep{brown1977extreme,kabluchko2009stationary} relaxes the assumption of second-order stationarity and permits the use of the variogram, which has shown to be practical in practice \citep{gaume2013mapping}. The extremal-$t$ model \citep{opitz2013extremal} is another popular max-stable model in the literature. A consequence of the max-stable processes is asymptotic dependence in the tails \citep{davison2013geostatistics}.


\FloatBarrier
\section{Optimization Background}
\label{sec:01optreview}

The following section provides background literature regarding geoscience and engineering-related inverse problems and the use of optimization to infer unknown model parameters.

\FloatBarrier
\subsection{Inverse Problems}
\label{subsec:01inverse}

Inverse problems encompass a broad class of problems where the objective is to infer a system's underlying causes or parameters from observed data or measurable outputs \citep{sen2013global}. Predicting a response is a forward problem while using a response or observed measurements to infer the properties of a model is an inverse problem \citep{tarantola2005inverse}. Inverse problems arise in various scientific disciplines, including physics, engineering, geosciences, medical imaging, and more. Geospatial inverse problems are common in both the fields of geophysics \citep{linde2015geological,giraud2019integration,grana2022probabilistic} and hydrogeology \citep{zhou2014inverse,ghorbanidehno2020recent} where the underlying geologic model is unknown, however a set of measured responses, such as hydraulic conductivities or seismic properties, are known. The inverse problem involves inferring interpretable geologic properties of the unknown model, such as lithology or porosity, that satisfy the observed measurements. \cite{grana2022probabilistic} describe these problems as rock-physics inversions with seismic measurements predicting rock and fluid properties.

Solving inverse problems involves constructing a mathematical forward model that describes the relationship between the unknown parameters and the observed data and then using this model to infer the unknown parameters. Seismic wave propagation and rock-physics models are generally well-understood forward models in geophysics \citep{grana2022probabilistic} where hydrogeological forward models consider mass conservation and Darcy's law to predict hydraulic head, drawdown or solute concentrations \citep{zhou2014inverse}. A challenge of inverse problems is ill-posedness, or the lack of a unique solution \citep{tarantola2005inverse}. Multiple (or infinite) solutions may be valid given the observed data. As an exact solution is rarely possible in natural, non-linear systems, one looks for solutions close to actual observations \citep{bardossy2016random}.

For this reason, many inverse problems are framed as optimization problems, minimizing an objective function relevant to the problem at hand \citep{giraud2019integration,nava-flores2023high,athens2022stochastic}. The objective function is minimized iteratively, which is generally computationally expensive \citep{zhou2014inverse}. A forward modeling operator predicts an outcome for the model parameters' current state, and the objective function evaluates the loss between this prediction and the observed measurements. The optimization algorithm updates the parameter vector until it matches the model output and observed measurements. Any iterative optimization algorithm is permissible; \cite{athens2022stochastic} use gradual deformation to generate a set of perturbed model realizations; \cite{nava-flores2023high} use simulated annealing for joint inversion of gravity gradient data; \cite{balkaya20173d} use differential evolution, and \cite{davilarodriguez2024threedimensional} a general evolution strategy for inversion of magnetic anomalies.

% Due to the inherent complexity and potential non-uniqueness of inverse problems, careful consideration of data quality, noise, and computational stability is essential to obtain meaningful and reliable solutions.

% \begin{enumerate}
%     \item something about the forward problem
%     \item prior knowledge is the specification of the pool
%     \item prior knowledge is the conceptual geologic model
%     \item no data reproduction at this point, only reproduction of pertinent statistics
%     \item constraints on wt magnitude, nscore keeps solution reasonable
% \end{enumerate}


\FloatBarrier
\subsection{Genetic Algorithms}
\label{subsec:01genetic}

\Glspl{GA} are metaheuristic global optimization algorithms inspired by natural processes like evolution and natural selection \citep{cui2024applications}. This family of algorithms was first proposed by \cite{holland1992genetic} and has wide-ranging applicability to engineering optimization problems as robust global optimizers that do not require differentiable objective functions \citep{carbas2021nature}. The \gls{GA} framework can efficiently explore a large solution space, handle constraints, and lend itself to parallel applications. In general, genetic algorithms are derivative-free, population-based algorithms with three main characteristics: crossover, mutation, and selection operators \citep{yang2018nature}. Each member of the population is a feasible solution vector. The crossover operator exchanges information between members of the population and can enhance key features of the population; the mutation operator permits exploration of the solution space by increasing population diversity; the selection process drives the population towards convergence by selecting the fittest or most elite member of the population for mutation and crossover \citep{yang2018nature}. A problem-specific objective function evaluates the fitness of each population member. \cite{yang2018nature} describes the general behaviours and components of \glspl{GA} as:
\begin{itemize}[noitemsep]
    \item A population of agents representing solution vectors, each with an associated fitness.
    \item The population evolves through mutation and crossover operations. The algorithm converges when all members of the population are sufficiently similar.
    \item New solutions are generated with random perturbations to avoid local optima.
    \item \Glspl{GA} search locally and globally, with the local and global search ratio controlled by the genetic operators.
    \item \Glspl{GA} employ a survival of the fittest approach, where the fittest members of the population are retained for the next generation, driving the population towards convergence.
\end{itemize}

\Gls{DE}, first proposed by \cite{storn1997differential}, is a \gls{GA} that uses the scaled difference between population members as a mutation operator. Many mutation variants exist \citep{meng2020enhancing}; purely random mutation variants explore the global solution space by exploiting differences between randomly selected vectors, while others explore both local and global spaces by mutating random vectors and the current best vector. Crossover occurs after mutation, generating a trial vector. If a randomly generated uniform number $\in [0,1]$ is less than a specified crossover probability, the element from the mutated vector transfers to the current member of the population \citep{price2013differential}. The crossover generates a trial vector hybrid between the current population and the mutation. The selection operator evaluates the fitness of the trial vector and replaces the current population member if $f_{trial} < f_{pop}$. If the trial vector improves the solution, it is kept in the population. \gls{DE} is widely used in engineering optimization problems \citep{georgioudakis2020comparative}, geophysical inversion \citep{balkaya20173d}, optimization of neural network architectures \citep{unal2022evolutionary, mirjalili2019evolutionary},  and others including electrical power systems, image processing, chemical engineering and manufacturing \citep{bilal2020differential}. Due to the widespread use of \gls{DE}, \cite{ahmad2022differential} report over 40 variants of the original algorithm.

\FloatBarrier
\section{Thesis Outline}
\label{sec:01outline}

Chapter \ref{ch:02outlier} discusses outlier management in the mining industry. Though the \gls{NMR} framework does not require explicit management of extreme values, it would be remiss not to discuss capping due to its ubiquitous presence and relation to outliers. A range of outlier management tools are discussed and a novel algorithm for identifying outliers in a spatial context is presented. The chapter finishes with an analytical model for predicting the frequency of intersecting extreme values.

Chapter \ref{ch:03framework} explores the core components of the \gls{NMR} framework. It begins with the concepts of high-order connectivity and the relationship with non-Gaussianity. The chapter introduces the network components: (1) definition of a latent Gaussian pool, (2) non-linearity and mapping to observed space, (3) latent imputation, and (4) continuous simulation and mapping. Here, the parameterization of the \gls{NMR} is posed as an inverse problem. The chapter finishes with a synthetic, non-Gaussian example to emphasize the effects of high-order connectivity on contained resources.

Chapter \ref{ch:04implement} presents details of implementing the \gls{NMR}, including the network architecture, activation function, latent factor design and parameter inference via optimization. The effects of mixing latent factors are discussed in detail, and an objective function relevant to the modeling goals is formulated. The concepts of \acrfull{DE} and its application to \gls{NMR} parameter optimization are presented, followed by checking and validating the network output. A \gls{3D} example is introduced and carried over into chapter \ref{ch:05impute}. Finally, practical implementation details are discussed including the potential non-uniqueness of the solution.

Chapter \ref{ch:05impute} introduces a novel algorithm for imputing latent factors within the \gls{NMR} framework. Imputation concepts and traditional Gibbs sampler approaches are touched on, followed by a presentation of \acrfull{SGRI}. The algorithm is an iterative, sequential imputation algorithm that uses the normal equations and rejection sampling to impute spatially correlated latent variables. Minimum acceptance criteria for checking the latent realizations are presented, followed by conditional simulation and practical checking using the example from Chapter \ref{ch:04implement}.

An application of the complete \gls{NMR} framework with a real dataset is shown in Chapter \ref{ch:06casestudy}. The data comes from an operating underground mine where personnel note that multivariate Gaussian simulation algorithms do not reproduce the connected high-grade features observed in drillhole data. This scenario is the ideal application of the \gls{NMR}, where it shows a 7\% improvement over \gls{SGS} in high-grade stopes. The \gls{NMR} results are validated with a hold-out dataset.

Chapter \ref{ch:07conclusions} summarizes the contributions made in this thesis. Consideration is given to the limitations of the developed methodologies and avenues for future work and improvements to the \gls{NMR} framework. All software developed for this research is documented in the appendices.