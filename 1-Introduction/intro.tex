%!TEX root = ../Thesis_example_compile_this.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{ch:01intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The field of geostatistics encompasses the problem of spatial prediction and the characterization of uncertainty within geological systems \citep{deutsch1992geostatistical}. We are concerned with describing the spatial aspects of mineralization as well as its variability or dispersion. Geostatistics was developed based on the need to forecast the recoverable resources at unsampled locations in mineral deposits \citep{matheron1963principles}. Though initially developed in a mining context, geostatistics has found practical uses in many fields concerned with spatially correlated data such as petroleum, hydrogeology, environmental science, remote sensing and others \citep{goovaerts1997geostatistics}.

Geostatistics utilizes observed categorical and continuous properties to generate exhaustive numerical models of the subsurface. These models are either deterministic or probabilistic, where equiprobable realizations are generated through stochastic simulation \citep{chiles2012geostatistics}. These realizations honour the spatial and multivariate characteristics of the input data with statistical fluctuations and provide a measure of joint uncertainty within the region of interest \citep{rossi2013mineral}. Characterizing and quantifying geologic uncertainty gives engineers and decision-makers practical tools for optimizing orebody extraction.

Mineral deposits, particularly precious metals, often exhibit strongly positively skewed grade distributions. These distributions pose challenges for spatial prediction as there are usually limited data characterizing the upper tail. Some component of the high values in the upper tail are characterized as ``outliers'' based on a subjective threshold. There is a risk of local overestimation with smooth kriging estimators if sparse, high-value data are left unmanaged \citep{leuangthong2015dealing}. Standard practice in mining is to cap high values to a maximum to avoid local conditional bias, but those high values may have tremendous economic value. Appreciating the potential and the upside of such values in a quantitative and repeatable manner is something of great practical interest.

The term ``extreme value'' is not regularly used in the mining industry; rather, ``outlier'' is used. Some mineral distributions with high coefficients of variation likely do contain extreme values in the classic statistical sense. It could be argued that a mineral deposit is an extreme value in the context of regional geology.


% Extreme value theory (EVT) is a statistical foundation for quantifying probability distributions and magnitudes of atypically high or low events. Extreme values are forecast using currently observed extremes \citep{gumbel1958statistics}. The properties of extreme values and their statistics are governed by upper or lower tails of the underlying distribution. Classical EVT attempts to characterize the underlying extreme value distribution based on extreme order statistics or isolating values above some high threshold. Extreme value distributions like the generalized extreme value and generalized Pareto distributions are commonly applied in domains dealing with time-series data such as meteorology, hydrology, economics and insurance \citep{reiss2007statistical}.


\FloatBarrier
\section{Problem Setting}
\label{sec:01problem}

Mitigating the impact of extreme values on resource estimation is a long-standing issue. Extreme values present unique challenges in the sense there usually few samples and understanding their spatial distribution is difficult. This is coupled with the potential of significant economic risk if mismanaged. To mitigate risk of overestimation, high grades are typically capped in practice. Numerous problems remain outstanding concerning (1) objectively defining what an extreme value is; (2) explicit approaches for limiting extreme value influence; (3) characterizing the statistical, or distribution component of extreme values; (4) characterizing the spatial component of extreme values and (5) develop practical advice to combine the statistical and spatial components.

The mining industry currently has no definitive consensus regarding extreme value management, and many approaches are developed on an ad hoc basis. Should high grades be capped? Should sub-regions be delineated to isolate higher grades? These questions influence the final metal content of our resource predictions. The impact of restricting outliers may be significant depending on the distribution of the available data. Many industry practices are born from practicality and based on experience-driven rules-of-thumb \citep{nowak2013suggestions}. These practices motivate a need for geostatistical tools with explicit consideration for the presence of extreme values.

High-grade values are generally sparse in mining data sets. Though sparse, these high-grades values may contribute significantly to the project economics. For example, at the Brucejack deposit in Northwest British Columbia, the top 1\% of drill core samples contain 83\% of the total deposit metal (Au) \citep{pretium2020}. Anecdotal evidence from producing gold mines suggests this scenario poses an economic risk if samples in the upper tail are not explicitly managed. This issue is potentially exacerbated by the use of smooth deterministic estimators such as kriging. Many strongly positively skewed distributions also exhibit non-Gaussian spatial characteristics \citep{journel1989nongaussian}. There is a need for a simulation framework that can correctly characterize the non-Gaussian spatial features related to extreme values without arbitrarily restricting their influence.

The spatial continuity of extreme values in the upper tail may not be the same as low values. The multivariate Gaussian assumption underlying many geostatistical algorithms does not allow for spatial connectivity of extreme values, nor does it allow for asymmetry in the loss of correlation away from the median \citep{journel1989nongaussian}. \Gls{MIK} \citep{journel1983nonparametric} was conceived for this purpose however it has proved difficult to implement effectively in practice and can be outperformed by simpler Gaussian techniques \citep{vincent2021mik}. The challenges associated with \gls{MIK} and traditional extreme value management motivate the need for a new spatial model to characterize the non-Gaussian spatial continuity of extreme values.

\FloatBarrier
\subsection{Thesis Statement}
\label{subsec:01thesis}

To address these challenges a framework for the simulation of continuous variables in the presence of extreme values is proposed. The framework constructs numerical geologic models with explicit consideration for the presence and spatial structure of extreme values. By representing a regionalized variable by a non-linear combination of underlying latent Gaussian factors, models can better characterize high-grade geologic features.

The framework is built on a non-linear \acrfull{NMR}, which is an expansion of the \acrfull{LMR} concept \citep{journel1974geostatistics}. Rather than a positive linear combination of latent factors, the \gls{NMR} introduces non-linear activations and a hidden layer to form a network structure. Non-linearity allows the spatial model to adapt to complex, high-order features and better control the known indicator asymmetry between low and high grades with non-Gaussian distributions \citep{journel1989nongaussian}. High-order measures of connectivity are shown to characterize non-Gaussianity. These measures of connectivity are extracted from drill strings and incorporated into the spatial models. Capturing richer spatial structure, beyond what is possible with two-point statistics, improves the prediction of high-grade in-situ resources. The \gls{NMR} framework is particularly advantageous for strongly positively skewed distributions such as precious metals, uranium or diamonds.

\vspace{\baselineskip}
\begin{tcolorbox}[]
    \textbf{Thesis Statement:} \textit{The breakdown of regionalized variables into fundamental latent components coupled with a non-linear network model of regionalization will improve probabilistic modeling of strongly positively skewed grade distributions.}
\end{tcolorbox}
\vspace{\baselineskip}

The key contributions of this thesis are the development of:
\begin{enumerate}[noitemsep]
    \item The \gls{NMR} framework for the simulation of high-order spatial features, improving the modeling of continuous variables in the presence of extreme values. The framework includes:
          \begin{enumerate}[noitemsep]
              \item Methodology for the parameterization of the network, permitting mapping between latent and observed spaces. This inverse problem is approached through stochastic optimization.
              \item Methodology for stable imputation of latent factors that (1) reproduce the correct spatial statistics, and (2) reproduce the observed data values.
              \item A novel activation function to impose spatial features in the tails of the continuous distribution.
          \end{enumerate}
    \item Tools for calculating high-order measures of connectivity from drillhole sequences; these connectivity features are a proxy for non-Gaussianity.
    \item An algorithm for identification of outliers in a spatial context. The algorithm considers both the spatial arrangement, and shape of the global empirical distribution to assign an outlier score.
\end{enumerate}

At present there is little research focused on the continuous simulation of high-order spatial features without the use of training images. In the \gls{NMR} framework, all high-order features are extracted directly from the observed data with no assumptions made regarding the geological system or underlying physical processes. These \gls{1D} patterns are restricted to the drill strings, however the network parameters enforce connectivity away from the data. Another key difference is the introduction of the latent Gaussian space. The local conditional \glspl{CDF} are not approximated by a combination of high-order statistical moments, rather calculated directly under a multivariate Gaussian assumption. The non-Gaussian spatial model is constructed as a mixture of Gaussians; the transform from latent to observed space captures the high-order features.


\FloatBarrier
\section{Geostatistical Background}
\label{sec:01geostatreview}

The proposed research covers a wide range of subjects involving modeling continuous, positively skewed variables with geostatistical simulation. The following section reviews the relevant geostatistical concepts. This section is not intended to be exhaustive, but rather provide a review of concepts related to the \gls{NMR} framework.

\FloatBarrier
\subsection{Overview}
\label{subsec:01overview}

Geostatistics is a field of applied statistics concerned with the characterization and modeling of spatially correlated variables. A variable that is dispersed in space and exhibits spatial structure is said to be regionalized \citep{matheron2019matheron}. A foundation of geostatistics is the concept of random variables where unknown values, $z$, at an unsampled location are modeled as outcomes of a random variable $Z$ \citep{deutsch1992geostatistical}. A random function represents a collection of spatially correlated, location-dependent random variables $Z(\mathbf{u})$ for every location $\mathbf{u}$ within the study region \citep{goovaerts1997geostatistics}.

The decision of stationarity is one to group or pool relevant data together. As no data replicates are available at location $\mathbf{u}$ to infer the random function $Z(\mathbf{u})$, geologically similar data must be pooled such that population statistics can be reliably inferred. This decision of stationarity allows the trade of unavailable replicates for data at other locations for statistical inference \citep{deutsch1992geostatistical}. Pooling too little data may lead to unreliable statistics, and too much data may lead to the masking of important geological features. Stationarity is a property related to the underlying random function model and cannot be checked or validated with data \citep{goovaerts1997geostatistics}.

Inference of the first and second-order moments (mean and covariance) of the random function is required for geostatistical estimation algorithms. When divided into a sub-region $A$, the variable of interest is considered first-order stationary if the expected value is constant within $A$. The variable is second-order stationary if the covariance depends only on the separation vector $\mathbf{h}$ within $A$. A random function $Z(\mathbf{u})$ is second-order stationary when:
\begin{align*}
    E\{Z(\mathbf{u})\}                                        & = m                                                    \\
    E\{Z(\mathbf{u}) - m^2\}                                  & = C(0) = \sigma^2                                      \\
    E\{Z(\mathbf{u}) \cdot Z(\mathbf{u} + \mathbf{h})\} - m^2 & = C(\mathbf{u}, \mathbf{u}+\mathbf{h}) = C(\mathbf{h}) \\
                                                              & \forall \ \mathbf{u}, \mathbf{u}+\mathbf{h} \ \in A
\end{align*}

Where $m$, $\sigma^2$ and $C(\mathbf{h})$ are the mean, variance and covariance, respectively, and do not depend on location. The invariance of the random function parameters to location within $A$ allows the relation $C(\mathbf{h}) = \sigma^2 - \gamma(\mathbf{h})$ which is the foundation of variogram interpretation \citep{pyrcz2014geostatistical}.

The linear model of regionalization (LMR) is used to fit experimental variograms with models that ensure positive definite covariance matrices. As not all combinations of variogram models may lead to a permissible model, the LMR constructs a random function $Z(\mathbf{u})$ to be the linear combination of $L+1$ independent, standard random functions $\{Y^{\ell}(\mathbf{u}), \ell=0,\dots,L\}$, each with its own permissible variogram function \citep{goovaerts1997geostatistics}, plus the stationary mean:
\begin{equation*}
    Z(\mathbf{u}) = \sum_{\ell=0}^{L} a^{\ell} Y^{\ell}(\mathbf{u}) + m(\mathbf{u})
\end{equation*}

Where $L$ is the number of nested structures in the model. By convention the $0^{th}$ structure is the isotropic nugget effect. The variogram model of can $Z(\mathbf{u})$ then be expressed as the sum of the of the variograms for each of the factors:
\begin{equation*}
    \gamma_z(\mathbf{h}) = \sum_{\ell=0}^{L} b^{\ell} \Gamma^{\ell}(\mathbf{h}), \ \ \ b^{\ell} = (a^{\ell})^{2}
\end{equation*}

Where $\Gamma_{\ell}(\mathbf{h})$ is the variogram of $Y_{\ell}$ and $b_{\ell}$ represents the variance contribution of each $\ell=0,\dots,L$ factors.

The LMR can be extended to the multivariate case with $k=1,\dots,K$ coregionalized variables. With the liner model of coregionalization (LMC), each coregionalized random function, $Z_{k}(\mathbf{u})$, is also the sum of the standard, uncorrelated factors:
\begin{equation*}
    Z_{k}(\mathbf{u}) = \sum_{\ell=0}^{L} a^{\ell}_{k} Y^{\ell}(\mathbf{u}) + m_{k}(\mathbf{u})
\end{equation*}

Where $a^{\ell}_{k}$ is the contribution of the $\ell^{th}$ factor the $k^{th}$ variable. The direct and cross variograms can be expressed as:
\begin{equation*}
    \gamma_{k,k^{\prime}}(\mathbf{h}) = \sum_{\ell=0}^{L} a^{\ell}_{k} a^{\ell}_{k^{\prime}} \Gamma^{\ell}(\mathbf{h}), \ \ \ k,k^{\prime} = 1,\dots,K
\end{equation*}
The LMC is commonly modeled in the cokriging paradigm for multivariate covariance inference.

\FloatBarrier
\subsection{Factorial Kriging}
\label{subsec:01factorial}

As demonstrated with the LMR notation, the regionalized variable is characterized by $m(\mathbf{u})$, the $L + 1$ $a_{\ell}$ values and the $L + 1$ variograms $\Gamma_{\ell}(\mathbf{h})$. The goal of factorial kriging is to model each nested spatial structure present in the LMR for filtering or feature extraction. The idea is that each regionalized factor has a correlation structure responsible for a different scale of continuity, and they can be estimated independently \citep{matheron1982factorial}. The factors are estimated as linear combinations of the data values \cite{deutsch2007recall}:
\begin{equation*}
    z^{\ell*}(\mathbf{u}) = a^{\ell} Y^{\ell}(\mathbf{u}) = \sum_{i=1}^{n} \lambda_{i}^{\ell} z(\mathbf{u}_{i})
\end{equation*}

% \begin{equation*}
%      m^{*}(\mathbf{u}) = \sum_{i=1}^{n} \lambda_{m,i} z(\mathbf{u}_{i})
%  \end{equation*} 

By convention ordinary kriging is used though there is no reason simple kriging cannot be used \citep{hong2007improved}. The estimation weights for the $\ell^{th}$ factor are obtained by minimizing the estimation variance which leads to the factorial kriging equations:
\begin{equation*}
    \begin{cases}
        % \sum_{i=1}^{n} \lambda_{m,i} C(\mathbf{u}_{i}, \mathbf{u}_{j}) - \mu_{m} = 0, \ \ i=1,\dots,n  \\
        % \sum_{i=1}^{n} \lambda_{m,i} = 1 \\
        \sum_{j=1}^{n} \lambda_{j}^{\ell} C(\mathbf{u}_{i}, \mathbf{u}_{j}) + \mu^{\ell} = C^{\ell}(\mathbf{u}_{i}, \mathbf{u}), \ \ \ i=1,\dots,n \\
        \sum_{j=1}^{n} \lambda_{j}^{\ell} = 0, \ \ \ \ell = 0,\dots,L
    \end{cases}
\end{equation*}

The right hand side covariances are the covariances corresponding to the particular structure $\ell$ being estimated. The sum of the estimated factors returns the original ordinary kriging estimate:
\begin{equation*}
    z^{*}(\mathbf{u}) = \sum_{\ell=0}^{L} z^{\ell*}(\mathbf{u}) + m^{*}(\mathbf{u})
\end{equation*}

Filtering properties of factorial kriging may be helpful if one is interested in removing a particular factor from estimated maps. For example, high-frequency variation from the nugget effect could be filtered by only considering the $\ell=1,\dots,L$ factors. Factorial kriging can be extended to the multivariate context by considering the LMC fitted to the direct and cross variograms where each coregionalized variable $\{Z_{k}(\mathbf{u}), k=1,\dots,K\}$ is a linear combination of the standard, independent factors $\{Y_{v}^{\ell}(\mathbf{u}), v=1,\dots,K; \ell=0,\dots,L\}$ \citep{wackernagel1988geostatistical}:
\begin{equation*}
    z^{\ell*}_{k}(\mathbf{u}) = \sum_{v=1}^{K} a_{k,v}^{\ell} Y^{\ell}_{v}(\mathbf{u}) = \sum_{v=1}^{K}\sum_{i=1}^{n} \lambda_{v,i} z_{v}(\mathbf{u}_{i})
\end{equation*}

% \begin{equation*}
%      m^{*}_{k}(\mathbf{u}) = \sum_{v=1}^{K} \sum_{i=1}^{n} \lambda_{v,i} z_{v}(\mathbf{u}_{i})
%  \end{equation*}

The cokriging equations for a particular spatial component are then:
\begin{equation*}
    \begin{cases}
        \sum_{k^{\prime}=1}^{K} \sum_{j=1}^{n} \lambda_{k^{\prime},j} C_{k,k^{\prime}}(\mathbf{u}_{i}, \mathbf{u}_{j}) + \mu_{k} = C_{k,k}^{\ell}(\mathbf{u}_{i},\mathbf{u}), \ \ \ k=1,\dots,K; i=1,\dots,n \\
        \sum_{j=1}^{n} \lambda_{k,j} = 0  \ \ \ k=1,\dots,K
    \end{cases}
\end{equation*}

Again, the sum of the estimated factors returns the original regionalized variable:
\begin{equation*}
    z^{*}_{k}(\mathbf{u}) = \sum_{\ell=0}^{L} z^{\ell*}_{k}(\mathbf{u}) + m^{*}_{k}(\mathbf{u})
\end{equation*}

Multivariate factorial kriging is a technique for characterizing the regionalized factors $Y_{\ell,k}$ from observations of $Z_{k}$. This technique is advantageous if the correlation between variables is dependent on scale and one would like to extract or filter a particular spatial structure.

Simple factorial kriging equations are equivalent to the simple kriging equations except for the right hand side covariance is the covariance of the nested structure being estimated \citep{hong2007improved}:
\begin{equation*}
    \begin{cases}
        \sum_{j=1}^{n} \lambda_{j}^{\ell} C(\mathbf{u}_{i}, \mathbf{u}_{j}) = C^{\ell}(\mathbf{u}_{i}, \mathbf{u}) \\
        i=1,\dots,n \ \ \ \ell=1,\dots,L                                                                           \\
    \end{cases}
\end{equation*}

\begin{equation*}
    z^{*}(\mathbf{u}) = \sum_{\ell=0}^{L} z^{\ell*}(\mathbf{u}) + m
\end{equation*}

The sum of the estimated factors plus the mean is equal to the simple kriging estimate.

\FloatBarrier
\subsection{Simulating Continuous Variables}
\label{subsec:03simulate}

Kriging, and its variants, generate smooth, deterministic estimates that do not reproduce the true variability of the data \citep{deutsch1992geostatistical}. The smoothness of this estimate is not desirable if the transfer function is sensitive to extreme values; under-representing the variability may be consequential. Simulation methods are designed to reproduce the covariance structure and spatial variability of the input variables, while honouring to data values at their locations \citep{goovaerts1997geostatistics}. Uncertainty in the \gls{RV} is captured by generating $L$ equiprobable realizations. Conventional simulation algorithms are built on the assumption that the input variables are multivariate Gaussian after a univariate normal score transform. Under the multivariate Gaussian assumption, all conditional distributions are defined by the normal equations, and thus linear combinations of the conditioning data \citep{chiles2012geostatistics}. Many algorithms are available for conditional simulation of Gaussian \gls{RF}. \cite{pinto2020independent} presents a comprehensive overview of the most common algorithms and best practices for selecting an algorithm given the problem context.

Gaussian simulation algorithms are prevalent in the mining industry, with \gls{SGS} likely being the most common \citep{rossi2013mineral}. \Gls{SGS} is a \gls{MCS} technique for simulation of Gaussian \glspl{RF} \citep{gomez-hernandez1993joint,isaaks1990application,goovaerts1997geostatistics,gomez-hernandez2021one}. \Gls{SGS} requires (1) parameterizing a multivariate conditional \gls{CDF}, and (2) drawing realizations. Given the dimensionality of the problems faced in mining, (1) is only possible if a parametric, multivariate Gaussian distribution is adopted \citep{leuangthong2008solved}. The \textit{curse of dimensionality} \citep{bellman1961adaptive} and the simplicity and tractability of the Gaussian distribution precludes the use of other distributions.

The following steps generalize the process of generating a realization with \gls{SGS}:
\begin{enumerate}[noitemsep]
    \item Define a random path through the grid nodes.
    \item At each location calculate the first and second order moments of the conditional \gls{CDF} using the normal equations.
    \item Randomly draw a simulated value from the conditional \gls{CDF}.
    \item Add the simulated value to the conditioning data.
    \item Visit the next grid node in the path.
\end{enumerate}

As simulation progresses the number of conditioning data increases. In practice, calculation of the conditional moments is restricted to a local neighbourhood about the location being simulated to prevent unreasonably large systems of equations. Multiple realizations are generated by varying the random path and draws from the conditional \glspl{CDF}.



% Factorial simulation utilizes a conditioning by simple factorial kriging scheme. Conditioning by kriging involves adding a simulated kriging error component to a kriging estimate \citep{chiles2012geostatistics}.
% \begin{equation*}
%     z_{cs}(\mathbf{u}) = z^{*}(\mathbf{u}) + [z_{us}(\mathbf{u}) - z_{us}^{*}(\mathbf{u})]
% \end{equation*}

% Where $z_{cs}(\mathbf{u})$ is the conditioned value, $z^{*}(\mathbf{u})$ is the simple kriging estimate, $z_{us}(\mathbf{u})$ is the unconditionally simulated value and $z_{us}^{*}(\mathbf{u})$ is the simple kriging estimate of the unconditional values at the data locations. This linear combination can be done in a factorial sense as each term can be generated for the given factor \citep{hong2007improved}:
% \begin{equation*}
%     z_{cs}^{\ell}(\mathbf{u}) = z^{\ell*}(\mathbf{u}) + [z_{us}^{\ell}(\mathbf{u}) - z_{us}^{\ell*}(\mathbf{u})]
% \end{equation*}

% Due to the exactitude of kriging, the sum of the conditionally simulated values at the data locations is equal to the sum of the simple kriging factors.

\FloatBarrier
\subsection{Multiple Point Statistics}
\label{subsec:01mps}

Two-point statistics summarize the relationship between points separated by a lag vector $\mathbf{h}$. Two-point statistics such as the variogram or correlogram are measures of linear continuity. \gls{MPS} are measures of continuity between multiple spatial arrangements of points with the possibility of reproducing curvilinear or ordering patterns \citep{boisvert2007multiplepoint}. There are numerous \gls{MPS} including the $n$-point connectivity function \citep{journel1989nongaussian}, the distribution of runs \citep{ortiz2003characterization} and the \gls{MPDF} \citep{boisvert2007multiplepoint}.

The foundation of multiple-point simulation algorithms \citep{guardiano1993multivariate,strebelle2002conditional} is the replacement of inference of two-point statistics with the \gls{MPDF}, where the \gls{MPDF} describes the frequency of occurrence of a particular pattern. The local conditional probabilities are derived from multiple-point configurations allowing for reproduction of non-linear features \citep{silva2014guide}. A challenge of simulation with \gls{MPS} is that inference of the \gls{MPDF} with limited data. This challenge is overcome by extracting the \gls{MPDF} from a \gls{TI} \citep{journel2005covariance}. The \gls{TI}, or substitute for a \gls{RF} model, is an exhaustive image at the support of realizations with the expected geologic variability of the final models \citep{gomez-hernandez2021one}. Though the \gls{TI} allows for inference of \gls{MPS} not available from the data, one is faced with selecting an appropriate \gls{TI}. \cite{boisvert2007multiplepoint} describes the choice of \gls{TI} being analogous to variogram modeling in the two-point paradigm. In general, the TI should represent the physics of the underlying geological process and be characteristic of the conceptual geology. \Glspl{TI} can be generated from outcrop data, object based models, or process based models \cite{tahmasebi2018multiple}. More recently, \cite{minniakhmetov2022highorder} present methodology for high-order simulation of categorical variables that does not rely on a training image.

Traditionally, \gls{MPS} simulations are focused on categorical modeling of stratigraphic deposits where object-based or process-mimicking models are applicable across geologic environments \citep{mariethoz2014multiplepoint}. Multi-point simulation of continuous variables is generally approached with high-order spatial cumulants, discussed in the next section.


\FloatBarrier
\subsection{High-Order Simulation}
\label{subsec:03hosim}

High-order simulation methods are similar in concept to the multi-point simulation framework, with applications for both continuous and categorical variables. These methodologies are predominantly data-driven, complemented by a \gls{TI}. Rather than inferring high-order conditional probabilities exclusively from a \gls{TI}, they are approximated by spatial cumulants calculated from the data. High-order simulation based on spatial cumulants is proposed by \cite{dimitrakopoulos2009highorder} and \cite{mustapha2010highorder,mustapha2011hosim}. A cumulant is defined as the logarithm of the moment generating function of a \gls{RF}. The idea is that spatial cumulants can be used to generalize the covariance to orders beyond two; \cite{dimitrakopoulos2009highorder} shows that the first and second order cumulants are the mean and variance, respectively. The high-order cumulants, similar to the concepts \gls{MPS}, are able to characterize complex non-linear and non-Gaussian geologic features. By incorporating spatial cumulants up to order-five, \cite{mustapha2010highorder} show the ability to capture multi-point periodicity, connectivity of extreme values and complex geometric characteristics. The key idea in the \textit{HOSIM} approach \citep{mustapha2011hosim} is that the local conditional \glspl{CDF} take the form Legendre polynomial expansions, where the polynomial coefficients are approximated with spatial cumulants \citep{mustapha2010highorder}. Both a \gls{TI} and available data are used to infer the Legendre polynomial coefficients; third- and fourth-order statistics are estimated from the data, while higher-order features come from the \gls{TI} \citep{minniakhmetov2022highorder}. \cite{minniakhmetov2017joint} extends the high-order simulation framework to the multivariate context. Further refinements to polynomial approximations are given in \cite{minniakhmetov2018highorder}, \cite{yao2020highorder} and \cite{yao2021learning}.

Though largely data-driven, a drawback of these methodologies is that a \gls{TI} is required. The inference of the spatial cumulants is drawn from both the \gls{TI} and available data; if the high-order multi-point replicates are not available in the data, they are incorporated from the \gls{TI} \citep{mustapha2010highorder,yao2021training}. Selecting or generating a \gls{TI} for continuous variables in mining problems is challenging without dense sampling; \cite{minniakhmetov2018highorder} and \cite{decarvalho2019highorder} use blast hole samples for \gls{TI} construction. \cite{yao2021training} presents methodology for \gls{TI} free simulation with aggregated kernel statistics.




% \begin{enumerate}

%     \item HOSIM:
%           \begin{enumerate}
%               \item \cite{dimitrakopoulos2009highorder,mustapha2010highorder,mustapha2011hosim,tamayomas2016testing}
%               \item based on training image
%               \item Legendre Polynomials
%               \item spatial cumulants
%               \item combinations of moments of statistical parameters that characterize non-Gaussian random fields
%               \item absence of any distribution-related assumptions
%           \end{enumerate}

%     \item Ilnur's approach:
%           \begin{enumerate}
%               \item \cite{minniakhmetov2017joint,minniakhmetov2018highorder, decarvalho2019highorder}
%               \item Legendre-like orthogonal splines
%               \item The coefficients of spline approximation are estimated using high-order spatial statistics inferred from the available sample data, additionally complemented by a training image.
%           \end{enumerate}

%     \item Yao (2021):
%           \begin{enumerate}
%               \item \cite{yao2020highorder,yao2021training,yao2021learning}
%               \item A training image free, high-order sequential simulation method
%               \item Legendre moment kernel spaces
%               \item All methods based on the approximation of a conditional distribution using Legendre polynomials
%           \end{enumerate}

% \end{enumerate}


\FloatBarrier
\subsection{Imputation}
\label{subsec:01impute}

Imputation is a key component of probabilistic modeling of continuous heterotopic data \citep{barnett2015multivariate,silva2018multivariate,hadavand2023spatial} and in truncated Gaussian categorical modeling techniques \citep{silva2018enhanced,arroyo2020iterative,madani2021enhanced,armstrong2011plurigaussian}. Simulation often considers a multiple imputation framework \citep{little2019statistical} where one generates realizations of missing values to correctly transfer imputation uncertainty to the final models.

In the heterotopic data context, the goal is to fill in missing values. Modern geostatistical workflows necessitate the use of multivariate transforms like \gls{PPMT}, \gls{SCT} and \gls{PCA} which require homotopic data. Any heterotopic data observations must be either excluded or imputed. Geologic data is often ``missing not at random'' and simply excluding heterotopic observations can lead to biases in the final model \citep{dasilva2019treatment}. The goal of any imputation algorithm is to define the distribution of the missing values conditional to the observed values. \cite{barnett2015multivariate} proposed a non-parametric methodology for imputing continuous variables based on Bayesian updating and Gibbs sampling.  A \gls{SK} mean and variance is merged with a collocated conditional distribution estimated with a multivariate \gls{KDE}. The parameters calculated with \gls{SK} account for the univariate spatial component while the \gls{KDE} accounts for the collocated multivariate component. Imputed values are then drawn from the merged distribution. This methodology is computationally expensive with many data due to \gls{KDE} calculations and Gibbs sampler iterations. \cite{silva2018multivariate} proposed a non-parametric imputation algorithm based on \glspl{GMM} to relieve the burden of \gls{KDE} calculations. The local conditional spatial distribution is still defined by \gls{SK}, however the collocated multivariate density is sampled from a fitted \gls{GMM}. \cite{hadavand2023spatial} proposed another non-parametric methodology where the multivariate relationships are characterized by deep learning rather than a \gls{GMM}. Two neural networks are trained to quantify the moments of the conditional missing value distribution; one for the mean, and another for the second, third, and fourth order moments. A lambda distribution is fit given the conditional moments that characterizes the collocated multivariate relationship.

Latent imputation is a special scenario where all values are missing \citep{little2019statistical}. Truncated Gaussian modeling techniques are based on the idea that categorical observations are generated by the truncation of underlying latent variables \citep{matheron1987conditional}. These latent variables are not observed and are a synthetic feature of the model. Imputation of latent variables subject to categorical observations is commonly approached with a Gibbs sampler \citep{geman1984stochastic}, where directly sampling the multivariate truncated Gaussian distribution is not possible, but sampling the marginal conditional distributions is possible \citep{silva2018enhanced,arroyo2020iterative,madani2021enhanced}. Though the Gibbs approach is common, \cite{emery2014simulating} and \cite{silva2018enhanced} note convergence issues with spatially correlated variables. More recently \cite{lauzon2020calibration,lauzon2020sequential,lauzon2023joint} proposed the sequential spectral turning band simulator as an alternative for Gibbs sampling, where Gaussian \glspl{RF} are constructed by addition of cosine functions. The proposed approach begins with Gaussian \glspl{RF} that meet the inequality constraints, and then the spatial component is introduced by sampling the spectral density. The Gibbs sample approach begins with Gaussian \glspl{RF} that have the correct spatial structure, and gradually introduce constraints through Gibbs iterations. The authors show the spectral approach is a valid alternative to the Gibbs sampler with stable convergence of many data, multiple rock types, and complex truncation rules.


% \begin{enumerate}
%     \item Gibbs:
%           \begin{enumerate}
%               \item \cite{barnett2015multivariate,silva2018multivariate,silva2018enhanced,arroyo2020iterative,madani2021enhanced}
%           \end{enumerate}

%     \item Lambda distribution:
%           \begin{enumerate}
%               \item \cite{hadavand2023spatial}
%           \end{enumerate}

%     \item Spectral:
%           \begin{enumerate}
%               \item \cite{lauzon2020calibration,lauzon2020sequential,lauzon2023joint}
%           \end{enumerate}
% \end{enumerate}


\FloatBarrier
\section{Extreme Value Background}
\label{sec:01evtreview}

The following section provides background literature for extreme values, outliers, and their significance in mining related problems. Though \gls{EVT} is not directly incorporated in the \gls{NMR} framework, the statistical foundation is presented for completeness.

\FloatBarrier
\subsection{Outlier Detection}
\label{subsec:01outlier}

Outlier detection is relevant to all statistical modeling and the body of literature is vast \citep{zimek2018there}. \cite{hodge2004survey} note that outlier detection may also be referred to as novelty, noise, anomaly or deviation detection. In all cases, however, an outlier is an observation sufficiently dissimilar to other observations \citep{barnett1984outliers}. \cite{wang2019progress} group outlier detection methodologies into (1) statistical methods, (2) distance-based methods, (3) density-based methods, (4) clustering-based methods, and (5) ensemble-based methods. Statistical methods, either parametric or non-parametric, compare the relationship of potential outliers with the remaining distribution. Distance-based methods (euclidean or non-euclidean) compare the distance between observations where potential outliers are ``far'' from other observations. Density-based methods consider outliers to be in a low density regions of a \gls{PDF}. Clustering based methods classify each observation and potential outliers are not within, or near dense clusters. Finally, ensemble methods are combinations of dissimilar methodologies to create a more robust outlier detection model. \cite{hodge2004survey,wang2019progress,pimentel2014review,boukerche2021outlier} provide comprehensive reviews of outlier detection methodologies with applications to fraud detection, cybersecurity, sensor networks, image processing, time series and data streams, medical diagnostics and industrial monitoring. \cite{pang2022deep} present a comprehensive review of outlier detection with deep learning though the concepts are largely beyond the scope of this thesis.

% Outliers are significant when considering data from potentially heavy-tailed distributions; there are likely few examples of a critical class. Extreme values are different from traditional outliers in the sense that all extreme values are outliers, but the reverse is not always true \citep{aggarwal2016outlier}. Most EVT outlier approaches model the ``inliers'' and test potential outliers against this model. Outlier detection with explicit use of extreme value theory have been applied to outlier detection in biomedical data processing \citep{roberts2000extreme}, signal processing processing \citep{hazan2012extreme}, image noise removal \citep{roberts1999novelty}, monitoring of critical equipment like jet-engines \citep{clifton2014extending}, cybersecurity and stock price analysis \citep{siffer2017anomaly}.

% \begin{enumerate}
%     \item Update references here
%     \item ML methods like LOF, Isolation Forest, MCD
%     \item \cite{pimentel2014review, wang2019progress,pang2022deep,boukerche2021outlier}
% \end{enumerate}

\FloatBarrier
\subsection{Geospatial Outlier Detection}
\label{subsec:01miningoutlier}

Outlier detection in the mining industry is based largely graphical methods \citep{leuangthong2015dealing, silva2021classification}. These methods are necessarily subjective as the practitioner must interpret a plot and select a threshold to define an outlier. \cite{leuangthong2015dealing,nowak2013suggestions,rossi2013mineral} provide practical advice on threshold selection for \glspl{CPP} where breaks in the upper tail may represent outlier populations. \cite{babakhani2014geostatistical} proposed a spatial bootstrap based methodology to characterize the relationship between the naive mean and capped mean. The goal of the methodology is to identify the values that cause higher mean values as potential outliers.

The spatial characteristics of sample values are often omitted when identifying outliers. The spatial context of the samples is likely relevant; an extreme value surrounded by other high values may not be an outlier. The correlation structure of the variable(s) is relevant to understanding the spatial context \citep{filzmoser2014identification}. \cite{babakhani2014geostatistical} proposed methodology for identification of spatial outliers based on the rank transform of cross-validation estimates. Outliers identified by the rank transform consider the spatial neighbourhood and are different from simply considering the univariate distribution. Many authors \citep{filzmoser2020multivariate,filzmoser2014identification,ernst2017comparison, leung2021sample,harris2014multivariate,chen2008detecting} have proposed methodology for spatial multivariate geochemical outlier detection using the \gls{MCD} estimator of \cite{rousseeuw1999fast}. The Mahalanobis distance \citep{mahalanobis2018generalized} is a common metric for comparing features in multivariate space, however it is sensitive to presence of outliers \citep{filzmoser2014identification}. The \gls{MCD} is a robust measure of the global correlation structure in the presence of outliers. Given the \gls{MCD} estimate, various measures of local and global multivariate distances are calculated to determine an outlier score. \cite{chen2008detecting} calculate an outlying distance based on the differences between each sample value and the median value over its neighbours. \cite{filzmoser2014identification} calculate a degree of isolation for each observation based on the robust Mahalanobis distance (\gls{MCD} estimate) gloablly, as well as within a local neighbourhood; values above a defined threshold are considered potential multivariate outliers.

\FloatBarrier
\subsection{Geospatial Outlier Management}
\label{subsec:01outliermanage}

Once outliers have been identified, practitioner must decide a management strategy. Managing extreme values and outliers prior to resource estimation is a key component of resource estimation, particularly for heavy-tailed mineral deposits. The general idea is that unadjusted grades may lead to unrealistic local estimates adjacent to high-grade composites \citep{nowak2013suggestions}. \cite{leuangthong2015dealing} provide an overview of outlier management practices in the context of mineral resource estimation. The practices are grouped into three general categories: (1) choosing appropriate domains; (2) grade capping; and (3) limiting the influence of outliers through the estimation process. The suggested practices are largely based on trade-craft and practitioner experience.

There are numerous practical methods for choosing how to explicitly manage the influence of high-grade samples. Traditional methods published in mining technical reports include decile analysis \citep{parrish1997geologist}, cumulative probability plots \citep{rossi2013mineral}, cutting curves \citep{roscoe1996cutting}, coefficient of variation thresholds \citep{parker1991statistical}, production reconciliation, arbitrary quantiles, metal-at-risk \citep{parker2006}, and indicator correlations \citep{nowak2019optimal}.  Some techniques such as \gls{MIK} \citep{journel1983nonparametric} directly treat outliers through the specification of upper class means \citep{rossi2013mineral}. \cite{babakhani2014geostatistical} proposed a less conventional method of projecting outliers to an extra dimension to reduce the local influence. This method requires specifying a distance $d$ to project the outlier; the distance could be determined by calibrating kriging results to an expected value from simulation, or selection of a specific quantile. \cite{babakhani2014geostatistical} also proposed the calibration of a capping limit based on an expected value from simulation. This approach is similar to the previous, however instead of projecting an outlier some distance, $d$, away, the outlier value is reduced. Within some local volume of influence, the outlier grade is reduced until the kriged grade matches the expected value of 100 realizations. The idea is that simulation is more robust in the presence of outliers and this resistance can be exploited to calibrate a capping grade. \cite{rivoirard2013topcut,maleki2014capping} propose the selection of an optimal capping limit through analysis of the ratios of direct and cross indicator variograms. The goal is to identify the range $[z_{min}, z_{max}]$ in where the capping limit should fall. The minimum value is defined by first threshold where the ratio of the cross and direct indicator variogram values are constant, independent of the lag vector. The maximum value is defined by threshold where the residual is pure nugget.

Many methodologies have been proposed to circumvent the practice of capping. \cite{costa2003reducing} proposed a variant of robust kriging \citep{hawkins1984robust} where the weight to outlier samples if different from that of inliers. The ``robust-edited'' values are adjusted based on the difference between the sample value and the weighted median at the same location. \cite{rivoirard2013topcut} proposed the decomposition of the grade value into a truncated grade, a weighted indicator above the top cut grade, and a residual. If the cutoff is sufficiently high, the residual is uncorrelated with the truncated grade and the indicator. The final estimate is then a kriged estimate of the residual plus a cokriged estimate of the indicator and truncated grade. \cite{maleki2014capping} proposed a similar decomposition approach where they suggest spatial prediction is improved as outlier values are omitted from variogram calculations. \cite{fourie2019limiting} proposed a methodology that post-processes kriging weights to generate realistic estimates without grade smearing. Kriging weights are adjusted based on ratio of the frequency of outlier samples to median samples. The methodology requires a subjective selection of frequency bins after normal score transforming the variable. Simply restricting the spatial range of influence of outlier values during estimation or simulation is a practical approach to circumvent capping. More recently, \cite{silva2021classification} proposed methodology for adjusting outlier grades based on Bayesian updating, and \cite{dutaut2021new} propose using an error-free \gls{CV} calculated from coarse duplicate correlation to determine a capping limit.

\subsection{Extreme Value Theory (EVT)}
\label{subsec:01evt}

\Acrfull{EVT} is a statistical foundation for quantifying probability distributions and magnitudes of atypically high or low events. Early works by \cite{frechet1927loi}, \cite{fisher1928limiting}, \cite{mises1936distribution} and later by \cite{gnedenko1943distribution}  systematically developed the theory and formalized the asymptotic distribution of extremes for block maximums. \cite{fisher1928limiting} show that for a sequence of \gls{iid} random variables with a common distribution function $F$, the so-called ``block maximum'', $M_n = max\{Z_1,...,Z_n\}$, can only converge to one of three distributions:

\begin{equation}
    G_{I}(z) = exp\left(-exp\left(-\left(\frac{z-\mu}{\sigma}\right)\right)\right), \ \ -\infty < z < \infty
    \label{eq:1}
\end{equation}

\begin{equation}
    G_{II}(z) =
    \begin{cases}
        0,                                                           & z \leq \mu \\
        exp\left(-\left( \frac{z-\mu}{\sigma} \right)^{-\xi}\right), & z > \mu
    \end{cases}
    \label{eq:2}
\end{equation}

\begin{equation}
    G_{III}(z) =
    \begin{cases}
        exp\left(-\left( -\left(\frac{z-\mu}{\sigma} \right)^{\xi} \right) \right), & z < \mu    \\
        1,                                                                          & z \geq \mu
    \end{cases}
    \label{eq:3}
\end{equation}

% To avoid degeneracy, the variable $M_n$ is normalized to $M_n^* = a_n^{-1}(M_n - b_n)$ for a sequence of ``normalizing'' constants $a_n > 0$ and $b_n$. \cite{fisher1928limiting} show that if these sequences of real numbers can be chosen such that $M_n^*$ has a non-degenerate limiting distribution it must be one of type I, II or III. These are the only possible limits for the distributions of $M_n^*$ regardless of the population distribution $F$ - this Extremal Types Theorem is essentially analogous to the Central Limit Theorem for extreme values \citep{coles2001introduction}. 

These collectively are termed extreme value distributions with \ref{eq:1} being Gumbel-type, \ref{eq:2} being Fr\'echet-type and \ref{eq:3} being Weibull-type. Each distribution has a location $\mu$, scale $\sigma$ and \ref{eq:2} and \ref{eq:3} have shape parameter $\xi$. The extreme value distributions are obtained as limiting distributions of $M_n$ as $n \to \infty$. The limit distributions for block maximum can be grouped into a single-family termed the \gls{GEV} distribution \citep{dehaan2007extreme}:
\begin{equation}
    G(z) = exp\left(-\left(1+\xi\left(\frac{z-\mu}{\sigma}\right)^{\frac{-1}{\xi}}\right)\right)
    \label{eq:GEV}
\end{equation}

The block maximum methodology may be inefficient as it ignores all but the maximum value in a given block \citep{davison2015statistics}. The second approach to identifying extremes is the so-called ``peak-over-threshold'' (POT) method. Consider a random variable $Z$ with distribution function $F$. The CDF of the excess over some threshold $u$, is defined by:
\begin{equation}
    \begin{aligned}
        F_u(y) & = P(Z - u \leq y | Z > u)                               \\
               & = \frac{F(u+y) - F(u)}{1-F(u)}, \ \ 0 \leq y \leq z_F-u
    \end{aligned}
    \label{eq:exceed}
\end{equation}

Where $y = z - u$ and $z_F$ is the right endpoint of $F$ \citep{gilli2006application}. \cite{pickands1975statistical} states that if $u$ is large, the conditional distribution of $Z$ given $Z$ is much larger than $u$ is well approximated by the \gls{GPD}:
\begin{equation}
    G(y) =
    \begin{cases}
        1 - \left(1 + \frac{\xi y}{\sigma}\right)^{\frac{-1}{\xi }}, & \text{ if } \xi  \neq 0 \\
        1-exp\left(\frac{-y}{\sigma} \right),                        & \text{ if } \xi  = 1
    \end{cases}
    \label{eq:GPD}
\end{equation}

The conditional distribution of the exceedances (Equation~\ref{eq:exceed}) can be modeled asymptotically with the \gls{GPD} by estimating the scale ($\sigma$) and shape ($\xi$) parameters.

\FloatBarrier
\subsection{Spatial Extreme Value Theory}
\label{subsec:01spevt}

The assumption of \gls{iid} observations underlies classical \gls{EVT}. In many real-world applications, one must account for correlation in space or time and the multivariate nature of regionalized variables. Spatial extreme value theory represents an intersection between classical \gls{EVT} and geostatistics \citep{neves2015geostatistical}. The primary difference between spatial \gls{EVT} and geostatistics is that in the geostatistical framework, there are no observed replicates at $Z(\mathbf{u})$. Fitting of a \gls{GEV} or \gls{GPD} in the classical \gls{EVT} sense requires multiple realizations of $Z_i(\mathbf{u})$ for parameter inference.

Spatial \gls{EVT} builds on the concepts of max-stable distributions extending to the max-stable process. A max-stable process is the infinite dimension generalization of the max-stable distribution where all lower-order marginal distributions are \gls{GEV} distributions \citep{schlather2003dependence}. If there exists normalizing constants $a_n(\mathbf{u}) > 0$ and $b_n(\mathbf{u})$ such that $a_n^{-1}(\mathbf{u})\{\max_{i=1,...,\infty} Z_i(\mathbf{u}) - b_n(\mathbf{u})\} = Y(\mathbf{u})$ then $Y$ is a max-stable process \citep{dehaan2007extreme}. The max-stable process is applicable to maximums as stable Gaussian processes with finite variance are applicable to the average \citep{chiles2012geostatistics}. All marginal distributions of a max-stable process are \gls{GEV} distributions defined by Equation~\ref{eq:GEV}.

% Unlike a Gaussian process which is fully defined by its correlogram $\rho(\mathbf{h})$, there is no unique model for max-stable processes \citep{chiles2012geostatistics}. Many different models are found in the literature. First introduced by \cite{smith1990maxstable} and later modified by \cite{schlather2002models}, Gaussian storm and extremal Gaussian processes are commonly used for modeling spatial extremes. The Brown-Resnick processes \citep{brown1977extreme,kabluchko2009stationary} relaxes the assumption of second-order stationarity and permits the use of the variogram, which has shown to be practical in practice \citep{gaume2013mapping}. The extremal-$t$ model \citep{opitz2013extremal} is another popular max-stable model in the literature. A consequence of the max-stable processes is asymptotic dependence in the tails \citep{davison2013geostatistics}.


\FloatBarrier
\section{Optimization Background}
\label{sec:01optreview}

The following section provides background literature regarding geoscience and engineering related inverse problems and the use of optimization to infer unknown model parameters.

\FloatBarrier
\subsection{Inverse Problems}
\label{subsec:01inverse}

Inverse problems encompass a broad class of problems where the objective is to infer the underlying causes or parameters of a system from observed data or measurable outputs \citep{sen2013global}. The prediction of a response is a forward problem, while the use of a response, or observed measurements to infer the properties of a model is an inverse problem \citep{tarantola2005inverse}. Inverse problems arise in various scientific disciplines, including physics, engineering, geosciences, medical imaging, and more. Geospatial inverse problems are common in both the fields of geophysics \citep{linde2015geological,giraud2019integration,grana2022probabilistic} and hydrogeology \citep{zhou2014inverse,ghorbanidehno2020recent} where the underlying geologic model is unknown, however a set of measured responses, such as hydraulic conductivities or seismic properties, are known. The inverse problem involves inferring interpretable geologic properties of the unknown model, such as lithology or porosity, that satisfy the observed measurements. \cite{grana2022probabilistic} describe these problems as rock-physics inversions where rock and fluid properties are predicted from seismic measurements.

The process of solving inverse problems involves constructing a mathematical forward model that describes the relationship between the unknown parameters and the observed data; then using this model to infer the unknown parameters. Seismic wave propagation and rock-physics models are generally well understood forward models in geophysics \citep{grana2022probabilistic} where hydrogeological forward models consider mass conservation and Darcy's law to predict hydraulic head, drawdown or solute concentrations \citep{zhou2014inverse}. A challenge of inverse problems is ill-posedness, or the lack of unique solution \citep{tarantola2005inverse}. It is possible that there are multiple (or infinite) solutions that are valid given the observed data. As an exact solution is rarely possible in natural, non-linear systems, one looks for possible solutions that are close to actual observations \citep{bardossy2016random}. For this reason many inverse problems are framed as an optimization problem, minimizing an objective function relevant to the problem at hand \citep{giraud2019integration,nava-flores2023high,athens2022stochastic}. The objective function is minimized iteratively which is generally computationally expensive \citep{zhou2014inverse}. A forward modeling operator predicts an outcome for the current state of the model parameters and the objective function evaluates the loss between this prediction and the observed measurements. The optimization algorithm updates the parameter vector until it achieves an acceptable match between the model output and observed measurements. Any iterative optimization algorithm may be used; \cite{athens2022stochastic} use gradual deformation to generate a set of perturbed model realizations; \cite{nava-flores2023high} use simulated annealing for joint inversion of gravity gradient data; \cite{balkaya20173d} use differential evolution, and \cite{davilarodriguez2024threedimensional} a general evolution strategy for inversion of magnetic anomalies.

% Due to the inherent complexity and potential non-uniqueness of inverse problems, careful consideration of data quality, noise, and computational stability is essential to obtain meaningful and reliable solutions.

% \begin{enumerate}
%     \item something about the forward problem
%     \item prior knowledge is the specification of the pool
%     \item prior knowledge is the conceptual geologic model
%     \item no data reproduction at this point, only reproduction of pertinent statistics
%     \item constraints on wt magnitude, nscore keeps solution reasonable
% \end{enumerate}


\FloatBarrier
\subsection{Genetic Algorithms}
\label{subsec:01genetic}

\Glspl{GA} are metaheuristic global optimization algorithms inspired by natural processes like evolution and natural selection \citep{cui2024applications}. This family of algorithms was first proposed by \cite{holland1992genetic} and have wide-ranging applicability to engineering optimization problems as robust global optimizers that do not require differentiable objective functions \citep{carbas2021nature}. The \gls{GA} framework can efficiently explore a large solution space, handle constraints, and lends its self to parallel applications. In general, genetic algorithms are derivative-free, population-based algorithms with three main characteristics: crossover, mutation, and selection operators \citep{yang2018nature}. Each member of the population is feasible solution vector. The crossover operator exchanges information between members of the population and can enhance key features of the population; the mutation operator permits exploration of the solution space by increasing population diversity; the selection process drives the population towards convergence by selecting the fittest, or most elite member of the population for mutation and crossover \citep{yang2018nature}. The fitness of a population member is evaluated with a problem specific objective function. \cite{yang2018nature} describes the general behaviours and components of \glspl{GA} as:
\begin{itemize}[noitemsep]
    \item A population of agents that represent solution vectors, each with an associated fitness.
    \item The population is evolved through mutation and crossover operators. The algorithm converges when all members of the population are sufficiently similar.
    \item New solutions are generated with random perturbations to avoid local optima.
    \item \Glspl{GA} search both locally and globally, with the ratio of local and global search controlled by the genetic operators.
    \item \Glspl{GA} employ a survival of the fittest approach, where the fittest members of the population are retained for the next generation, driving the population towards convergence.
\end{itemize}

\Gls{DE}, first proposed by \cite{storn1997differential}, is a \gls{GA} that uses the scaled difference between population members as a mutation operator.

\begin{enumerate}
    \item \cite{price2013differential, georgioudakis2020comparative,bilal2020differential, piotrowski2017review, penunuri2016study, conn2009introduction, rios2013derivativefree, yang2010nature, yang2018nature, carbas2021nature}
\end{enumerate}

\FloatBarrier
\section{Thesis Outline}
\label{sec:01outline}

Chapter \ref{ch:02outlier} presents a discussion on outlier management in the mining industry. Though the \gls{NMR} framework does not require explicit management of extreme values, it would be remiss to not discuss capping due to its ubiquitous presence and relation to outliers. A range of outlier management tools are discussed and a novel algorithm for the identification of outliers in a spatial context is presented. The chapter finishes with an analytical model for predicting the frequency of intersecting extreme values.

Chapter \ref{ch:03framework} explores the core components of the \gls{NMR} framework. It begins with the concepts of high-order connectivity and the relationship with non-Gaussianity. The network components are discussed: (1) definition of a latent Gaussian pool, (2) non-linearity and mapping to observed space, (3) latent imputation, and (4) continuous simulation and mapping. Here the parameterization of the \gls{NMR} is posed as an inverse problem. The chapter finishes with a synthetic, non-Gaussian example to emphasize the effects of high-order connectivity on contained resources.

Chapter \ref{ch:04implement} presents details of implementing the \gls{NMR} including the network architecture, activation function, latent factor design and parameter inference via optimization. The effects of mixing latent factors is discussed in detail, as well as the formulation of an objective function relevant to the modeling goals. The concepts of \acrfull{DE} and its application to \gls{NMR} parameter optimization are presented, followed by checking and validating the network output. A \gls{3D} example is introduced, and carried over into chapter \ref{ch:05impute}. Finally, practical implementation details are discussed including the potential non-uniqueness of the solution.

Chapter \ref{ch:05impute} introduces a novel algorithm for imputation of latent factors within the \gls{NMR} framework. Imputation concepts and traditional Gibbs sampler approaches are touched on, followed by a presentation of \acrfull{SGRI}. The algorithm is an iterative, sequential imputation algorithm that uses the normal equations and rejection sampling to impute spatially correlated latent variables. Minimum acceptance criteria for checking the latent realizations are presented, followed by conditional simulation and practical checking using the example from Chapter \ref{ch:04implement}.

An application of the complete \gls{NMR} framework with a real dataset is shown in Chapter \ref{ch:06casestudy}. The data comes from an operating underground mine where personal have noted that multivariate Gaussian simulation algorithms do not reproduce the connected high-grade features observed in drillhole data. This scenario is the ideal application of the \gls{NMR}, where it shows a 7\% improvement over \gls{SGS} in high-grade stopes. The \gls{NMR} results are validated with a hold-out dataset.

Chapter \ref{ch:07conclusions} summarizes the contributions made in this thesis. Consideration is given to the limitations of the developed methodologies as well as avenues for future work and improvements to the \gls{NMR} framework. All software developed for the purpose of this research is documented in the appendices.