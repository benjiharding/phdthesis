%!TEX root = ../Thesis_example_compile_this.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{ch:01intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The field of geostatistics encompasses the problem of spatial prediction and the characterization of uncertainty within geological systems \citep{deutsch1992geostatistical}. We are concerned with describing the spatial aspects of mineralization as well as its variability or dispersion. Geostatistics was developed based on the need to forecast the recoverable resources at unsampled locations in mineral deposits \citep{matheron1963principles}. Though initially developed in a mining context, geostatistics has found practical uses in many fields concerned with spatially correlated data such as petroleum, hydrogeology, environmental science, remote sensing and others \citep{goovaerts1997geostatistics}.

Geostatistics utilizes observed categorical and continuous properties to generate exhaustive numerical models of the subsurface. These models are either deterministic or probabilistic, where equiprobable realizations are generated through stochastic simulation \citep{chiles2012geostatistics}. These realizations honour the spatial and multivariate characteristics of the input data with statistical fluctuations and provide a measure of joint uncertainty within the region of interest \citep{rossi2013mineral}. Characterizing and quantifying geologic uncertainty gives engineers and decision-makers practical tools for optimizing orebody extraction.

Mineral deposits, particularly precious metals, often exhibit strongly positively skewed grade distributions. These distributions pose challenges for spatial prediction as there are usually limited data characterizing the upper tail. Some component of the high values in the upper tail are characterized as ``outliers'' based on a subjective threshold. There is a risk of local overestimation with smooth kriging estimators if sparse, high-value data are left unmanaged \citep{leuangthong2015dealing}. Standard practice in mining is to cap high values to a maximum to avoid local conditional bias, but those high values may have tremendous economic value. Appreciating the potential and the upside of such values in a quantitative and repeatable manner is something of great interest.

The term ``extreme value'' is not regularly used in the mining industry; rather, ``outlier'' is used. Some mineral distributions with high coefficients of variation likely do contain extreme values in the classic statistical sense. It could be argued that a mineral deposit is an extreme value in the context of regional geology. Extreme value theory (EVT) is a statistical foundation for quantifying probability distributions and magnitudes of atypically high or low events. Extreme values are forecast using currently observed extremes \citep{gumbel1958statistics}. The properties of extreme values and their statistics are governed by upper or lower tails of the underlying distribution. Classical EVT attempts to characterize the underlying extreme value distribution based on extreme order statistics or isolating values above some high threshold. Extreme value distributions like the generalized extreme value and generalized Pareto distributions are commonly applied in domains dealing with time-series data such as meteorology, hydrology, economics and insurance \citep{reiss2007statistical}.


\FloatBarrier
\section{Problem Statement}
\label{sec:01problem}

Mitigating the impact of extreme values on resource estimation is a long-standing issue. Extreme values present unique challenges in the sense there usually few samples and understanding their spatial distribution is difficult. This is coupled with the potential of significant economic risk if mismanaged. To mitigate risk of overestimation, high grades are typically capped in practice. Numerous problems remain outstanding concerning (1) objectively defining what an extreme value is; (2) explicit approaches for limiting extreme value influence; (3) characterizing the statistical, or distribution component of extreme values; (4) characterizing the spatial component of extreme values and (5) develop practical advice to combine the statistical and spatial components.

The mining industry currently has no definitive consensus regarding extreme value management, and many approaches are developed on an ad hoc basis. Should high grades be capped? Should sub-regions be delineated to isolate higher grades? These questions influence the final metal content of our resource predictions. The impact of restricting outliers may be significant depending on the distribution of the available data. Many industry practices are born from practicality and based on experience-driven rules-of-thumb \citep{nowak2013suggestions}. These practices motivate a need for exploratory data analysis and objective outlier identification tools framed in the perspective of extreme value theory.

High-grade values are generally sparse in mining data sets. Though sparse, these high-grades values may contribute significantly to the project economics. For example, at the Brucejack deposit in Northwest British Columbia, the top 1\% of drill core samples contain 83\% of the total deposit metal (Au) \citep{pretium2020}. Anecdotal evidence from producing gold mines suggests this scenario poses an economic risk if samples in the upper tail are not explicitly managed. This issue is potentially exacerbated by the use of smooth deterministic estimators such as kriging. There is a need for a statistical framework to assess the probability of extreme values and the influence of capping limits on potential resources.

The spatial continuity of extreme values in the upper tail may not be the same as low values. The multivariate Gaussian assumption underlying many geostatistical algorithms does not allow for spatial connectivity of extreme values, nor does it allow for asymmetry in the loss of correlation away from the median \citep{journal1989nongaussian}. Multiple indicator kriging (MIK) \citep{journel1983nonparametric} was conceived for this purpose however it has proved difficult to implement effectively in practice and can be outperformed by simpler Gaussian techniques \citep{vincent2021mik}. The challenges associated with MIK and traditional extreme value management motivate the need for a new spatial model to characterize the non-linear spatial continuity of extreme values.

\FloatBarrier
\subsection{Thesis Statement}
\label{subsec:01thesis}

The proposed research hypothesizes that numerical geologic models would be improved by explicitly considering the presence and spatial structure of extreme values. It is proposed that breaking down regionalized variables into their underlying latent factors will better characterize high-grade geologic features.

It is hypothesized that developing a non-linear network model of (co)regionalization will improve the prediction of in-situ resources. This is particularly true for strongly positively skewed distributions such as precious metals, uranium or diamonds. Non-linearity will allow the spatial model to adapt to complex features and better control the well-understood indicator asymmetry between low and high grades.

Additionally, it is hypothesized that developing objective measures to identify spatial extreme values and best practices for explicitly managing these values is of great practical importance to geostatistics practitioners in the mining industry.

% \vspace{\baselineskip} 

\begin{tcolorbox}
    \textbf{Thesis Statement:} \textit{The breakdown of regionalized variables into fundamental components coupled with non-linear models of regionalization will improve probabilistic modeling of strongly positively skewed grade distributions.}
\end{tcolorbox}



\FloatBarrier
\section{Literature Review}
\label{sec:01litreview}

The proposed research covers a wide range of subjects involving modeling continuous, positively skewed variables with geostatistical simulation. The relevant literature is reviewed in this section.

\FloatBarrier
\subsection{Geostatistics}
\label{subsec:01geostats}

Geostatistics is a field of applied statistics concerned with the characterization and modeling of spatially correlated variables. A variable that is dispersed in space and exhibits spatial structure is said to be regionalized \citep{matheron2019matheron}. A foundation of geostatistics is the concept of random variables where unknown values, $z$, at an unsampled location are modeled as outcomes of a random variable $Z$ \citep{deutsch1992geostatistical}. A random function represents a collection of spatially correlated, location-dependent random variables $Z(\mathbf{u})$ for every location $\mathbf{u}$ within the study region \citep{goovaerts1997geostatistics}.

The decision of stationarity is one to group or pool relevant data together. As no data replicates are available at location $\mathbf{u}$ to infer the random function $Z(\mathbf{u})$, geologically similar data must be pooled such that population statistics can be reliably inferred. This decision of stationarity allows the trade of unavailable replicates for data at other locations for statistical inference \citep{deutsch1992geostatistical}. Pooling too little data may lead to unreliable statistics, and too much data may lead to the masking of important geological features. Stationarity is a property related to the underlying random function model and cannot be checked or validated with data \citep{goovaerts1997geostatistics}.

Inference of the first and second-order moments (mean and covariance) of the random function is required for geostatistical estimation algorithms. When divided into a sub-region $A$, the variable of interest is considered first-order stationary if the expected value is constant within $A$. The variable is second-order stationary if the covariance depends only on the separation vector $\mathbf{h}$ within $A$. A random function $Z(\mathbf{u})$ is second-order stationary when:
\begin{align*}
    E\{Z(\mathbf{u})\}                                        & = m                                                    \\
    E\{Z(\mathbf{u}) - m^2\}                                  & = C(0) = \sigma^2                                      \\
    E\{Z(\mathbf{u}) \cdot Z(\mathbf{u} + \mathbf{h})\} - m^2 & = C(\mathbf{u}, \mathbf{u}+\mathbf{h}) = C(\mathbf{h}) \\
                                                              & \forall \ \mathbf{u}, \mathbf{u}+\mathbf{h} \ \in A
\end{align*}

Where $m$, $\sigma^2$ and $C(\mathbf{h})$ are the mean, variance and covariance, respectively, and do not depend on location. The invariance of the random function parameters to location within $A$ allows the relation $C(\mathbf{h}) = \sigma^2 - \gamma(\mathbf{h})$ which is the foundation of variogram interpretation \citep{pyrcz2014geostatistical}.

The linear model of regionalization (LMR) is used to fit experimental variograms with models that ensure positive definite covariance matrices. As not all combinations of variogram models may lead to a permissible model, the LMR constructs a random function $Z(\mathbf{u})$ to be the linear combination of $L+1$ independent, standard random functions $\{Y^{\ell}(\mathbf{u}), \ell=0,\dots,L\}$, each with its own permissible variogram function \citep{goovaerts1997geostatistics}, plus the stationary mean:
\begin{equation*}
    Z(\mathbf{u}) = \sum_{\ell=0}^{L} a^{\ell} Y^{\ell}(\mathbf{u}) + m(\mathbf{u})
\end{equation*}

Where $L$ is the number of nested structures in the model. By convention the $0^{th}$ structure is the isotropic nugget effect. The variogram model of can $Z(\mathbf{u})$ then be expressed as the sum of the of the variograms for each of the factors:
\begin{equation*}
    \gamma_z(\mathbf{h}) = \sum_{\ell=0}^{L} b^{\ell} \Gamma^{\ell}(\mathbf{h}), \ \ \ b^{\ell} = (a^{\ell})^{2}
\end{equation*}

Where $\Gamma_{\ell}(\mathbf{h})$ is the variogram of $Y_{\ell}$ and $b_{\ell}$ represents the variance contribution of each $\ell=0,\dots,L$ factors.

The LMR can be extended to the multivariate case with $k=1,\dots,K$ coregionalized variables. With the liner model of coregionalization (LMC), each coregionalized random function, $Z_{k}(\mathbf{u})$, is also the sum of the standard, uncorrelated factors:
\begin{equation*}
    Z_{k}(\mathbf{u}) = \sum_{\ell=0}^{L} a^{\ell}_{k} Y^{\ell}(\mathbf{u}) + m_{k}(\mathbf{u})
\end{equation*}

Where $a^{\ell}_{k}$ is the contribution of the $\ell^{th}$ factor the $k^{th}$ variable. The direct and cross variograms can be expressed as:
\begin{equation*}
    \gamma_{k,k^{\prime}}(\mathbf{h}) = \sum_{\ell=0}^{L} a^{\ell}_{k} a^{\ell}_{k^{\prime}} \Gamma^{\ell}(\mathbf{h}), \ \ \ k,k^{\prime} = 1,\dots,K
\end{equation*}
The LMC is commonly modeled in the cokriging paradigm for multivariate covariance inference.

\FloatBarrier
\subsection{Factorial Kriging}
\label{subsec:01factorial}

As demonstrated with the LMR notation, the regionalized variable is characterized by $m(\mathbf{u})$, the $L + 1$ $a_{\ell}$ values and the $L + 1$ variograms $\Gamma_{\ell}(\mathbf{h})$. The goal of factorial kriging is to model each nested spatial structure present in the LMR for filtering or feature extraction. The idea is that each regionalized factor has a correlation structure responsible for a different scale of continuity, and they can be estimated independently \citep{matheron1982factorial}. The factors are estimated as linear combinations of the data values \cite{deutsch2007recall}:
\begin{equation*}
    z^{\ell*}(\mathbf{u}) = a^{\ell} Y^{\ell}(\mathbf{u}) = \sum_{i=1}^{n} \lambda_{i}^{\ell} z(\mathbf{u}_{i})
\end{equation*}

% \begin{equation*}
%      m^{*}(\mathbf{u}) = \sum_{i=1}^{n} \lambda_{m,i} z(\mathbf{u}_{i})
%  \end{equation*} 

By convention ordinary kriging is used though there is no reason simple kriging cannot be used \citep{hong2007improved}. The estimation weights for the $\ell^{th}$ factor are obtained by minimizing the estimation variance which leads to the factorial kriging equations:
\begin{equation*}
    \begin{cases}
        % \sum_{i=1}^{n} \lambda_{m,i} C(\mathbf{u}_{i}, \mathbf{u}_{j}) - \mu_{m} = 0, \ \ i=1,\dots,n  \\
        % \sum_{i=1}^{n} \lambda_{m,i} = 1 \\
        \sum_{j=1}^{n} \lambda_{j}^{\ell} C(\mathbf{u}_{i}, \mathbf{u}_{j}) + \mu^{\ell} = C^{\ell}(\mathbf{u}_{i}, \mathbf{u}), \ \ \ i=1,\dots,n \\
        \sum_{j=1}^{n} \lambda_{j}^{\ell} = 0, \ \ \ \ell = 0,\dots,L
    \end{cases}
\end{equation*}

The right hand side covariances are the covariances corresponding to the particular structure $\ell$ being estimated. The sum of the estimated factors returns the original ordinary kriging estimate:
\begin{equation*}
    z^{*}(\mathbf{u}) = \sum_{\ell=0}^{L} z^{\ell*}(\mathbf{u}) + m^{*}(\mathbf{u})
\end{equation*}

Filtering properties of factorial kriging may be helpful if one is interested in removing a particular factor from estimated maps. For example, high-frequency variation from the nugget effect could be filtered by only considering the $\ell=1,\dots,L$ factors. Factorial kriging can be extended to the multivariate context by considering the LMC fitted to the direct and cross variograms where each coregionalized variable $\{Z_{k}(\mathbf{u}), k=1,\dots,K\}$ is a linear combination of the standard, independent factors $\{Y_{v}^{\ell}(\mathbf{u}), v=1,\dots,K; \ell=0,\dots,L\}$ \citep{wackernagel1988geostatistical}:
\begin{equation*}
    z^{\ell*}_{k}(\mathbf{u}) = \sum_{v=1}^{K} a_{k,v}^{\ell} Y^{\ell}_{v}(\mathbf{u}) = \sum_{v=1}^{K}\sum_{i=1}^{n} \lambda_{v,i} z_{v}(\mathbf{u}_{i})
\end{equation*}

% \begin{equation*}
%      m^{*}_{k}(\mathbf{u}) = \sum_{v=1}^{K} \sum_{i=1}^{n} \lambda_{v,i} z_{v}(\mathbf{u}_{i})
%  \end{equation*}

The cokriging equations for a particular spatial component are then:
\begin{equation*}
    \begin{cases}
        \sum_{k^{\prime}=1}^{K} \sum_{j=1}^{n} \lambda_{k^{\prime},j} C_{k,k^{\prime}}(\mathbf{u}_{i}, \mathbf{u}_{j}) + \mu_{k} = C_{k,k}^{\ell}(\mathbf{u}_{i},\mathbf{u}), \ \ \ k=1,\dots,K; i=1,\dots,n \\
        \sum_{j=1}^{n} \lambda_{k,j} = 0  \ \ \ k=1,\dots,K
    \end{cases}
\end{equation*}

Again, the sum of the estimated factors returns the original regionalized variable:
\begin{equation*}
    z^{*}_{k}(\mathbf{u}) = \sum_{\ell=0}^{L} z^{\ell*}_{k}(\mathbf{u}) + m^{*}_{k}(\mathbf{u})
\end{equation*}

Multivariate factorial kriging is a technique for characterizing the regionalized factors $Y_{\ell,k}$ from observations of $Z_{k}$. This technique is advantageous if the correlation between variables is dependent on scale and one would like to extract or filter a particular spatial structure.

Simple factorial kriging equations are equivalent to the simple kriging equations with the exception of the right hand side covariance is the covariance of the nested structure being estimated. The sum of the estimated factors plus the mean is equal to the simple kriging estimate \citep{hong2007improved}:
\begin{equation*}
    \begin{cases}
        \sum_{j=1}^{n} \lambda_{j}^{\ell} C(\mathbf{u}_{i}, \mathbf{u}_{j}) = C^{\ell}(\mathbf{u}_{i}, \mathbf{u}) \\
        i=1,\dots,n \ \ \ \ell=1,\dots,L                                                                           \\
    \end{cases}
\end{equation*}

\begin{equation*}
    z^{*}(\mathbf{u}) = \sum_{\ell=0}^{L} z^{\ell*}(\mathbf{u}) + m
\end{equation*}

Factorial simulation utilizes a conditioning by simple factorial kriging scheme. Conditioning by kriging involves adding a simulated kriging error component to a kriging estimate \citep{chiles2012geostatistics}.
\begin{equation*}
    z_{cs}(\mathbf{u}) = z^{*}(\mathbf{u}) + [z_{us}(\mathbf{u}) - z_{us}^{*}(\mathbf{u})]
\end{equation*}

Where $z_{cs}(\mathbf{u})$ is the final conditioned value, $z^{*}(\mathbf{u})$ is the simple kriging estimate, $z_{us}(\mathbf{u})$ is the unconditionally simulate value and $z_{us}^{*}(\mathbf{u})$ is the simple kriging estimate of the unconditional values at the data locations. This linear combination can be done in a factorial sense as each term can be generated for the given factor \citep{hong2007improved}:
\begin{equation*}
    z_{cs}^{\ell}(\mathbf{u}) = z^{\ell*}(\mathbf{u}) + [z_{us}^{\ell}(\mathbf{u}) - z_{us}^{\ell*}(\mathbf{u})]
\end{equation*}

Due to the exactitude of kriging, the sum of the conditionally simulated values at the data locations is equal to the sum of the simple kriging factors.

\subsection{Extreme Value Theory}
\label{subsec:01evt}

Extreme value theory is a statistical foundation for quantifying probability distributions and magnitudes of atypically high or low events. Early works by \cite{frechet1927loi}, \cite{fisher1928limiting}, \cite{mises1936distribution} and later by \cite{gnedenko1943distribution}  systematically developed the theory and formalized the asymptotic distribution of extremes for block maximums. \cite{fisher1928limiting} show that for a sequence of independent and identically distributed (iid) random variables with a common distribution function $F$, the so-called ``block maximum'', $M_n = max\{Z_1,...,Z_n\}$, can only converge to one of three distributions:

\begin{equation}
    G_{I}(z) = exp\left(-exp\left(-\left(\frac{z-\mu}{\sigma}\right)\right)\right), \ \ -\infty < z < \infty
    \label{eq:1}
\end{equation}

\begin{equation}
    G_{II}(z) =
    \begin{cases}
        0,                                                           & z \leq \mu \\
        exp\left(-\left( \frac{z-\mu}{\sigma} \right)^{-\xi}\right), & z > \mu
    \end{cases}
    \label{eq:2}
\end{equation}

\begin{equation}
    G_{III}(z) =
    \begin{cases}
        exp\left(-\left( -\left(\frac{z-\mu}{\sigma} \right)^{\xi} \right) \right), & z < \mu    \\
        1,                                                                          & z \geq \mu
    \end{cases}
    \label{eq:3}
\end{equation}

% To avoid degeneracy, the variable $M_n$ is normalized to $M_n^* = a_n^{-1}(M_n - b_n)$ for a sequence of ``normalizing'' constants $a_n > 0$ and $b_n$. \cite{fisher1928limiting} show that if these sequences of real numbers can be chosen such that $M_n^*$ has a non-degenerate limiting distribution it must be one of type I, II or III. These are the only possible limits for the distributions of $M_n^*$ regardless of the population distribution $F$ - this Extremal Types Theorem is essentially analogous to the Central Limit Theorem for extreme values \citep{coles2001introduction}. 

These collectively are termed extreme value distributions with \ref{eq:1} being Gumbel-type, \ref{eq:2} being Fr\'echet-type and \ref{eq:3} being Weibull-type. Each distribution has a location $\mu$, scale $\sigma$ and \ref{eq:2} and \ref{eq:3} have shape parameter $\xi$. The extreme value distributions are obtained as limiting distributions of $M_n$ as $n \to \infty$. The limit distributions for block maximum can be grouped into a single-family termed the generalized extreme value (GEV) distribution \citep{dehaan2007extreme}:
\begin{equation}
    G(z) = exp\left(-\left(1+\xi\left(\frac{z-\mu}{\sigma}\right)^{\frac{-1}{\xi}}\right)\right)
    \label{eq:GEV}
\end{equation}

The block maximum methodology may be inefficient as it ignores all but the maximum value in a given block \citep{davison2015statistics}. The second approach to identifying extremes is the so-called ``peak-over-threshold'' (POT) method. Consider a random variable $Z$ with distribution function $F$. The CDF of the excess over some threshold $u$, is defined by:
\begin{equation}
    \begin{aligned}
        F_u(y) & = P(Z - u \leq y | Z > u)                               \\
               & = \frac{F(u+y) - F(u)}{1-F(u)}, \ \ 0 \leq y \leq z_F-u
    \end{aligned}
    \label{eq:exceed}
\end{equation}

Where $y = z - u$ and $z_F$ is the right endpoint of $F$ \citep{gilli2006application}. \cite{pickands1975statistical} states that if $u$ is large, the conditional distribution of of $Z$ given $Z$ is much larger than $u$ is well approximated by the generalized Pareto distribution (GPD):
\begin{equation}
    G(y) =
    \begin{cases}
        1 - \left(1 + \frac{\xi y}{\sigma}\right)^{\frac{-1}{\xi }}, & \text{ if } \xi  \neq 0 \\
        1-exp\left(\frac{-y}{\sigma} \right),                        & \text{ if } \xi  = 1
    \end{cases}
    \label{eq:GPD}
\end{equation}

The conditional distribution of the exceedances (Equation~\ref{eq:exceed}) can be modeled asymptotically with the GPD by estimating the scale ($\sigma$) and shape ($\xi$) parameters.

\FloatBarrier
\subsection{Spatial Extreme Value Theory}
\label{subsec:01spevt}

The assumption of independent and identically distributed (iid) observations underlies classical EVT. In many real-world applications, one must account for correlation in space or time and the multivariate nature of regionalized variables. Spatial extreme value theory represents an intersection between classical EVT and geostatistics \citep{neves2015geostatistical}. The primary difference between spatial EVT and geostatistics is that in the geostatistical framework, there are no observed replicates at $Z(\mathbf{u})$. Fitting of a GEV or GPD in the classical EVT sense requires multiple realizations of $Z_i(\mathbf{u})$ for parameter inference.

Spatial EVT builds on the concepts of max-stable distributions extending to the max-stable process. A max-stable process is the infinite dimension generalization of the max-stable distribution where all lower-order marginal distributions are GEV distributions \citep{schlather2003dependence}. If there exists normalizing constants $a_n(\mathbf{u}) > 0$ and $b_n(\mathbf{u})$ such that $a_n^{-1}(\mathbf{u})\{\max_{i=1,...,\infty} Z_i(\mathbf{u}) - b_n(\mathbf{u})\} = Y(\mathbf{u})$ then $Y$ is a max-stable process \citep{dehaan2007extreme}. The max-stable process is applicable to maximums as stable Gaussian processes with finite variance are applicable to the average \citep{chiles2012geostatistics}. All marginal distributions of a max-stable process are GEV distributions defined by Equation~\ref{eq:GEV}.

% Unlike a Gaussian process which is fully defined by its correlogram $\rho(\mathbf{h})$, there is no unique model for max-stable processes \citep{chiles2012geostatistics}. Many different models are found in the literature. First introduced by \cite{smith1990maxstable} and later modified by \cite{schlather2002models}, Gaussian storm and extremal Gaussian processes are commonly used for modeling spatial extremes. The Brown-Resnick processes \citep{brown1977extreme,kabluchko2009stationary} relaxes the assumption of second-order stationarity and permits the use of the variogram, which has shown to be practical in practice \citep{gaume2013mapping}. The extremal-$t$ model \citep{opitz2013extremal} is another popular max-stable model in the literature. A consequence of the max-stable processes is asymptotic dependence in the tails \citep{davison2013geostatistics}. 

\FloatBarrier
\subsection{Outlier Detection}
\label{subsec:01outlier}

Outlier or ``novelty'' detection is applicable in virtually all statistical modeling. An outlier is an observation sufficiently dissimilar to other observations \citep{barnett1984outliers}. Measures of ``outlierness'' are typically based on (1) statistics of observations for the rest of the distribution (parametric or non-parametric); (2) distances (euclidean or non-euclidean) between observations with outliers being ``far'' from neighbours and (3) probability density-based measures where outliers have low densities \citep{hodge2004survey}.

Outliers are significant when considering data from potentially heavy-tailed distributions; there are likely few examples of a critical class. Extreme values are different from traditional outliers in the sense that all extreme values are outliers, but the reverse is not always true \citep{aggarwal2015outlier}. Most EVT outlier approaches model the ``inliers'' and test potential outliers against this model. Outlier detection with explicit use of extreme value theory have been applied to outlier detection in biomedical data processing \citep{roberts2000extreme}, signal processing processing \citep{hazan2012extreme}, image noise removal \citep{roberts1999novelty}, monitoring of critical equipment like jet-engines \citep{clifton2009comparison}, cybersecurity and stock price analysis \citep{siffer2017anomaly}.

\FloatBarrier
\subsection{Outlier Detection in Mining}
\label{subsec:01miningoutlier}

Identifying and managing extreme values or possible outliers prior to resource estimation is a key component of resource estimation, particularly for heavy-tailed mineral deposits. \cite{leuangthong2015dealing} break the process of outlier management into three categories: (1) choosing appropriate domains; (2) grade capping; and (3) limiting the influence of outliers through the estimation process. A portion of this research focuses on the second category. Grade capping or ``top-cutting'' is a common practice in the mining industry. Grades above a given threshold are reset to that threshold. The general idea is that uncapped grades may lead to unrealistic local estimates adjacent to high-grade composites \citep{nowak2013suggestions}. The ``smearing'' may be significant if sparse data are estimated with kriging. Due to the normal score transform, simulation is somewhat more robust to the presence of outliers though some cases may require capping.

There are numerous practical methods for explicitly reducing the influence of high-grade samples. Commonly observed methods in mining technical reports \citep{harding2021outlier} include decile analysis \citep{parrish1997geologist}, cumulative probability plots, cutting curves \citep{roscoe1996cutting}, coefficient of variation thresholds, production reconciliation with capped and uncapped estimates, arbitrary percentiles or quantiles, metal-at-risk \citep{parker2006}, indicator correlations, multiple indicator kriging or no capping at all. An additional point of contention is the support of the data to be capped. There is no clear consensus on whether to cap assay or composited data.

\FloatBarrier
\subsection{Inverse Problems}
\label{subsec:03inverse}



Inverse problems encompass a broad class of problems where the objective is to infer the underlying causes or parameters of a system from observed data or measurable outputs. The prediction of a response is a forward problem, while the use of a response, or observed measurements to infer the properties of a model is an inverse problem \citep{tarantola2005inverse}. Inverse problems arise in various scientific disciplines, including physics, engineering, geosciences, medical imaging, and more. Geospatial inverse problems are common in both the fields of geophysics \citep{linde2015geological} and hydrogeology \citep{zhou2014inverse} where the underlying geologic model is unknown, however a set of measured responses, such as density or seismic properties, are known. The inverse problem involves inferring interpretable geologic properties of the unknown model, such as lithology or porosity, that satisfy the observed measurements. The process of solving inverse problems involves constructing a mathematical forward model that describes the relationship between the unknown parameters and the observed data; then using this model to infer the unknown parameters. A challenge of inverse problems is ill-posedness, or the lack of unique solution \citep{tarantola2005inverse}. It is possible that there are multiple (or infinite) solutions that are valid given the observed data. As an exact solution is rarely possible in natural, non-linear systems, one looks for possible solutions that are close to actual observations \citep{bardossy2016random}. For this reason many inverse problems are framed as an optimization problem, minimizing an objective function relevant to the problem at hand \citep{giraud2019integration,nava-flores2023high,athens2022stochastic}. The objective function is minimized iteratively. A forward modeling operator predicts an outcome for the current state of the model parameters and the objective function evaluates the loss between this prediction and the observed measurements. The optimization algorithm updates the parameter vector until it achieves an acceptable match between the model output and observed measurements.

Due to the inherent complexity and potential non-uniqueness of inverse problems, careful consideration of data quality, noise, and computational stability is essential to obtain meaningful and reliable solutions.

\begin{enumerate}
    \item something about the forward problem
    \item prior knowledge is the specification of the pool
    \item prior knowledge is the conceptual geologic model
    \item no data reproduction at this point, only reproduction of pertinent statistics
    \item constraints on wt magnitude, nscore keeps solution reasonable
\end{enumerate}

\FloatBarrier
\section{Notation and Concepts}
\label{sec:01notation}

\FloatBarrier
\section{Thesis Outline}
\label{sec:01outline}