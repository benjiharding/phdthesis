%!TEX root = ../Thesis_example_compile_this.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Network Model of Regionalization Framework}
\label{ch:03framework}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter introduces the \gls{NMR} paradigm and the framework for generating non-Gaussian spatial fields. The main idea of the \gls{NMR} is that capturing non-Gaussian spatial features requires reproducing high-order statistics. Reproduction of high-order statistics should also reproduce two-point statistics such as the variogram. Multi-point connectivity measures can be easily calculated from drillholes by considering them as sequences. The \gls{NMR} is ultimately a function that maps an unknown latent space to an interpretable observed space. The mapping of the latent space generates a non-Gaussian spatial distribution with the correct high-order statistics. Inference of this mapping function is an inverse problem as we only observe the system's output. The chapter begins by framing the problem setting and addressing known issues with the multivariate Gaussian assumption. Next, the relationship between connectivity and non-Gaussianity is discussed. Multi-point measures of spatial connectivity, like distributions of runs, are a core component of the \gls{NMR} framework. Connectivity measures are critical in the context of extreme values; the spatial arrangement of these values is likely significant concerning a transfer function. The connectivity of extremes likely drives the project economics in many mining scenarios. Finally, an overview of the \gls{NMR} methodology is presented with a \gls{2D} synthetic example highlighting improved resources relative to a traditional \gls{SGS} model.


\FloatBarrier
\section{Problem Overview}
\label{sec:03overview}

Geostatistical problems are often high dimensional, considering multiple variables at millions of locations. The appeal of the Gaussian distribution is its mathematical tractability in  any dimensions, where a mean vector and variance-covariance matrix fully parameterizes it; thus, it is pervasive in geostatistics. Many algorithms take advantage of the fact that under the multivariate Gaussian assumption, all conditional distributions are Gaussian and calculated by linear combinations of the conditioning data. Multivariate geostatistical problems necessitate a parametric distribution as there are typically only hundreds to thousands of data available. The curse of dimensionality \citep{bellman1961adaptive} precludes the use of non-parametric distributions.

Connectivity of extreme values is commonly discussed as a shortcoming of the Gaussian \gls{RF} model \citep{journel1993entropy,journel1989nongaussian,yan2020multivariate,guthke2017link,kerrou2008issues}. The maximum entropy characteristic of the Gaussian \gls{RF} model leads to maximum disorder for a given covariance structure; it does not allow for spatial correlation or connectivity of extreme quantile indicators \citep{kerrou2008issues}. Figure \ref{fig:connectivity_example} emphasizes these disconnected extreme values. The left panel is a reference image showing the volume of shale for interbedded sands and shale of the McMurray Formation. The middle panel shows a Gaussian realization generated with the normal score variogram calculated from the reference. The right panel shows the connection probability for subsequent steps in the vertical direction. The Gaussian realization is more disorganized and shows less connectivity than the true image. Furthermore, the nature of the bivariate Gaussian relationship leads to symmetric destructuring of the indicator variogram about the median. In practice, natural non-Gaussian distributions show asymmetric destructuring of indicator variograms \citep{vincent2021multipleindicator} and correlation need not approach zero as $z_{k}$ approaches an extreme quantile \citep{journel1989nongaussian}.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/connectivity_example.png}
    \caption{Reference image showing the volume of shale for interbedded sands and shale of the McMurray Formation (left), a Gaussian realization generated with \gls{SGS} (middle), and the corresponding $n$-point connectivity functions (right). The reference image is part of a core photograph.}
    \label{fig:connectivity_example}
\end{figure}

High-order connectivity metrics can characterize non-Gaussianity. This concept is explored in detail in Section \ref{sec:03connect}. These multi-point measures include the frequencies of runs and the $n$-point connectivity function. As properties of the Gaussian \gls{RF} model lead to maximum disorder, one can use a connectivity measure to assess non-Gaussianity from sequences. The idea is that a Gaussian \gls{RV} would show different connectivity than a non-Gaussian \gls{RV}. The multi-point spatial arrangement would differ depending on the underlying \gls{RV}. \Glspl{RV} with more connectivity, will have fewer but longer runs. This connectivity concept applies to sequences with structured extreme values not suited by a Gaussian distribution. There is some degree of connectivity of extremes in mineral systems; the underlying spatial phenomena are not entirely disorganized.

The challenges associated with \gls{MIK} and traditional outlier management motivate the need for spatial models that can characterize the non-Gaussian continuity of extreme values. Though the goal is to achieve non-Gaussian features, the \gls{NMR} framework remains based on Gaussian distributions. The idea is to extend the concept of the \gls{LMR} and map a collection of regionalized, independent, standard normal factors $\{ Y_{m}, \ m = 0, \dots, M\}$ through a weighted network consisting of non-linear activations. The \gls{NMR} framework poses aspects of the generation of stochastic realizations as an optimization problem rather than the traditional random function approach. The parameters that map the factors are unknown, though we have observed measurements at the data locations. This model-based inverse problem is approached through stochastic optimization. This approach allows the incorporation of multi-point geologic information from various sources into the final model while honouring the statistics of the observed data. A network-based mixing architecture and non-linearities allow the final models to capture richer and higher-order spatial features; this is particularly useful when constructing models in the presence of extreme values.

The following components summarize the overall structure of the \gls{NMR} framework:
\begin{enumerate}[noitemsep]
    \item Objective targets: specifying the two- and multi-point statistics for the final model to reproduce.
    \item Latent factor design: selection of the base pool of Gaussian distributions for mixing.
    \item Parameter inference: determining the parameters of the mixing function that result in a distribution with the correct spatial features.
    \item Latent factor imputation: imputation of the latent factors such that mixing reproduces the observed data values.
    \item Latent factor simulation and mapping: conditional simulation of the imputed latent factors from (4) and mapping to observed space with parameters from (3).
\end{enumerate}

The first component is specifying the goals of the model. These goals, or objectives, are quantified by two-point spatial statistics and higher-order statistics. The second component involves choosing the covariance structure of each latent factor to mix. These choices depend strongly on the modeling goals. The third component involves inferring the parameters of the mixing function, which results in the model meeting the objective targets. The fourth component involves generating synthetic realizations of the factors. The fifth component is conditionally simulating the factors on a modeling grid and mapping them to observed space. This mapping results in gridded realizations that reproduce the observed data and the objective targets away from the data. Figure \ref{fig:nmr_framework} shows the complete \gls{NMR} framework as a flow chart. 

\begin{figure}[htb!]
    \centering
    \begin{tikzpicture}[node distance=1.5cm, align=center]

        % Specification of styles
        \tikzstyle{accept1Block} = [rectangle, rounded corners, minimum width=3cm, minimum height=1.0cm,text centered, draw=black]

        % Specification of nodes (position, etc.)
        \node (start) [base] {Define Objectives};
        \node (poolBlock) [base, below of=start, yshift=-0.50cm] {Define Latent Pool};
        \node (inferBlock) [base, below of=poolBlock, yshift=-0.50cm] {Infer Parameters};
        \node (accept1Block) [accept1Block, below of=inferBlock, yshift=-0.50cm] {Acceptable Objective Reproduction?};
        \node (latentBlock) [base, below of=accept1Block, yshift=-0.50cm] {Latent Imputation};
        \node (accept2Block) [accept1Block, below of=latentBlock, yshift=-0.50cm] {Acceptable Latent Reproduction?};
        \node (simBlock) [base, below of=accept2Block, yshift=-0.50cm] {Gridded Latent Simulation};
        \node (mapBlock) [base, below of=simBlock, yshift=-0.75cm] {Map Latent to Observed Space};
        \node (accept3Block) [accept1Block, below of=mapBlock, yshift=-0.50cm] {Acceptable Gridded Reproduction?};
        \node (end) [base, below of=accept3Block, yshift=-0.50cm] {Final Gridded Realizations};

        % Specification of lines between nodes specified above
        \draw [->] (start) -- (poolBlock);
        \draw [->] (poolBlock) -- (inferBlock);
        \draw [->] (inferBlock) -- (accept1Block);
        \draw [->] (accept1Block) -- node[anchor=east] {yes} (latentBlock);
        \draw [->] (accept1Block.west) -- ++(-0.5,0) -- ++(0,3.3) -- ++(0,0.7) -- node[xshift=-1.5cm, yshift=-2cm] {no} (poolBlock.west);
        \draw [->] (accept1Block.west) -- ++(-0.5,0) -- ++(0,1.3) -- ++(0,0.7) -- (inferBlock.west);
        \draw [->] (latentBlock) -- (accept2Block);
        \draw [->] (accept2Block) -- node[anchor=east] {yes} (simBlock);
        \draw [->] (accept2Block.west) -- ++(-0.5,0) -- ++(0,1.3) -- ++(0,0.7) -- node[xshift=-1.5cm, yshift=-1cm] {no} (latentBlock.west);
        \draw [->] (simBlock) -- (mapBlock);
        \draw [->] (mapBlock) -- (accept3Block);
        \draw [->] (accept3Block) -- node[anchor=east] {yes} (end);
        \draw [->] (accept3Block.west) -- ++(-0.5,0) -- ++(0,3.55) -- ++(0,0.7) -- node[xshift=-1.5cm, yshift=-2cm] {no} (simBlock.west);
    \end{tikzpicture}
    \caption{High level flow chart illustrating the key components of the \gls{NMR} framework. There are multiple junctions where previous steps can be revisited and refined. }
    \label{fig:nmr_framework}
\end{figure}







The remaining sections of this chapter discuss the calculation of high-order statistics and their use for measures of non-Gaussianity, details regarding the components of the \gls{NMR} framework, and finally, a small synthetic example of the complete \gls{NMR} workflow with highly structured extreme values.


\FloatBarrier
\section{Connectivity and Non‐Gaussianity}
\label{sec:03connect}

A sequence is a collection of elements where the order of the elements matters. The arrangement or order of the elements can be used to characterize the connectivity within the sequence. Grades assayed on intervals down a drillhole represent a one-dimensional sequence of real numbers in a geostatistical context. The connectivity of high and low values is often of practical importance, mainly when the transfer function is sensitive to extreme values. A one-dimensional sequence provides access to data-driven multiple-point configurations or patterns that would be difficult to infer in two- or three-dimensions. Connectivity is a different way of measuring correlation within a sequence. Each drillhole can be considered an exhaustive, one-dimensional training image from which $n\text{-point}$ statistics can be inferred.

A natural extension of this concept is the analysis of runs of binary sequences from linear strings of data \citep{ortiz2003characterization}. A binary sequence is either 0 or 1 and computed through the indicator transform of a continuous \gls{RV}. For a given threshold $z_{k}, \ k=1,\dots,K$:
\begin{equation}
    I(\mathbf{u}_{i};z_{k}) =
    \begin{cases}
        1, & \text{ if }z(\mathbf{u}_{i}) \leq z_{k} \\
        0, & \text{ otherwise }                      \\
    \end{cases}
    \label{eq:indicator}
\end{equation}

It is common to consider multiple thresholds resulting in multiple binary sequences. A \textit{run} of length $L$ is defined as $L$ identical values bound on either end by an opposite value. Runs of consecutive values above or below the threshold can be assessed. The theory of the distributions of runs for random uniform sequences is well documented by \cite{fu2003distribution}; the moments of the distribution of runs have analytical expressions, and they show that the limit distributions are normal. Though useful in many applications like cryptography and random number generation \citep{rukhin2010statistical}, the assumption of independence between elements in the sequence is limiting in the spatially correlated scenario.

\cite{ortiz2003characterization} shows that the analytical derivation of multi-point events is only possible when the multivariate spatial law is known. Practically, this is either the random or multivariate Gaussian case. A run of length $L$ above a threshold $z_{k}$ consists of $L+2$ elements where the first and last elements are below the threshold. In the general case, the probability of a run of length $L$ is defined as:
\begin{align}
    \begin{split}
        P\{\text{run L}\} & =                                                                                                                                                                \\
                          & P\{Z(\mathbf{u}) \leq z_{k}| Z(\mathbf{u + h}) > z_{k},\dots, Z(\mathbf{u} + L \cdot \mathbf{h}) > z_{k}, Z(\mathbf{u} + (L+1)) \leq z_{k}\} \cdot ,\dots, \cdot \\
                          & P\{Z(\mathbf{u} + L \cdot \mathbf{h}) > z_{k} | Z(\mathbf{u} + (L+1)) \leq z_{k}\} \cdot                                                                         \\
                          & P\{Z(\mathbf{u} + (L+1)) \leq z_{k}\}
    \end{split}
\end{align}

The separation vector between elements is $\mathbf{h}$. This definition amounts to a recursive application of Bayes' Law to determine the joint probability of the multiple-point event. In the multivariate Gaussian case, the conditional probabilities are calculated using simple indicator kriging \citep{journel1989nongaussian}. With correlated sequences, the range of correlation influences the frequencies of runs. As correlation increases, there are fewer short and more long runs relative to random \citep{ortiz2003characterization}.

Correlated Gaussian sequences with an arbitrary covariance structure can be easily generated using any stochastic simulation algorithm and the indicator formalism of Equation~\ref{eq:indicator}. The total number of runs and frequencies of run lengths can be determined experimentally for the multivariate Gaussian scenario. If the covariance structure of the Gaussian realizations matches that of the true one-dimensional drillhole sequence, run-based statistics can measure non-Gaussianity. Section \ref{subsec:03ngmeasures} presents this idea further. If the number or frequency of runs in the data deviates significantly from the Gaussian case, one may investigate the sequence as non-Gaussian. These measures can provide insight into domain sub-regions that exhibit non-Gaussian behaviour and warrant further investigation. Run-based connectivity measures in the following sections provide access to high-order statistics to characterize non-Gaussianity.

\subsection{Distribution of Runs}
\label{subsec:03runs}

High-order statistics characterize the spatial relationship between multiple points. In contrast to two-point statistics, such as the variogram, multiple-point statistics can reproduce curvilinear features and more complex ordering \citep{guardiano1993multivariate}. Runs are one-dimensional patterns or a type of multi-point statistic calculated from data sequences like drillholes \citep{boisvert2007multiplepoint}.

The indicator formalism (Equation \ref{eq:indicator}) applied to linear, one-dimensional drill strings (Figure~\ref{fig:gauss_indicators}) generates binary sequences. This transform characterizes each element in the sequence as a binary event relative to thresholds $z_{k}, \ k=1,\dots,K$. It is implicit that the elements in the sequence are equidistant; that is, the drillhole is composited. Multiple-point configurations, such as runs, can then be extracted from the sequences. In practice, one may have to adapt tolerances to infer statistics from approximately linear or equidistant sequence elements \citep{ortiz2003characterization}.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/gauss_indicators.png}
    \caption{Indicator transform of a Gaussian \gls{RV} using the 0.1, 0.5, and 0.9 quantiles as thresholds. }
    \label{fig:gauss_indicators}
\end{figure}

Calculating runs considers a cumulative or ``overlapping'' approach. That is, one run of 3 is also two runs of 2 and three runs of 1; Figure~\ref{fig:cumulative_runs} shows this nesting. This generalizes to a run of length $L$ being $i$ runs of length $L-i+1, \ i=1,\dots,L$. This cumulative approach accounts for the dependence between overlapping runs within the sequence \citep{fu2003distribution}. Considering cumulative runs generates a histogram of run lengths that decreases as run lengths increase. This consideration controls long runs as a long run contains elements for all shorter runs \citep{ortiz2003characterization}.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.4, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/cumulative_runs.png}
    \caption{Example of lower-order runs for a single run of 3. One run of 3 is also two runs of 2 and three runs of 1.}
    \label{fig:cumulative_runs}
\end{figure}

It is interesting to note that the indicator transform results in nested sequences. Sequences that are above the current threshold are also above any lower threshold. Figure~\ref{fig:gauss_indicators} illustrates this concept where all sequences above the 0.9 quantile (zeros) are nested within a sequence above the 0.5 quantile, which is nested within a sequence above the 0.1 quantile. \cite{ortiz2003characterization} uses this property of sequences to simulate hierarchically as a run at one threshold can be used to condition the next threshold.

\subsection{\textit{N}-point Connectivity Function}
\label{subsec:03npoint}

A connectivity function quantifies the connectedness of a sequence \citep{renard2011conditioning}. Two-point connectivity is characterized by the expected value of the product of indicators $I(\mathbf{u}; z_{k})$ and $I(\mathbf{u}+\mathbf{h}; z_{k})$. The $n\text{-point}$ connectivity function, proposed by \cite{journel1989nongaussian}, in a given direction can be generalized as the expected value of the product of $n$ indicators where the lag distance $\mathbf{h}$ is the distance between sequence elements:
\begin{equation}
    \psi_{z_{k}}(n;\mathbf{h}) = E\left\{\prod_{j=1}^{n} I(\mathbf{u} + (j-1)\mathbf{h}; z_{k})\right\}
    \label{eq:npt}
\end{equation}

The $n\text{-point}$ connectivity function describes the probability of $n$ successive elements in the sequence being jointly \emph{below} the threshold $z_{k}$. To consider the probability of being jointly \emph{above} the threshold $z_{k}$, Equation \ref{eq:npt} becomes:
\begin{equation}
    \psi_{z_{k}}(n;\mathbf{h}) = E\left\{\prod_{j=1}^{n} 1 - I(\mathbf{u} + (j-1)\mathbf{h}; z_{k})\right\}
    \label{eq:npt2}
\end{equation}

\cite{journel1989nongaussian} show that the $n\text{-point}$ connectivity function for a \gls{RV} with connected extreme values simulated with a Gaussian \gls{RF} model is much less than the true connectivity. Figure~\ref{fig:npt_connectivity} shows an example of an $n\text{-point}$ connectivity function for a non-Gaussian one-dimensional string data (red) and a Gaussian realization of the same string (gray). The Gaussian realizations are generated with LU matrix simulation \citep{davis1987production} and have the same covariance structure as the non-Gaussian string. The $n$-point connectivity function shows that the probability of connected steps within the Gaussian realizations is significantly less than the non-Gaussian data for the first 20 steps.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/npt_connectivity.png}
    \caption{$n$-Point connectivity function for a string of non-Gaussian data (red) and the expected value of Gaussian realizations (gray) with the same covariance structure.}
    \label{fig:npt_connectivity}
\end{figure}

Notably, the $n\text{-point}$ connectivity function considering runs \emph{above} the threshold (Equation \ref{eq:npt2}) is analogous to cumulative run-length frequencies of runs above the threshold scaled to the fraction of total elements in the sequence. Considering the $n\text{-point}$ connectivity function above a quantile is the same as considering the cumulative run-length frequency above that quantile.

\subsection{Measures of Non‐Gaussianity}
\label{subsec:03ngmeasures}

Four measures of non-Gaussianity are proposed. Three based on the concepts of runs and connectivity within drill string sequences, and one based on the volume-variance relation:
\begin{enumerate}[noitemsep]
    \item Total Runs: the number of unique runs above or below a threshold.
    \item Run Length Frequencies: the frequency of cumulative run lengths above or below a threshold.
    \item $N$-Point connectivity: the number of connected steps above or below a threshold.
    \item Change of Support: the relationship between continuous variance and scale.
\end{enumerate}

The first measure calculates the total cumulative runs within the binary sequence. As connectivity within the sequence increases, fewer but longer \emph{total} runs exist. This connectivity translates to more \emph{cumulative} runs as each longer run contains $\{L-i+1, \ i=1,\dots,L\}$ lower-order runs. A non-Gaussian sequence with connectivity of extremes is expected to exhibit a greater number of cumulative runs than a Gaussian sequence. The second measure calculates the cumulative run-length frequencies within the binary sequence. Increased connectivity within the sequence leads to fewer but longer run lengths. Similar to the first measure of total runs, a non-Gaussian sequence with connectivity of extremes is expected to exhibit a greater frequency of longer run lengths. The nature of cumulative run lengths leads to a histogram of lengths that decreases as run length increases, facilitating a more straightforward comparison of distributions. The third measure is the binary sequence's $n\text{-point}$ connectivity. The $n\text{-point}$ connectivity function quantifies the probability of $n$ successive elements in the sequence being below (or optionally above) the indicator thresholds. Only elements below the threshold contribute to the probability in Equation~\ref{eq:npt}. As connectivity within the sequence increases, the probability of successive elements being jointly below the given threshold increases. A highly structured non-Gaussian sequence is expected to have a greater probability of $n$ connected steps compared to a maximum entropy Gaussian sequence as $n$ increases. Figure~\ref{fig:npt_connectivity} highlights this characteristic.

The fourth measure is not sequence-based but calculates the change of support for the original continuous variable. This measure is quantified by averaging $n$ consecutive elements within the sequence and calculating the change in variance relative to the original. As volume increases, the variance of the elements within the sequence decreases. The idea is that if the sequence has structured or connected extreme values, the variance of the sequence should be less sensitive to scale. A non-Gaussian sequence with connected extreme values is expected to show a less drastic reduction in variance as scale increases compared to a maximum entropy Gaussian sequence.

The covariance structure of each sequence is calculated from the exhaustive drill string to measure non-Gaussianity. This covariance is used to generate multivariate Gaussian realizations with the exact covariance of the original sequence. The Gaussian realizations are back-transformed to original units, and then indicator transformed with Equation~\ref{eq:indicator}. Each metric is calculated for each one-dimensional data string and compared to the distribution of metrics observed from the Gaussian realizations. The deviation between the original sequence and the Gaussian distribution measures non-Gaussianity. The general workflow for calculating the proposed measures of non-Gaussianity on a drillhole-by-drillhole basis is as follows:
\begin{enumerate}[noitemsep]
    \item Indicator transform the grades of all drillholes for quantiles of interest.
    \item Calculate cumulative runs and run-length frequencies for all thresholds for all drillholes.
    \item Calculate the $n$-point connectivity function for all thresholds for all drillholes.
    \item Composite the continuous variable by a number of length factors to calculate the relationship between variance and scale.
    \item Normal score transform the grades of all drillholes.
    \item Calculate the autocovariance matrix for each drillhole and simulate $\ell=1,\dots,L$ unconditional Gaussian realizations of each drillhole.
    \item Back transform Gaussian realizations to original units.
    \item Repeat steps 1-4 for each realization of each drillhole.
    \item Calculate the score for each measure as $y = \frac{z_{dh}-\mu_{r}}{\sigma_{r}}$ where $z_{dh}$ is the measure from the original drillhole and $\mu_{r}$ and $\sigma_{r}$ are the mean and standard deviation of the distribution of measures from the realizations, respectively.
    \item The final $y$ score is taken as $\lvert y \rvert$ such that only the magnitude, not the sign, is considered.
\end{enumerate}

The $y$ score measures how many standard deviations away from the Gaussian distribution the original drillhole is. Figure~\ref{fig:score} illustrates $y$-score calculation. This score indicates the appropriateness of a Gaussian \gls{RF} model, as the Gaussian realizations have the exact covariance structure of the original drillhole. Any drillhole with a $y$ value greater than 2.5 is considered highly non-Gaussian.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.65, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/score.png}
    \caption{Schematic illustrating the calculation of the non-Gaussian Score $y$ where $z_{dh}$ is the metric from the drillhole, and $\mu_{r}$ and $\sigma_{r}$ are the mean and standard deviation of the Gaussian distribution.}
    \label{fig:score}
\end{figure}

Consider a \gls{2D} synthetic example generated from an image of a meandering river system. The \gls{RGB} colour channels are averaged to generate a greyscale image, which is then normalized $\in [0,1]$. The main river channels show structured high values relative to the background flood plane and abandoned channels (Figure \ref{fig:non_gauss4_grid}). These features are a natural example of non-Gaussian characteristics in geospatial data. The image is ``drilled'', resulting in 10 drill strings or sequences used to calculate non-Gaussianity measures.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/non_gauss4_grid.png}
    \caption{Image of a meandering river system sampled in ten locations to generate sequences of pixel data. DHID = drillhole ID, px = pixel. }
    \label{fig:non_gauss4_grid}
\end{figure}

Table \ref{tab:ng_metrics} shows the expected non-Gaussian scores for each sequence across 100 realizations for the 0.1 and 0.9 quantile indicator transforms. The structured regions are predominantly high-grade, so the 0.1 quantile indicators do not show significant non-Gaussianity. Drillholes 5 and 7 show strong non-Gaussianity for all connectivity metrics, while holes 8 and 9 are moderately non-Gaussian. These drillholes intersect structured high-grade river channels in the eastern portion of Figure \ref{fig:non_gauss4_grid} and exhibit connectivity features that cannot be reproduced by a multivariate Gaussian \gls{RF}. Abrupt transitions between high- and low-grade regions are difficult to capture with a maximum entropy \gls{RF} that leads to increased connectivity of median values and disconnected extremes.

\begin{table}[!htb]
    \centering
    \caption{Non-Gaussian metrics calculated for ten drillholes considering the 0.1 and 0.9 quantile indicator transforms. $y$-scores $\geq 2.5$ are considered strongly non-Gaussian. DHID corresponds to Figure \ref{fig:non_gauss4_grid}.}
    \resizebox{1\width}{!}{\input{0-Tables/ng_metrics.tex}}
    \label{tab:ng_metrics}
\end{table}

There is likely a spectrum of non-Gaussianity, and the practitioner must decide on appropriate thresholds. The proposed measures indicate that some regions within the domain have non-Gaussian features. These measures can identify sub-regions that warrant further investigation. The \gls{NMR} is designed to capture these non-Gaussian connectivity measures. The goal of \gls{NMR} is to develop a framework for incorporating two- and $n$-point statistics where two-point statistics come from the continuous variogram and indicator variograms, and the $n$-point statistics come from drillhole sequences.


\FloatBarrier
\section{Network Methodology}
\label{sec:method}

Sections \ref{subsec:03runs} and \ref{subsec:03npoint} present methodology for accessing higher-order statistics from drillhole sequences. Section \ref{subsec:03ngmeasures} shows that these statistics can effectively characterize non-Gaussianity. This section presents the overall \gls{NMR} framework and the generation of non-Gaussian spatial fields by integrating these statistics. Achieving higher-order connectivity is a key component of the \gls{NMR} framework. Two-point, variogram-based statistics alone cannot capture the complex multi-point relationships we seek in a non-Gaussian \gls{RF}. The indicator transform of drill strings provides access to binary sequences that permit the calculation of multi-point connectivity statistics.

% Reproduction of high-order statistics, in theory, should reproduce all lower-order statistics. However, the \gls{NMR} explicitly enforces two- and multi-point statistics.

\subsection{Notation}
\label{subsec:03notation}

This section shows an overview of the \gls{NMR} mathematical notation and definitions, while Chapters \ref{ch:04implement} and \ref{ch:05impute} present complete details. Consider a continuous \gls{RF}:
\begin{equation}
    \{Z(\mathbf{u}), \ \forall \mathbf{u} \in \mathcal{D}\}
    \label{eq:zu}
\end{equation}

\lowercase{Where} $\mathcal{D}$ is a domain of interest. The location vector $\mathbf{u}$ could be data or grid node locations. Next, consider a set of $M+1$ latent variables, each characterized by a Gaussian \gls{RF}:
\begin{equation}
    \{\mathbf{Y}(\mathbf{u}) = (Y_{0}(\mathbf{u}), \dots, Y_{M}(\mathbf{u})), \ \forall \mathbf{u} \in \mathcal{D}\}
    \label{eq:gpool}
\end{equation}

Finally, consider a forward mapping function $\mathcal{F}_{\theta}$ that defines the mapping from the real-valued latent space to the real-valued observed space:
\begin{equation}
    \{\mathcal{F}_{\theta}: \mathbb{R}^{M} \mapsto \mathbb{R} \}
    \label{eq:fmap}
\end{equation}

\lowercase{Where} $\theta$ is a parameter vector that characterizes $\mathcal{F}$, and $\mathcal{F}_{\theta}$ is such that the mapping of the latent space to the observed space reproduces the observed data values:
\begin{equation}
    \{\mathcal{F}_{\theta}(\mathbf{Y}(\mathbf{u})) = Z(\mathbf{u}), \ \forall \mathbf{u} \in \mathcal{D}\}
    \label{eq:ftheta}
\end{equation}

The \gls{NMR} approximates the forward mapping function $\mathcal{F}_{\theta}$. Chapter \ref{ch:04implement} discusses inference of the parameter vector, $\theta$, and Chapter \ref{ch:05impute} presents imputation of the Gaussian \glspl{RF} such that the equality in Equation \ref{eq:ftheta} holds.

\subsection{The NMR Inverse Problem}
\label{subsec:03nmrinverse}

The \gls{NMR} approximates the forward mapping function from latent to observed space and can be considered a model-based inversion problem \citep{sen2013global}. The parameters of this function (or model) are unknown and must be inferred from the observed measurements or data. The goal is to find the unknown parameters, $\theta$, so the model output has the desired spatial characteristics. In this context, the observed measurements are drillhole data or empirical statistics calculated from these data. Synthetic data are generated by mapping latent Gaussian variables though $\mathcal{F}_{\theta}$ for a given state of $\theta$. If the match between the spatial characteristics of the synthetic data and those of the observed data is acceptable, then the parameters are retained. Otherwise, $\theta$ is perturbed until the match is acceptable. An objective function quantifies the mismatch between the model output and observed data. Figure \ref{fig:flowchart} shows a high-level flowchart of this optimization process. The iterative parameter search, or minimization of the objective function, is an optimization problem approached with \gls{DE}, a directed Monte Carlo search method; Chapter \ref{ch:04implement} presents details on the specific formulation of this problem.

% \begin{figure}[htb!]
%     \centering
%     \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/flowchart.png}
%     \caption{Flowchart showing an overview of the optimization workflow for determining the parameter vector $\theta$.}
%     \label{fig:flowchart}
% \end{figure}


\begin{figure}[htb!]
    \centering
    \begin{tikzpicture}[node distance=1.5cm, align=center]
        \tikzstyle{mapBlock} = [rectangle, rounded corners, minimum width=3cm, minimum      height=1cm,text centered, draw=black]
        \tikzstyle{incBlock} = [rectangle, rounded corners, minimum width=2.0cm, minimum height=1cm,text centered, draw=black]
        % Specification of nodes (position, etc.)
        \node (start)       [base, ] {$M + 1$ Prior Latent Models};
        \node (updateBlock) [base, right of=start, xshift=1.5cm] {Update $\theta_{i}$};
        \node (mapBlock)    [mapBlock, right of=updateBlock,xshift=1.5cm, yshift=1.25cm] {$\mathcal{F}_{\theta_{i}}(\mathbf{Y}(\mathbf{u}))=Z(\mathbf{u})$};
        \node (incBlock)    [incBlock, right of=updateBlock,xshift=1.5cm, yshift=-1.25cm] {$i=i+1$};
        \node (evalBlock)   [base, right of=mapBlock, xshift=1.5cm, yshift=-1.25cm] {Evaluate Objective Function};
        \node (finishBlock) [base, right of=evalBlock,xshift=1.5cm, ] {Finish if $i=i^{MAX}$ or $fobj=0.0$};
        % Specification of lines between nodes specified above
        % with aditional nodes for description 
        \draw [->] (start) -- (updateBlock);
        \draw [->] (updateBlock) |- (mapBlock);
        \draw [->] (mapBlock) -| (evalBlock);
        \draw [->] (evalBlock) |- (incBlock);
        \draw [->] (incBlock) -| (updateBlock);
        \draw [->] (evalBlock) -- (finishBlock);
    \end{tikzpicture}
    \caption{Flowchart showing an overview of the optimization workflow for determining the parameter vector $\theta$.}
    \label{fig:flowchart}
\end{figure}

Inverse problems are typically ill-posed; that is, the solution is non-unique. To help mitigate the non-uniqueness of the solution, constraints in the form of (1) prior information and (2) parameter constraints are imposed on the solution. Prior geologic information is incorporated through models of latent Gaussian variables (Section \ref{subsec:03latent}) and forms the basis of the \gls{NMR} output. Careful design of these latent variables ensures the optimization algorithm explores the appropriate solution space from a geologic perspective and acts as a regularization element \citep{zhou2014inverse}. The model output is a mixture of these latent variables; the observed statistics are reproduced by integrating spatial features from the prior models. Geologically reasonable prior information ensures a solution for $\theta$ is feasible. Limiting parameter values based on their physical meaning can further constrain the problem. As the \gls{NMR} is a positive, non-linear combination of the latent variables, each latent variable's relative contribution cannot be negative; these values are always constrained to be $\geq 0$. Uncertainty in the parameter vector $\theta$ is captured by considering multiple realizations of the prior models during optimization. The objective function is minimized in expectation; that is, it considers the mismatch error across all realizations. Considering a space of uncertainty in the prior models ensures the parameters are not overly sensitive to the features of a particular realization, acting as an additional regularization element.

Determining the parameters of the \gls{NMR} does not directly consider the reproduction of the observed data values but rather the observed two- and multi-point statistics. The goal is to learn how to map from the latent to the observed space, which results in the desired spatial structure. As discussed above, multiple distributions could reproduce these statistics, so explicit data matching is unnecessary, simplifying the optimization. Additionally, by relaxing this data-matching constraint, the prior latent models can be generated through unconditional simulation. Exact reproduction of the observed data values is ultimately required, and Chapter \ref{ch:05impute} addresses this imputation problem in detail.


% \begin{enumerate}
%     \item directed Monte Carlo approach
%     \item to capture uncertainty multiple solutions (realizations) are generated?
%     \item approached with optimization, presented in Chapter 4
%     \item objective function considers two- and multi-point statistics
%     \item considers non-linearity
%     \item actual data observations are reproduced only during imputation
%     \item ill-posedness accounted for through multiple realizations and reproduction of statistics in expectation
%     \item iteratively adjust model parameters until the acceptable match is achieved
% \end{enumerate}

\subsection{Latent Spatial Structure}
\label{subsec:03latent}

The set of latent Gaussian variables of Equation \ref{eq:gpool} is the foundation of the \gls{NMR}. This set is referred to as the Gaussian ``pool'' throughout this text; the components of the pool are referred to as latent ``factors''. It is a pool in that there is a collection of Gaussian \glspl{RF} to be shared with the goal of reproducing high-order statistics. This pooling is analogous to a \gls{GMM} in the spatial context. The idea of a \gls{GMM} is that a finite mixture of Gaussian densities can approximate a continuous distribution (as in Equation \ref{eq:zu}) \citep{mclachlan2019finite}. Furthermore, \cite{silva2018multivariate} shows that a mixture of Gaussian components can fit complex non-Gaussian uni- and multivariate distributions. The \gls{NMR} is an extension of this concept; rather than a mixture of Gaussian densities, $Z(\mathbf{u})$ is approximated by a mixture of Gaussian \glspl{RF}. Rather than using the \acrlong{EM} algorithm to fit the parameters of each Gaussian component \citep{mclachlan2019finite}, $M$ standard Gaussian \glspl{RF}, with covariance structures specified by $\gamma_{m}(\mathbf{h})$, are explicitly chosen. The factors are chosen such that:
\begin{enumerate}[noitemsep]
    \item Latent factors are standard normal: $E\left\{Y_{m}(\mathbf{u})\right\}=0$, \ $E\left\{Y_{m}(\mathbf{u})^{2}\right\}=1, \ \forall \ \mathbf{u}$
    \item Latent factors are independent: $E\left\{Y_{m}(\mathbf{u}_{i})Y_{n}(\mathbf{u}_{j})\right\}=0, \ \ \forall \ m\neq n, \ \forall \ i \neq j$
    \item Latent factors reproduce their respective variogram model $\gamma_m(\mathbf{h}), \ \forall \ \mathbf{h}$
\end{enumerate}
The output of the mapping function $\mathcal{F}_{\theta}$ is a spatial mixture of the latent factor components of $\mathbf{Y}(\mathbf{u})$.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/uncond_f1_f2_grid.png}
    \caption{Unconditional realizations of Gaussian \glspl{RF} with orthogonal directions of continuity.}
    \label{fig:uncond_f1_f2_grid}
\end{figure}

Consider a hypothetical scenario where the modeling goal is to have highly continuous low grades in the east-west direction and highly continuous high-grade continuity in the north-south direction. This orthogonal continuity is challenging for a multivariate Gaussian simulation algorithm, as a normal score variogram cannot simultaneously capture both directions of continuity. The \gls{NMR} can draw different spatial structures from different latent factors within the pool; the sharing of structures is a core concept of the \gls{NMR} approach. Consider the \gls{2D} realizations of two unconditional Gaussian \glspl{RF} in Figure \ref{fig:uncond_f1_f2_grid}. It is possible to achieve the modeling goals if we can draw high-grade continuity from factor 1 and low-grade continuity from factor 2. A simple linear combination of the factors, analogous to the \gls{LMC}, cannot capture the orthogonal continuities as high and low values cancel when combined. Before mixing, we must emphasize the high values in factor 1 (and mute the low values) and the low values in factor 2 (and mute the high values). This ``extraction'' of specific factor features requires the introduction of a non-linearity.

\subsection{Non-Linearity and Mapping}
\label{subsec:03nonlinear}

Emphasizing a portion of a range of values requires the definition of a threshold. Given item (1) from the list in the previous section, zero is the logical threshold value of a Gaussian distribution. A straightforward approach to introducing non-linearity is applying a power law type function. The non-linear value is the original value raised to a power, $\omega$. If we want to emphasize the high values, all values $> 0$ are raised to the exponent $\omega \geq 1$ while the values $< 0$ are raised to the exponent $\frac{1}{\omega}$. The non-linearity effectively mutes the magnitude of values below zero and exponentially increases values above zero. The opposite is true if we wish to emphasize low values; all values $< 0$ are raised to the exponent $\frac{1}{\omega}$ while the values $> 0$ are raised to the exponent $\omega$, where $\omega < 1$. Figure \ref{fig:powerlaw_f1_f2} shows this power law relationship graphically for the two realizations shown in Figure \ref{fig:uncond_f1_f2_grid} using $\omega_{1}=4$ and $\omega_{2}=0.25$. Where factor 1 is greater than zero, the function significantly emphasizes values $> 1$. Where factor 1 is less than zero, the function mutes or dampens values $< -1$. This non-linearity advantageously isolates the high-grade structure from factor 1. The opposite is true for factor 2. Chapter \ref{ch:04implement} presents the complete details of the non-linearity $\phi(\mathbf{y}, \boldsymbol{\omega})$.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/powerlaw_f1_f2.png}
    \caption{Power law relationship between the linear input $y$ and the non-linear output $\phi(y, \omega)$, where $\phi(\dots)$ is the power law function. }
    \label{fig:powerlaw_f1_f2}
\end{figure}

Figure \ref{fig:activate_f1_f2_grid} (left, center) shows the spatial distribution of factors 1 and 2 after applying the power law function. Note the magnitude of the colour bars between the plots. The power law function can isolate the desired components of each factor and can be thought of as an ``activation'' function. Figure \ref{fig:activate_f1_f2_stats} (left, center) shows the histograms of factors 1 and 2 after transformation. As expected $\phi(y_{1}, \omega_{1})$ is strongly positively skewed, and $\phi(y_{2}, \omega_{2})$ is strongly negatively skewed. $\phi(\dots)$ results in a distribution of arbitrary activation units. These units are not interpretable and require combination and mapping to an observed space in Gaussian units. Combining the ``activated'' realizations is a weighted, linear combination, followed by a normal score transform. For the sake of the example, the weights to each factor are $\mathbf{a} = [1,1]$. The final model is then $\mathbf{z} = G^{-1}(\phi(\mathbf{Y}, \boldsymbol{\omega}) \cdot \mathbf{a}^{T})$, where $G^{-1}$ is the inverse of the Gaussian \gls{CDF}.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/activate_f1_f2_grid.png}
    \caption{Factors 1 (left) and 2 (center) after application of the power law function, and the final mapped, normal score transformed realization (right).}
    \label{fig:activate_f1_f2_grid}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/activate_f1_f2_stats.png}
    \caption{Histograms of factors 1 (left) and 2 (center) after application of the power law function, and the final mapped, normal score transformed realization (right).}
    \label{fig:activate_f1_f2_stats}
\end{figure}

Figure \ref{fig:activate_f1_f2_grid} (right) shows the spatial distribution of the final model, while Figure \ref{fig:activate_f1_f2_stats} (right) shows the histogram. Note that the spatial distribution is complex and non-multivariate Gaussian. However, the univariate histogram is perfectly Gaussian. The high values show north-south continuity, while the lows show east-west continuity. The overprinting of the factors appears natural, resulting in a spatial structure that a single variogram model cannot generate. These steps provide an overview of the forward pass through the \gls{NMR}. In summary, they are:
\begin{enumerate}[noitemsep]
    \item Unconditionally simulate latent factors to form the Gaussian pool
    \item Activate the factors with $\boldsymbol{\omega}$ to emphasize certain features
    \item Linearly combine the activated factors with weights $\mathbf{a}$
    \item Normal score transform the combination
\end{enumerate}

In practice, parameters $\boldsymbol{\omega}$ and $\mathbf{a}$ are unknown and must be inferred. As Section \ref{subsec:03nmrinverse} discusses, inverse problems are commonly approached by optimization. The optimization problem consists of determining $\boldsymbol{\omega}$ and $\mathbf{a}$ for a given Gaussian pool to minimize an objective function. The objective function in this context is the reproduction of desired two- and multi-point statistics in the final model produced by the steps above.

\subsection{Two-Point Statistics}
\label{subsec:03twopt}

Two-point statistics are the experimental normal score and indicator variograms. The experimental variogram is the expected squared difference between data pairs separated by lag vector $\mathbf{h}$. Figure \ref{fig:varios_activate_f1_f2} shows the normal score variogram (blue), the 0.1 quantile indicator variogram (black), and the 0.9 indicator variogram (red) of the mapped model in the north-south (left) and east-west (right) directions, respectively. The final model achieves its goals from the perspective of two-point statistics. The indicator variograms show increased low-grade continuity in the east-west direction and high-grade continuity in the north-south direction. The normal score variogram is a blend of the indicator variograms; in the north-south direction, it has the continuity of the highs with the cyclicity of the lows and the range of the lows and cyclicity of the highs in the east-west direction. This feature blending further emphasizes how a single variogram cannot capture the complexity of the \gls{NMR}.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/varios_activate_f1_f2.png}
    \caption{Normal score experimental variogram (blue), the 0.1 quantile indicator experimental variogram (black), and the 0.9 indicator experimental variogram (red) of the mapped model in the north-south (left) and east-west (right) directions, respectively.}
    \label{fig:varios_activate_f1_f2}
\end{figure}

An interesting experiment to confirm this behaviour is to generate a realization with a multivariate Gaussian simulation algorithm using the normal score variogram from Figure \ref{fig:varios_activate_f1_f2} and compare it to the final \gls{NMR} model. Figure \ref{fig:nmr_sgs_grid} shows the same \gls{NMR} realization from above (left) and a realization generated with \gls{SGS} (right). The \gls{SGS} model uses the variogram model fitted to the blue experimental variogram in Figure \ref{fig:varios_activate_f1_f2}. The \gls{NMR} and \gls{SGS} models are significantly different, though they share the same two-point covariance structure. The \gls{SGS} model shows less connectivity in low and high values and generally more disorder. The normal score variogram shows a mixing between the orthogonal structures and cannot adequately capture multiple anisotropies when considering the full range of values. In contrast, the \gls{NMR} isolates a particular covariance structure to values above and below zero.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/nmr_sgs_grid.png}
    \caption{ The final \gls{NMR} model (left) and a \gls{SGS} model generated with the same normal score variogram model. }
    \label{fig:nmr_sgs_grid}
\end{figure}

Beyond two-point statistics, the network approach also considers the multi-point statistics discussed in Section \ref{sec:03connect}. A non-Gaussian \gls{RF} should show increased multi-point connectivity of extreme values over a Gaussian \gls{RF}. Whether it is connectivity of high or lows depends on the structure of the latent pool and power law exponents, $\omega$.

\subsection{Multi-Point Statistics}
\label{subsec:03multipt}

Consider the \gls{2D} models above as sections rather than plan view. We can ``drill'' the realizations to generate sequences of data. Sampling the realization results in 32 synthetic drillholes, each with 64 samples. Figure \ref{fig:nmr_sgs_dh_cont_ind} shows the drillhole configuration and the corresponding 0.9 quantile indicator transform for the \gls{NMR} (left) and \gls{SGS} models (right). The direction of continuity of the high-grade factor corresponds with the direction of drilling.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/nmr_sgs_dh_cont_ind.png}
    \caption{ Synthetic drillhole samples and corresponding 0.9 quantile indicator transform for the \gls{NMR} model (left) and the \gls{SGS} model (right).}
    \label{fig:nmr_sgs_dh_cont_ind}
\end{figure}

As expected, the \gls{NMR} model visually shows increased high-grade connectivity. The more structured high-features result in fewer total but longer connected sequences. The \gls{SGS} model is more disorganized, resulting in shorter connected sequences. The $n$-point connectivity function in Figure \ref{fig:nmr_sgs_grid_runs} quantifies this visual discrepancy. The probability of connection above the 0.9 quantile for the multivariate Gaussian model steeply declines in the first five steps and is effectively zero at seven steps. The probability of connection in the non-Gaussian model decreases notably slower and remains $> 0$ at 20 connected steps.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/nmr_sgs_grid_runs.png}
    \caption{ $N$-point connectivity function for the 0.9 quantile indicator transform of the drillholes in Figure \ref{fig:nmr_sgs_dh_cont_ind}.}
    \label{fig:nmr_sgs_grid_runs}
\end{figure}

Connectivity measures are calculated globally, that is, considering all drillholes. The primary orientation of drilling is an important consideration when designing latent factors with the goal of high-order connectivity. In practice, high-grade structures are generally orthogonal to the primary orientation of drilling, so specific factors aligned with the tertiary variogram direction may be required.

\subsection{Latent Imputation}
\label{subsec:03impute}

The previous sections consider using unconditional realizations to infer the mapping function $\mathcal{F}_{\theta}$. The ultimate goal is to generate gridded realizations conditional to the observed data with non-Gaussian spatial features characterized by $\theta$. This process requires gridded conditional realizations of each latent factor in the pool. As the latent factors are a synthetic construct of the \gls{NMR} model and not directly observed, they must be imputed \citep{little2019statistical}. The imputed factors have the conditions listed in Section \ref{subsec:03latent} plus the additional constraint that they must reproduce the observed data (within a specified tolerance) when mapped through $\mathcal{F}_{\theta}$:
\begin{equation}
    \mathcal{F}_{\theta}(\mathbf{y}(\mathbf{u})) = \mathbf{z}(\mathbf{u}) \pm \alpha, \ \forall \mathbf{u}
    \label{eq:ftheta0}
\end{equation}

\lowercase{Where} $\alpha$ is a data matching tolerance. A straightforward approach to satisfying Equation \ref{eq:ftheta0} is to assign random Gaussian values to the vector $\mathbf{y}(\mathbf{u})$ until the equality is met \citep{silva2018enhanced}. This approach ensures the correct collocated multivariate relationship between the latent factors. However, it does not ensure that each regionalized factor has the correct spatial variability. If the imputed latent factors do not have the correct spatial variability defined by the pool, the mapping function $\mathcal{F}_{\theta}$ is no longer valid. Directly sampling the high-dimensional multivariate distribution is difficult, though sampling the marginal conditional distributions is possible. This problem is typically approached by a Gibbs sampler \citep{geman1984stochastic} and commonly employed in the truncated Gaussian simulation paradigm \citep{arroyo2020iterative,madani2021enhanced}. Noted Gibbs sampler convergence issues with spatially correlated data motivates the development of a novel imputation algorithm presented in Chapter \ref{ch:05impute}. The algorithm combines sequential simulation and rejection sampling components to iteratively sample the marginal distributions, resulting in latent factors that satisfy Equation \ref{eq:ftheta0} and have the correct covariance structure.

The solution to Equation \ref{eq:ftheta0} is non-unique. Multiple combinations of latent variables can reproduce the observed value when mapped. Figure \ref{fig:imputed_scatters_example} shows the relationship between two imputed factors and the observed value, $z(\mathbf{u})$. In this example, if $z(\mathbf{u})$ is high, say 1.5, factor 2 is constrained to be high. However, factor 1 can take any value from the range of $[-2.5, 2.5]$. The opposite is true for low values, where factor 1 is constrained. The \gls{NMR} framework utilizes multiple imputation to transfer uncertainty related to non-uniqueness to the final gridded realizations. Multiple latent factor realizations are imputed, and a unique data realization conditions each gridded realization.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.8, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/imputed_scatters_example.png}
    \caption{Bivariate scatter plots comparing the observed value, $z$, and factors 1 and 2. The histograms are the marginal distributions of each variable. }
    \label{fig:imputed_scatters_example}
\end{figure}

Chapter \ref{ch:05impute} presents the details of the imputation algorithm and the of checking latent data realizations. The algorithm shows stable convergence for spatially correlated variables and correctly reproduces the latent pool's collocated multivariate relationships and covariance structure.

\subsection{Simulation and Mapping}
\label{subsec:03simulate}

After imputing all latent factors at the data locations, they are conditionally simulated at grid node locations. Any conditional simulation algorithm is valid; different algorithms could be used for different structures depending on the range of correlation, structure type and anisotropies \citep{pinto2020independent}. Once the latent factors are defined at all grid nodes, the gridded realizations are mapped from latent to observed space with $\mathcal{F}_{\theta}$. The mapping function includes a normal score transform based on a reference distribution of activation values output by the network. The corresponding normal score values of these activations are used as a transform table to transform the gridded realizations into Gaussian units.

After transformation to observed space, the gridded \gls{NMR} realizations reproduce all specified two- and multi-point spatial features while being univariate Gaussian. Transforming the realizations to original units is simply the inverse of the normal score transform. Chapter \ref{ch:05impute} presents further details regarding the checking and validating of both latent and mapped gridded realizations.


\FloatBarrier
\section{Effects of High-Order Continuity}
\label{sec:03effect}

The \gls{NMR} framework permits a flexible approach to continuous variable simulation in the presence of non-Gaussian geologic domains. One can design latent factors to account for differences between background and high-grade mineralization or changes in the orientation of continuity if sub-domains cannot be defined. The flexibility of the latent pool is advantageous, providing $M \cdot 7$ orientation, range and covariance structure parameters, compared to the 7 of a single variogram model in a traditional multivariate Gaussian simulation algorithm. The following section presents a small synthetic, non-Gaussian example to highlight the effect of multi-point connectivity on expected mineral resources. \Gls{NMR} resources are contrasted against those from \gls{SGS}.

The reference truth comes from a natural image of fork lightning rotated 90 degrees. The image is chosen as it exhibits narrow, highly connected, high-value, dendritic features with abrupt changes in grade between the lightning and the background features. It is reasonable to suggest similarities between the image and narrow vein-type mineral deposits. The image is rotated 90 degrees, so the synthetic drillholes are roughly orthogonal to the lightning structures. The \gls{RGB} colour channels are averaged, generating a grayscale image. The grayscale image is normalized $\in [0, 1]$ and then transformed to a log-normal distribution with a $\mu = 1$ and $\sigma = 2$. Data with a \gls{CV} of 2 is common in mineral systems with positively skewed distributions, such as precious metals. Twenty synthetic drillholes are sampled from the image, extracting every third pixel, resulting in 1560 samples. Figure \ref{fig:lightning_dhs} shows the reference image (left) and the synthetic drillhole configuration (right). The drillhole data is strongly non-stationary; the high-grade features are vertically continuous in some areas and horizontally in others. The dip of the structures also changes locally. The goal of the \gls{NMR} model is to capture these non-stationary features and correctly characterize the connectivity of high and extreme values observed in the true image. Though synthetic, this scenario provides an excellent opportunity to highlight the capabilities of the \gls{NMR}.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.5, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/lightning_dhs.png}
    \caption{The non-stationary reference truth image of fork lightning (left) and the drillhole samples extracted from the image (right). }
    \label{fig:lightning_dhs}
\end{figure}

The normal score variogram model plus two additional highly anisotropic factors comprise the Gaussian pool. The normal score variogram model exhibits strong zonal anisotropy where the range in the direction parallel to the sampling is larger than the domain size. The first of the highly anisotropic factors is oriented 100 degrees with a $10:1$ anisotropy ratio, and the second is oriented 45 degrees with a  $5:1$ anisotropy ratio. The normal score variogram factor can influence any grade range with $\omega \in [0.25, 4.0]$ while the additional factors are constrained to influence high values with $\omega \in [2.0, 4.0]$. The first factor can capture the orientation of medium to high-grade values, while factors two and three capture the orientation of the highest-grade structures. The mapping function, $\mathcal{F}_{\theta}$, is inferred by minimizing the sum of squared errors between the objective function components and the two and multi-point statistics extracted from the synthetic data. Chapter \ref{ch:04implement} gives the complete details of the objective function and optimization algorithm. One hundred realizations of each latent factor are imputed at the data locations such that Equation \ref{eq:ftheta0} holds using $\alpha = 0.01$. Once the latent factors are defined at the data locations, they are conditionally simulated on a 1 x 1 pixel point scale grid using \gls{SGS}. The point scale latent realizations are mapped through the same function, resulting in a univariate Gaussian spatial mixture with features characterized by $\theta$.

The point scale realizations are back-transformed from Gaussian to original units and block averaged to a 5 x 5 pixel \gls{SMU} scale grid. The \gls{SMU} scale realizations are post-processed to calculate the e-type mean. Figure \ref{fig:lightning_indicators} shows the block averaged reference truth (top left) and corresponding 0.9 quantile indicator transform (bottom left), with the \gls{SMU} scale \gls{NMR} e-type model and indicators (top middle, bottom middle), and the \gls{SMU} scale \gls{SGS} e-type model and indicators (top right, bottom right). An outcome of correctly characterizing the point scale, high-grade continuity is that the \gls{SMU} scale realizations should show more connectivity. As discussed in Section \ref{subsec:03ngmeasures}, disordered realizations are more sensitive to changes in scale; connected or organized features should remain as scale increases (to an extent). This connectivity is evident in the 0.9 quantile indicator transform of the \gls{NMR} e-type model. The indicator model exhibits increased east-west high-grade continuity over the \gls{SGS} model, particularly in the central and top portions of the grid. The \gls{SGS} model effectively captures the vertical high-grade continuity; however, the east-west structures are visibly more disconnected.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/03-Ch03/lightning_indicators.png}
    \caption{The reference truth block averaged to a 5 x 5 pixel \gls{SMU} (top left) and corresponding 0.9 quantile indicator transform (bottom left), with the \gls{SMU} scale \gls{NMR} model and indicators (top middle, bottom middle), and the \gls{SMU} scale \gls{SGS} model and indicators (top right, bottom right)}
    \label{fig:lightning_indicators}
\end{figure}

Table \ref{tab:lightning_resources} summarizes the resources of the \gls{SMU} scale \gls{NMR} and \gls{SGS} models above the 0.1, 0.5, and 0.9 quantiles as a fraction of true resources. The resources assume the synthetic variable is measured in grams per metric tonne, priced in troy ounces, and density is a constant value of $2.6 g/cm^{3}$. The \gls{NMR} and \gls{SGS} models show similar resources for the 0.1 and 0.5 quantile cutoffs. However, the  \gls{NMR} model shows improvement in both tonnes and grade above the 0.9 quantile cutoff, leading to a 9\% increase in contained metal ounces relative to the \gls{SGS} model. The increase in tonnes above the true 0.9 quantile is attributed to the increase in east-west continuity imparted by factors two and three. A single covariance model considering the complete range of grade values cannot effectively capture the non-stationary features of the true image.

\begin{table}[!htb]
    \centering
    \caption{\Gls{SMU} scale resources above the 0.1, 0.5, and 0.9 quantiles as a fraction of the true resources. Cutoff values are calculated from the true image. g/t=grams per tonne.}
    \resizebox{1\width}{!}{\input{0-Tables/lightning_resources.tex}}
    \label{tab:lightning_resources}
\end{table}

Drill hole data may exhibit non-Gaussian features that are difficult to capture with a single covariance model based on two-point statistics. The \gls{NMR} model permits generating multiple realizations that consider both two- and multi-point statistics from the observed data. These considerations allow for extreme values that are more structured than what is possible with a multivariate Gaussian simulation algorithm. This small example emphasizes the importance of the connectivity of extreme values concerning \gls{SMU} scale resources. Correctly characterizing the connectivity of extremes significantly impacts the contained metal of a resource estimate.

\FloatBarrier
\section{Discussion}
\label{sec:03discuss}

The \gls{NMR} is a framework for generating non-Gaussian spatial fields. Characterizing non-Gaussian spatial features, like the connectivity of extreme values, requires statistics above the second order. It is shown that these higher-order statistics, such as the distribution of runs and the $n$-point connectivity function, can differentiate non-Gaussian from Gaussian sequences. The \gls{NMR} is designed to overcome some shortcomings of the multivariate Gaussian \gls{RF} model, particularly with strongly positively skewed distributions. This advancement is achieved by considering both two- and multi-point statistics in generating a probabilistic model. Examples highlight spatial structures that cannot be reproduced with a two-point covariance structure and a multivariate Gaussian simulation algorithm. A core concept of the \gls{NMR} is that it is a spatial \gls{GMM}, where the mixing of Gaussian components results in a non-Gaussian output.

A unique covariance structure defines each component of the mixture; this ``pool'' of components is designed such that certain components impart certain spatial features in the final model. A power law activation function applies non-linearity to the input components. This non-linear activation function allows features of the latent factors to be emphasized in particular regions of the continuous grade range. That is, the low values and high values can have different spatial structures and different multi-point connectivity. This difference in continuity is something that cannot be easily achieved with two-point statistics alone. The spatial features of the final model are defined at the beginning of the modeling process. The practitioner specifies the model's goals using two-point and high-order statistics. These goals include the normal score variogram model, indicator variogram models, cumulative run-length frequencies, and the $n$-point connectivity function. The parameters of the mapping function are inferred with stochastic optimization. These parameters result in a spatial mixture reproducing the statistics outlined in the modeling goals. The generation of conditioning data for latent factor realizations is an imputation problem, and these imputed factors become conditioning data for gridded factor realizations. Finally, the gridded factors are mapped to observed space with the inferred parameters, resulting in the final gridded realizations with the correct high-order statistics.

A small example highlights the ability of the \gls{NMR} to capture connected extreme values and non-stationary features like orientation changes. A well-designed Gaussian pool can accommodate multiple orientations and anisotropies. The model significantly improves contained metal relative to an \gls{SGS} model characterized by a two-point covariance structure. The following chapter presents the details of network components of the \gls{NMR} framework, including latent factor design and parameter inference.