%!TEX root = ../Thesis_example_compile_this.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Latent Factor Imputation}
\label{ch:05impute}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter presents a novel algorithm for imputing latent factors called \gls{SGRI}. Imputation of latent factors implies a problem where all values are missing or unknown. Conditioning gridded realizations of latent factors requires assigning valid latent values at the data locations. The goal is to impute latent factors with a unique covariance structure that, when combined with inferred \gls{NMR} mapping, returns the true observed values at the data locations within a tolerance. Traditionally, this problem has been approached through Gibbs sampling, though there are noted challenges in achieving stable convergence with spatially correlated variables \citep{silva2018enhanced}. \gls{SGRI} is a simple approach that combines \gls{SGS}, rejection sampling and exact data matching. Conditional moments are calculated with \gls{SGS}, ensuring the spatial relationships are correct and iterative rejection sampling ensures the collocated multivariate relationships are correct. Realizations of latent factors are generated and become conditioning data for gridded realizations. When mapped to observed space, the realizations reproduce non-Gaussian spatial features specified by the \gls{NMR} and uni- and multivariate statistics. A small synthetic example continued from Chapter \ref{ch:04implement} demonstrates imputing latent factors, conditional simulation and mapping the latent space to observed space.

\FloatBarrier
\section{Imputation Concepts}
\label{sec:05imputation}

Imputation is a method to ``fill in'' missing values \citep{little2019statistical}. Missing values may be uni- or multivariate, and multiple mechanisms or patterns of missingness may be possible. Multivariate transformations, common in modern geostatistical workflows, such as \gls{PPMT} \citep{barnett2014projection}, require homotopic sampling, necessitating imputation methods. Ignoring these missing values results in a loss of information or potential bias if the missingness mechanism is not random. The simplest deterministic approach to imputation is taking the global mean or median of sampled values or employing a regression model. This inference permits using all data in subsequent modeling; however, it captures no uncertainty in the imputed values.

Single imputation involves imputing a single value for each missing data value. \cite{little2019statistical} describes single imputation techniques as (1) mean imputation, where the global mean value is substituted; (2) regression imputation, where a predicted value from the regression of the missing variable on the observed variables replaces missing values; and (3) stochastic regression imputation where a predicted value plus a residual replace missing values. The single imputation paradigm indicates that the missing values are certain or constant. Though this imputation approach includes all data in statistical analysis, this assumption results in incorrect uncertainty as the true values are unknown. Furthermore, datasets imputed with mean or regression imputation will not have the correct mean or variance \citep{barnett2015multivariate}. For these reasons, multiple imputation techniques are preferred when characterizing imputation uncertainty, which is important.

Multiple imputation involves generating realizations of missing values, allowing assessment of imputation uncertainty. A model of the conditional distribution of the missing values given the observed values is inferred and then stochastically sampled, resulting in complete dataset realizations. Multiple imputation in a geostatistical context is often a constrained problem where the imputed values must (1) reproduce underlying multivariate relationships and (2) reproduce the spatial variability of observed values \citep{barnett2015multivariate}. Collocated variables characterize the multivariate relationships, and the covariance structure of the observed variables characterizes the spatial variability. The conditional distribution from which the imputed values are drawn is informed by these components \citep{hadavand2023spatial}.

A \textit{latent variable} is a variable that is not directly observed but is assumed related to, and can be inferred from, measured or observed variables \citep{everitt2010cambridge}. Imputation of latent factors is a unique imputation problem where all variables are missing or unsampled \citep{little2019statistical}. The latent factors are not directly observed; they are a synthetic feature of the inferred mathematical model. An example of a latent model is the \gls{LMR}, which is composed of multiple latent independent random factors operating at different scales \citep{goovaerts1992factorial}. The latent factors are never measured or directly observed but characterize the regionalized random variable $Z(\mathbf{u})$. Simulating geologic latent variables subject to other observations or constraints is a key component of truncated Gaussian categorical simulation techniques. The techniques in this chapter do not directly consider categorical values as constraints, though they enforce the reproduction of the true continuous values. Latent variable imputation is most commonly performed with a Gibbs Sampler \citep{emery2014simulating,silva2017multiple,arroyo2020iterative,madani2021enhanced} or alternatively the sequential spectral turning bands method \citep{lauzon2020calibration,lauzon2020sequential,lauzon2023joint}. The Gibbs sampler is a Markov chain Monte Carlo method used to sample a multivariate distribution where direct sampling is complex but sampling marginal distributions is simple. The Gibbs sampler is practical for indirectly sampling high-dimensional distributions using univariate conditional distributions, though convergence of the algorithm is a known issue with correlated variables \citep{silva2018enhanced}. Many data require a restricted kriging search, which may cause the simulated Gaussian vector to deviate from the desired covariance matrix \citep{emery2014simulating}.


\FloatBarrier
\section{Gibbs Sampler}
\label{sec:05gibbs}

The Gibbs sampler \citep{geman1984stochastic} is an iterative simulation algorithm designed to sample an $M$-dimensional multivariate distribution $f(y_{1}, \dots, y_{M})$ of $M$ random variables $\{Y_{1}, \dots, Y_{M}\}$ \citep{little2019statistical}. It is particularly useful where sampling the joint distribution is difficult, but sampling the marginal conditional distributions $f(y_{m}|y_{1},\dots,y_{m-1},y_{m+1},\dots,y_{M}), \ \ m=1,\dots,M$ is possible. The following general steps summarize the Gibbs sampler:

\begin{enumerate}[noitemsep]
    \item Initialize counter $t=0$
    \item Initialize a valid arbitrary vector $y^{(0)}$
    \item For each dimension $m=1,\dots,M$:
          \begin{enumerate}[noitemsep]
              \item $t=t+1$
              \item set $y^{(t)}_{j} = y^{(t-1)}_{j} \ \ \forall j \neq m$
              \item draw $y^{(t)}_{m}$ from  $f(y_{m}|y_{1},\dots,y_{m-1},y_{m+1},\dots,y_{M})$
              \item finish if $t=t_{MAX}$
          \end{enumerate}
    \item Return $y^{(t)}$
\end{enumerate}

Over a sufficient number of iterations, the sampled vector converges on the joint distribution. In a geostatistical context, the Gibbs sampler is based on the fact that the distribution of a Gaussian vector $Y$ conditioned on other values is Gaussian; the mean and variance of this distribution are calculated by simple kriging \citep{emery2014simulating}. The Gibbs sampler simulates both conditional and unconditional vectors. Conditional Gibbs simulation is commonly used for latent variable assignment in truncated-Gaussian techniques \citep{armstrong2011plurigaussian,silva2017multiple} where latent values must respect both the mapping between categorical and continuous space and match the categorical observations when truncated. Imputation of latent factors in the context of this work does not require satisfying inequality constraints at the data locations. However, it does require satisfying the mapping condition between observed and latent space.

\cite{silva2018enhanced} documents Gibbs sampler convergence issues related to restricting the search neighbourhood for updating the marginal conditional distributions. Considering all $n$ data requires the inversion of a rank $n-1$ covariance matrix which becomes unpractical with increasing $n$. Restricting the search to reduce the size of the covariance results in an approximation of the conditional moments and affects algorithm convergence \citep{emery2014simulating, lauzon2020sequential} as realization quantiles diverge to extreme highs and lows with increasing iterations. \cite{lantuejoul2012simulation} proposed the propagative Gibbs sampler to avoid the matrix inversion requirement, though \cite{silva2018enhanced} shows convergence issues are still present with greater than two latent variables and complex truncation rules. These convergence challenges motivate the development of a new simulation algorithm for the imputation of latent geologic variables. The \gls{SGRI} algorithm also utilizes a restricted search neighbourhood to calculate conditional moments. However, the relaxed constraints relative to imputation for truncated Gaussian methods allow for stable convergence.


\FloatBarrier
\section{Sequential Gaussian Rejection Imputation}
\label{sec:05sgri}

Generating Gaussian \gls{NMR} realizations requires inference of the latent factors at the data locations. These imputed factors become conditioning data for simulation on a regular grid. The problem involves assigning $M$ unknown latent Gaussian values with the correct spatial structure at the data locations. When mapped through the \gls{NMR}, these imputed values must also reproduce the observed regionalized random variable $Z(\mathbf{u})$. This solution is non-unique, and multiple combinations of latent factors could reproduce the observed values. In order to correctly transfer this latent uncertainty, a multiple imputation \citep{barnett2015multivariate} approach is adopted where each realization of $Z(\mathbf{u})$ is generated with a unique imputed realization, $\mathbf{y}$.

\Gls{SGRI} is an iterative simulation algorithm for imputing continuous latent Gaussian variables subject to a mapping constraint $\mathcal{F}_{\theta}\left(\mathbf{y}\right)=\mathbf{z} \pm \alpha$, where $\alpha$ is a data matching tolerance. \Gls{SGRI} first iteratively samples all univariate conditional distributions of the $M$-dimensional latent distribution until the mapped value is within a coarse tolerance with the observed value. After the simulation meets the coarse tolerance, a polishing step iteratively perturbs each initial imputed value until the mapped value is within a second, finer tolerance with the observed value. At any point during the perturbation, the sample is rejected if the new value does not decrease the error between observed and imputed. The rejection component of the algorithm is not rejection sampling in the strict statistical sense but rather a constraint to ensure the solution remains within a space of feasible solutions. Initial sampling of the conditional distributions ensures the correct covariance structure for each latent factor, and iterative polishing ensures data reproduction. Though \cite{armstrong2011plurigaussian} suggests rejection sampling is not feasible for latent imputation, the relaxed constraints relative to imputation for truncated-Gaussian techniques allow for stable algorithm convergence.

Latent factors are imputed such that when mapped through the \gls{NMR}, the observed values at location $\mathbf{u}_{i}$ are reproduced exactly, and each latent factor reproduces its covariance structure. Figure \ref{fig:impute_schematic} shows this relationship schematically with a sketch of a drillhole with $i=1,\dots,n$ observed data locations. The latent space consists of $m=1,\dots,M$ factors to be imputed at each observed location. The observed values are a function of the mapped latent factors. It is straightforward to generate independent Gaussian values that reproduce observed values; however, latent spatial continuity must also be correct. $F_{m}(y)=1,\dots,M$ \glspl{CDF} must be standard normal, and the spatial distribution must reproduce variograms $\gamma_{m}, \ m=1,\dots,M$.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=1.0, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/impute_schematic.png}
    \caption{A schematic drillhole showing the multivariate relationships between observed and latent spaces. The observed data values are a function $\mathcal{F}_{\theta}\left(\mathbf{y}\right)$ of the imputed latent factors at location $\mathbf{u}_{i}$. The imputed values must reproduce the true observed values, and each regionalized latent factor must reproduce its covariance structure $\gamma_{m}$. }
    \label{fig:impute_schematic}
\end{figure}

The three general steps of the \gls{SGRI} algorithm are as follows:
\begin{enumerate}[noitemsep]
    \item Calculate $m=1,\dots,M$ conditional means and standard deviations at the imputation location $\mathbf{u}_{i}$.
    \item Repeatedly sample the Gaussian vector $\tilde{\mathbf{y}}(\mathbf{u}_{i}) = \{\tilde{y}_{m}(\mathbf{u}_{i}), m=1,\dots,M \}$ from the conditional distributions until $\left|\mathcal{F}_{\theta}(\tilde{\mathbf{y}}(\mathbf{u}_{i})) - z(\mathbf{u}_{i})\right|$ is within a specified first tolerance.
    \item Iteratively refine the solution from (2) until $\left|\mathcal{F}_{\theta}(\tilde{\mathbf{y}}(\mathbf{u}_{i})) - z(\mathbf{u}_{i})\right|$ is within a specified second tolerance.
\end{enumerate}

The normal score transform within the latent to observed mapping function $\mathcal{F}_{\theta}$ is slightly different than in the context of network parameter inference in Chapter \ref{ch:04implement}. The \gls{SGRI} algorithm constructs a reference distribution for mapping trial latent vectors, $\tilde{\mathbf{y}}(\mathbf{u}_{i})$,  to a scalar value in observed space, $\tilde{z}(\mathbf{u}_{i})$. The reference distribution facilitates the normal score transform of a single NMR output value. The table is constructed by mapping a $10^{6}$ x $M$ dimensional table of standard normal independent factors through  $\mathcal{F}_{\theta}$, resulting in an exhaustive table of outputs of unitless, raw activation values. This output table is normal score transformed, and $\tilde{z}(\mathbf{u}_{i})$ values are calculated by linearly interpolating the normal score array. The notation $\mathcal{F}_{\theta}(\tilde{\mathbf{y}}(\mathbf{u}_{i}))$ implies a scalar Gaussian deviate. In the following sections, the subscript $i$ replaces the location vector $\mathbf{u}_{i}$ to simplify notation.

% \subsection{Factor Precedence}
% \label{subsec:05precedence}

% Application of precedence is an optional component of the \gls{SGRI} algorithm. Precedence allows the spatial features of certain latent factors to be prioritized in the mapping function. A particular factor can be given precedence during imputation if one desires its spatial features in certain portions of the grade range. Precedence is achieved by employing a simple sigmoid weighting function where $s(y) = \frac{1}{1+e^{-y}}$. This function is the same weighting function optionally applied during network parameter inference in Chapter \ref{ch:04implement}. The weighting function modifies the forward pass through the network to:
% \begin{equation}
%     \tilde{\mathbf{z}} = a_{p} \cdot \phi(\mathbf{y}_{p}) + \sum_{m=1}^{M-1} \phi(\mathbf{y}_{m}) \cdot s(\mathbf{y}_{p} \cdot x), \ \ m \neq p
%     \label{eq:wtpass}
% \end{equation}

% Where $a$ is the weight to the factor, subscript $p$ is the factor index with precedence, $\phi(\dots)$ is the power law activation function, $s(\dots)$ is the sigmoid function, and $x$ is a constant $\in [-10, 10]$. The sign and magnitude of $x$ controls what part of the grade range factor $\mathbf{y}_{p}$ influences. If $x<0$ $\mathbf{y}_{p}$ influences high values and $x>0$ influences low values. Figure \ref{fig:sigmoid} shows a weighting function where $x=-1.5$. The y-axis is the weight to the remaining factors for the range of values of $\mathbf{y}_{p}$. In this scenario $\mathbf{y}_{p}$ receives $\approx 95\%$ of the weight when equal to 2.0, $\approx 80\%$ of the weight when equal to 1.0 and so on. As the magnitude of $x$ increases, $s(\dots)$ tends towards a binary step function centered on zero.

% \begin{figure}[htb!]
%     \centering
%     \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/sigmoid.png}
%     \caption{Sigmoid weighting function where $x=-1.5$. The x-axis is the range of $\mathbf{y}_{p}$ and the y-axis is the weight to the remaining M-1 factors. }
%     \label{fig:sigmoid}
% \end{figure}

% The sigmoid weighting function allows a certain factor to take precedence where the observed data takes on either high or low values. This increased control is particularly useful for capturing local high- or low-grade continuity with well-designed latent factors.

\subsection{Precedence and Constraints}
\label{subsec:05constraints}

Application of precedence is an optional component of the \gls{SGRI} algorithm. Precedence is achieved by employing the same simgoid weighting function optionally applied during network parameter inference in Section \ref{subsec:04precedence}. Application of constraints is an optional component of the \gls{SGRI} algorithm that may be used in conjunction with factor precedence. The use of constraints enforces the values of a certain factor to be either above or below a chosen threshold. The goal of constraint application is to ensure that some high or low values of a chosen factor correspond with the high or low values of the observed data. When enforcing factor precedence, constraints are necessary as the mapping function $\mathcal{F}_{\theta}$ is inferred using unconditional realizations. There is no conditioning mechanism to ensure the low- or high-grade features of $\mathbf{y}_{p}$ align with low- or high-grade features in the data. $\mathcal{F}_{\theta}$ is non-unique and multiple latent vectors can reproduce $z_{i}$. Therefore, it is possible, for example, to have a negative value of $\mathbf{y}_{p}$ where $z_{i}$ is a large positive value. This scenario is not ideal if we explicitly want the spatial features of $\mathbf{y}_{p}$ where $z_{i}$ is high and warrants the use of constraints on $\mathbf{y}_{p}$ during imputation.

Constraints can be thought of as ``seeding'' some number of high or low values of $\mathbf{y}_{p}$ prior to imputation by using a semi-random path. The constraint threshold and an exclusion radius parameter control the number of seeded values. A threshold is specified, and all values in the observed data, either above or below, are flagged. These data locations are the initial seed locations. The initial seed locations are sorted in descending (high values) or ascending (low values) order based on the observed grade value. The first seed location is visited, and the algorithm removes all other initial seed locations within the exclusion radius from the available possible seed locations. The next valid seed location is visited; again, the algorithm removes all other locations within the radius from the possible locations. This process continues until all initial seed locations are included or excluded. The included seed locations become the first $n$ locations in the imputation path. Imputed values of $\mathbf{y}_{p}$ at these locations are constrained to be above or below the threshold through rejection. The initial excluded seed locations and all other data locations become the remaining $\text{ndata} - n$ locations in the imputation path. Imputation of the remaining $M-1$ factors uses a strictly random path through the locations.

The exclusion radius is an important parameter as the mean and variance of the imputed $\mathbf{y}_{p}$ distribution are sensitive to the number of constrained locations. For example, if the 0.9 quantile is used as a threshold with no exclusion radius, approximately 10\% of the imputation locations will be seeded. This strong degree of conditioning can inflate the mean and variance of the imputed values and cause a departure from the standard normal distribution. By enforcing an exclusion radius, only a subset of the locations are seeded, and the local effect of the conditioning is less pronounced. Seeding many locations may also negatively affect other factors or lead to non-convergence. Too strong of a constraint on $\mathbf{y}_{p}$ may lead to a scenario where the remaining factors lack sufficient flexibility to reproduce the observed value when mapped. Practice shows that an exclusion radius of 5-10 times the composite length is sufficient to impart the desired factors of $\mathbf{y}_{p}$ above or below the threshold without negatively affecting convergence.

\subsection{Conditional Moments}
\label{subsec:05condmom}

The algorithm begins by determining a random (unconstrained) or semi-random (constrained) path through all data locations to be imputed. At each imputation location, the conditional moments are solved using the normal equations:
\begin{align}
     & y_{m,i} \sim {\mathcal {N}}\left( \mu_{m,i}, \sigma_{m,i} \right)                           \\
     & \mu_{m,i} = \sum_{j=1}^{N_{m,i}} \lambda_{j,i} \times y_{m,j}                               \\
     & \sigma_{m,i}^{2} = 1 - \sum_{j=1}^{N_{i}} \lambda_{m,j,i} \times C_{m,j,i}                  \\
     & \sum_{j=1}^{N_{m,i}} \lambda_{m,j,i} \times C_{m,j,k} = C_{m,k,i} \ \ \forall k \in N_{m,i}
\end{align}

Where $N_{m,i}$ is the neighbourhood about location $i$ using search anisotropy derived from $Y_{m}$; this could either be all neighbours or a restricted search around location $i$. $C_{m,j,k}$ is the covariance between spatial locations $j$ and $k$ for latent variable $m$. The system of equations is solved $M$ times under the assumption of zero mean and unit variance, resulting in a vector of conditional means and standard deviations. Retaining all conditional moments allows fast, repeated simulation of spatially correlated latent values in the coarse search phase.

\subsection{Coarse Search}
\label{subsec:05coarse}

The coarse search begins after calculating the $M$ conditional moments at location $i$. The goal of the coarse search is to find the trial vector $\tilde{\mathbf{y}}_{i}$ that closely reproduces $z_{i}$ and has the correct spatial covariance. Values are drawn from each valid conditional distribution through \gls{MCS} for a set number of iterations, $j$:
\begin{align}
    \tilde{y}_{m, i}^{(j)}       & = r_{\in[0,1]} \times \sigma_{m,i} + \mu_{m,i} \ \ \forall M \\
    \tilde{\mathbf{y}}_{i}^{(j)} & = \{ y_{1, i}^{(j)}, y_{2, i}^{(j)}, \dots, y_{M, i}^{(j)}\}
\end{align}

Where $r_{\in[0,1]}$ is a uniform random number between 0 and 1. Simulated $\tilde{y}_{m, i}^{(j)}$ values are practically constrained $\in[-5, 5]$. The trial observed value is calculated by mapping $\mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i})$. Coarse imputation error, $e_{1}$, is the absolute difference between the trial value, $\tilde{z}_{i}^{(j)}$, and the true observed data value:
\begin{align}
    \tilde{z}_{i}^{(j)} & = \mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i}^{(j)}) \\
    e_{1}               & = |\tilde{z}_{i}^{(j)} - z_{i}|
\end{align}
\begin{equation}
    \tilde{\mathbf{y}}_{i} =
    \begin{cases}
        \tilde{\mathbf{y}}_{i}^{(j)},   & \text{ if }e_{1} < t_{1} \\
        \tilde{\mathbf{y}}_{i}^{(j-1)}, & \text{ otherwise }
    \end{cases}
\end{equation}

If the error, $e_{1}$, is less than the first rejection tolerance, $t_{1}$, the trial vector $\tilde{\mathbf{y}}_{i}^{(j)}$ is retained as the initial latent vector at the $i^{th}$ location, otherwise it is rejected. If the coarse search fails to converge after the specified number of iterations, the sample $\tilde{y}_{m, i}^{(j)}$ is flagged for resimulation. The values of both $\tilde{y}_{m, i}^{(j)}$ and a small neighbourhood of nearest samples are reset. Resimulating the nearest neighbours prevents situations where local conditioning leads to non-convergence. This scenario is possible with adjacent, opposite, extreme values in a drillhole, such as at a vein's or high-grade structure's boundary. Resimulation uses a default of 10 nearest neighbours. Practice shows that $\approx 0.1$ is a reasonable rejection tolerance for $t_{1}$. Exact reproduction with the observed value is not expected during this step as the multivariate relationship between $M$ collocated factors has not been addressed. Any exact matching now would be disturbed in subsequent polishing steps.


\subsection{Solution Polishing}
\label{subsec:05polish}

Once an initial coarse solution is accepted, it can be refined to meet the collocated multivariate requirements. The imputed vector generated in the coarse search ensures $Y_{m}$ reproduces $\gamma_{m}(\mathbf{h})$ however $\mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i})$ must match $z_{i} \pm t_{2}$, where $t_{2}$ is the polishing tolerance. Solution polishing at the $i^{th}$ location begins by assessing the sensitivity of each component of  $\tilde{\mathbf{y}}_{i}^{(j)}$ on the target observed value $z_{i}$. Due to the nature of the learned mapping function, it is not immediately clear how each latent factor influences the observed value. $\tilde{y}_{m, i}^{(j)}$ can either be positively or negatively correlated with $\tilde{z}_{i}$ and the magnitude of the sensitivity depends on $\mathcal{F}_{\theta}$. To assess the sensitivity of each factor, a step size of $\Delta_{y} = G^{-1}(0.525)-G^{-1}(0.500)$, or 2.5\% in Gaussian probability space, is chosen. Perturbing a component, $m$, of $\tilde{\mathbf{y}}^{(j)}_{i}$ by $\pm \Delta_{y}$ results in two trial vectors $\tilde{\mathbf{y}}_{m+, i}^{(j)}$, and $\tilde{\mathbf{y}}_{m-,i}^{(j)}$. The trial vectors permit calculation of sensitivity as  $\Delta\tilde{z}_{m+,i} = \tilde{z}_{i} - \mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{m+, i}^{(j)})$ and $\Delta\tilde{z}_{m-,i} = \tilde{z}_{i} - \mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{m-,i}^{(j)})$. Iterating over $m=1,\dots,M$ components establishes the sensitivity and direction of change of $\tilde{z}_{i}$ with respect to $\tilde{y}_{m, i}^{(j)}$. Figure \ref{fig:tornado} shows this sensitivity graphically as a ``tornado'' chart.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=1.0, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/tornado.png}
    \caption{Schematic representation of latent factor sensitivity as a tornado chart. The x-axis shows $\Delta \tilde{z}$, or how much $\tilde{z}$ changes for a given perturbation of $Y_{m}$. In this scenario, the target delta is a positive value. The y-axis shows $m=1,\dots,M$ latent factors sorted by descending sensitivity. $a = \Delta z_{t} - c$; $b=\Delta z_{t}$; $c=$ the largest possible change to $\tilde{z}$.}
    \label{fig:tornado}
\end{figure}

Factors are sorted from most to least sensitive. $\Delta z_{t}$ is the target delta, or the difference between $z_{i}$ and $\tilde{z}_{i}$. $\Delta z_{t}$ tells us which direction to move, and the tornado bars tell us which factor(s) are sensitive enough to achieve the target delta. $a$ represents the delta between the most sensitive factor and the target delta, $b$ is equal to the target delta, and $c$ is equal to the largest possible change in $\tilde{z}_{i}$ given the current vector $\tilde{\mathbf{y}}_{i}^{(j)}$. Suppose the target delta lies within one or more of the sensitivity bars, as shown in Figure \ref{fig:tornado}. In that case, the algorithm adjusts the least sensitive factor to solve $\mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i}^{(j)}) = z_{i} \pm t_{2}$ by performing a binary search \citep{nowak2008generalized} on the interval $\tilde{y}_{m, i}^{(j)} \pm \Delta_{y}$. The least sensitive factor is selected to have the smallest possible impact on the initial vector from the coarse search. Suppose the target delta lies outside the maximum sensitivity of any factor. In that case, the most sensitive factor is perturbed to the maximum amount, and the algorithm reassesses the sensitivity of all latent factors. This perturbing and sensitivity assessment loop repeats until a binary search can solve the problem. The benefit of this approach is the latent vector $\tilde{\mathbf{y}}_{i}^{(j)}$ is always being perturbed in the correct direction, and $\Delta z_{t}$ is always approaching zero. Given a sufficient number of iterations, solution polishing must converge. That being said, there may be situations where the required amount of polishing negatively influences the covariance structure imposed in the coarse search. If at any point $|\tilde{z}_{i}^{(j)} - z_{i}| < t_{2}$ the loop breaks as $\mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i}^{(j)}) \approx z_{i}$. Practice shows that $\approx 0.01$ is a reasonable tolerance for $t_2$.

Assessing latent sensitivity permits the calculation of a measure of ``difficulty'' in the polishing phase. Consider the equality:
\begin{equation}
    \frac{a}{b} = \frac{\Delta z_{t} - c}{\Delta z_{t}}
    \label{eq:aoverb}
\end{equation}

If the target delta falls within the sensitivity of one or more factors, $\frac{a}{b}<0$, and the correct vector can be solved with binary search. In this scenario, solution polishing converges quickly. If the target delta falls outside the sensitivity of all factors, $\frac{a}{b}>0$, factor sensitivity must be repeatedly assessed, and convergence takes additional iterations. This ratio provides insight into locations or observed samples that are more difficult to impute. However, it is possible there are locations that are difficult to impute but require little polishing. Consider a location where resimulation is necessary for the coarse search, but the algorithm eventually generates a latent vector where $e_{1} \approx t_{2}$. This scenario is still difficult to impute, though $\frac{a}{b}$ does not reflect this. When evaluating problematic locations, the number of resimulations at each data location should also be considered. Data locations persistently challenging to impute across realization are subject to further investigation. Algorithm \ref{alg:sgri} summarizes the complete pseudocode for the \gls{SGRI} algorithm.

\begin{algorithm}
    \caption{SGRI pseudo code.}\label{alg:sgri}
    \begin{algorithmic}[1]
        \State initialize search parameters based on covariance
        \For{$\ell = 1, \dots, L$} \Comment{Main loop over realizations}
        \State establish a random or semi-random path through data
        \State $\text{nresim} = 0$
        \For{$i = 1, \dots, ndata$} \Comment{Loop over data locations}
        \For{$m = 1, \dots, M$} \Comment{Loop over factors at $i^{th}$ location}
        \State establish search neighbourhood $N_{m,i}$
        \State calculate conditional moments $\mu_{m,i}$, $\sigma_{m,i}^{2}$
        \EndFor \Comment{End loop over factors}
        \State $j = 0$
        \While{$e_{1} < t_{1}$} \Comment{Start coarse search}
        \State $j = j + 1$
        \For{$m = 1, \dots, M$}
        \State $y_{m, i} = r_{\in[0,1]} \times \sigma_{m,i} + \mu_{m,i}$ \Comment{Monte Carlo simulation}
        \EndFor
        \State $\tilde{z}_{i} = \mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i}^{{(j)}})$
        \State $e_{1} = |\tilde{z}_{i}^{(j)} - z_{i}|$ \Comment{Compare to observed value}
        \If{$j > iter1$}
        \State break
        \EndIf
        \EndWhile \Comment{End coarse search}
        \If{$e_{1} > t_{1}$}
        \State $\text{nresim} = \text{nresim} + 1$
        \State cycle data loop
        \EndIf
        \State $j = 0$
        \State $e_{2} = e_{1}$
        \While{$e_{2} < t_{2}$} \Comment{Start solution polishing}
        \State $j = j + 1$
        \State assess latent sensitivity
        \If{$\frac{a}{b} < 0$}
        \State solve $\tilde{\mathbf{y}}_{i}^{(j)}$ with binary search
        \Else
        \State set most sensitive factor to its bound
        \State reassess latent sensitivity
        \EndIf
        \State $\tilde{z}_{i} = \mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i}^{(j)})$
        \State $e_{2} = |\tilde{z}_{i}^{(j)} - z_{i}|$ \Comment{Retain new absolute difference}
        \If{$j > iter2$}
        \State break
        \EndIf
        \EndWhile \Comment{End solution polishing}
        \EndFor \Comment{End loop over data locations}
        \If{$\text{nresim} > 0$}
        \State resimulate at nresim locations
        \EndIf
        \EndFor \Comment{End loop over realizations}
    \end{algorithmic}
\end{algorithm}


\FloatBarrier
\subsection{Imputation Checks}
\label{subsec:05impcheck}

There are any number of latent vectors $\tilde{\mathbf{y}}_{i}$ that can reproduce the observed value $z_{i}$. As the solution is highly non-unique, multiple imputed realizations of the latent factors should be considered. Considering a multiple imputation framework transfers the uncertainty in the latent variables to subsequent model realizations \citep{silva2017multiple}. \Gls{SGRI} imputes realizations of latent variables with the correct spatial structure, are standard normal, and are independent. The minimum acceptance criteria of the imputed values in practice are:

\begin{enumerate}[noitemsep]
    \item Mapping the latent vector $\tilde{\mathbf{y}}_{i}$ (red in Figure \ref{fig:impute_schematic}) to observed space via $\mathcal{F}_{\theta}$  reproduces the observed value $z_{i} \pm t_{2}$ (blue in Figure \ref{fig:impute_schematic}).
    \item Latent factors are standard normal: $E\left\{Y_{m}\right\}=0$, \ $E\left\{Y_{m}^{2}\right\}=1$
    \item Latent factors are independent: $E\left\{Y_{m,i}Y_{n,j}\right\}=0, \ \ \forall \ m\neq n, \ \forall \ i \neq j$
    \item Latent factors reproduce their respective variogram model $\gamma_m(\mathbf{h}), \ \forall \ \mathbf{h}$
\end{enumerate}

Consider the same small synthetic example from Chapter \ref{ch:04implement}; the problem involves imputing three latent factors at 746 data locations. Factor three is the nugget effect. The \gls{SGRI} algorithm is run using a search neighbourhood of 40 nodes, a maximum of 20,000 and 10,000 iterations for the coarse search and polishing steps, respectively, and rejection tolerances of 0.1 and 0.01 for the coarse search and polishing steps, respectively. Figure \ref{fig:fact_scatter_marginals} shows a scatter plot matrix between all imputed factors, the mapped values in observed space, $\tilde{z}$, and the true data values, $z$ for a single realization. As expected, the correlation between $z$ and $\tilde{z}$ is 1.0. The top row of the matrix in Figure \ref{fig:fact_scatter_marginals} is somewhat redundant due to this perfect correlation; however, it highlights the exact data matching and validates item (1). The marginal distributions (histograms in Figure \ref{fig:fact_scatter_marginals}) are all standard normal validating item (2). Deviation from the standard normal distribution is possible, and one must consider the variogram range relative to the domain size. Long-range variogram structures may generate low-variance imputed distributions. Imputed latent factors are uncorrelated with roughly concentric density contours and $\lvert \rho \rvert < 0.10$ validating item (3). The relationships between factors 1 and 2 with the mapped values highlight the influence of the $\omega$ parameter. Factor 1 is correlated with the mapped value when it is low, and factor 2 is correlated when it is high. Figure \ref{fig:factor_varios} shows variogram reproduction for each imputed factor. There is no nugget effect variogram model, so the reproduction is not shown. The red-shaded area highlights uncertainty in the variogram across all imputed realizations. The expected imputed variograms reproduce the single structure variogram models reasonably well for 746 data.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/fact_scatter_marginals.png}
    \caption{Scatter plot matrix for a single imputed realization highlighting the perfect correlation between $z$ and $\tilde{z}$, the uncorrelated nature of all latent factors, and the standard normal nature of all marginal distributions. These features are the first three minimum acceptance criteria for \gls{SGRI} imputed realizations.}
    \label{fig:fact_scatter_marginals}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/factor_varios.png}
    \caption{Latent factor variogram reproduction for the first two factors in the major (left), minor (center) and vertical (right) directions, respectively. The expected variograms of the imputed realizations reproduce the models well, given the data density.}
    \label{fig:factor_varios}
\end{figure}

Beyond the minimum acceptance criteria, one should investigate data locations requiring multiple resimulations. Samples at transitions between extreme grade ranges or local neighbourhood outliers may require multiple simulation passes for $\tilde{\mathbf{y}}_{i}$ to converge. Table \ref{tab:nresim} shows six adjacent samples from a single drillhole, where $z$ is the observed value, $\tilde{z}$ is the mapped standard normal value, Factors 1-3 are the imputed latent Gaussian values, $\frac{a}{b}$ is the ratio in Equation \ref{eq:aoverb}, and nresim is the number of resimulations at the given data location. The third sample represents a transition from 0.563 to -1.306 or approximately the $71^{st}$ to the $9^{th}$ quantile of the Gaussian distribution. This short-scale, extreme change is a challenge for the \gls{SGRI} algorithm, though it can converge with a sufficient number of rejection iterations. It is noteworthy that $\frac{a}{b} < 0$, or the coarse search was able to produce a latent vector $\tilde{\mathbf{y}}_{i}^{(j)}$ where minimal solution polishing is required.

\begin{table}[!htb]
    \centering
    \caption{Six adjacent samples from a single drillhole, where $z$ is the observed value, $\tilde{z}$ is the mapped standard normal value, Factors 1-3 are the imputed latent Gaussian values, $\frac{a}{b}$ is the ratio in Equation \ref{eq:aoverb} and nresim is the number of resimulations at the given data location. The polishing tolerance is 0.01. }
    \resizebox{0.9\width}{!}{\input{0-Tables/nresim.tex}}
    \label{tab:nresim}
\end{table}

If solution polishing does not converge after a reasonable number of iterations, the algorithm can draw a valid Gaussian from the internal \gls{SGRI} reference distribution. $\tilde{\mathbf{y}}_{i}^{(jmax)}$ is selected from the ``lookup table'' such that $|\tilde{z}_{i}^{(jmax)} - z_{i}|$ is minimized. A reference distribution with $10^{6}$ entries is sufficiently large to contain a latent vector that satisfies the polishing tolerance when mapped to observed space. Frequent use of a lookup table may affect multivariate and spatial properties of the latent factors, so \gls{SGRI} issues a warning if more than 1\% of the imputed samples are drawn from the reference distribution.

\FloatBarrier
\section{Latent Factor Simulation}
\label{sec:05latentsim}

\gls{NMR} realizations are generated by conditionally simulating latent factors on a grid and then mapping the gridded factors to observed space. Latent factors imputed at the data locations become the conditioning data for any conditional simulation algorithm. Standard conditional simulation algorithms include \gls{SGS} \citep{gomez-hernandez1993joint}, turning-bands \citep{journel1974geostatistics} and LU simulation \citep{davis1987production}. \gls{SGS} is likely the most commonly implemented algorithm due to its simplicity and availability in commercial software \citep{rossi2013mineral}, though any conditional algorithm is valid. Figure \ref{fig:gridded_factors_real0_xy} shows plan view sections through the first conditional realization of the three gridded factors.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/gridded_factors_real0_xy.png}
    \caption{Plan view sections through the first conditional realization of the three gridded factors with imputed conditioning data.}
    \label{fig:gridded_factors_real0_xy}
\end{figure}

Once the latent factors are defined at every grid node, the realizations are mapped to the observed space through $\mathcal{F}_{\theta}$. Figure \ref{fig:nmr_ns_real} shows sections through the first \gls{NMR} realization mapped to Gaussian units. The non-Gaussian characteristics of the realizations are evident in these sections. The localized high-grade values clearly overprint the low-grade, longer-range background values defined by the Gaussian pool and $\omega$ constraints. These features are also evident in the back-transformed realizations in Figure \ref{fig:nmr_au_real}. The realizations show localized but internally connected high-grade regions that correspond to the covariance structure of the 0.9 quantile indicator variogram.

\begin{figure}
    \centering
    \tabskip=0pt
    \valign{#\cr
        \hbox{%
            \begin{subfigure}[b]{.50\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./0-Figures/05-Ch5/nmr_real0_xy.png}
                \caption{}
            \end{subfigure}%
        }\cr
        \noalign{\hfill}
        \hbox{%
            \begin{subfigure}{.50\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./0-Figures/05-Ch5/nmr_real0_xz.png}
                \caption{}
            \end{subfigure}%
        }\vfill
        \hbox{%
            \begin{subfigure}{.50\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./0-Figures/05-Ch5/nmr_real0_yz.png}
                \caption{}
            \end{subfigure}%
        }\cr
    }
    \caption{Plan (a), east-west (b) and north-south (c) sections through the first \gls{NMR} realization mapped to Gaussian space with observed data values in Gaussian units.}
    \label{fig:nmr_ns_real}
\end{figure}

\begin{figure}[!htb]
    \centering
    \tabskip=0pt
    \valign{#\cr
        \hbox{%
            \begin{subfigure}[b]{.50\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./0-Figures/05-Ch5/nmr_au_real0_xy.png}
                \caption{}
            \end{subfigure}%
        }\cr
        \noalign{\hfill}
        \hbox{%
            \begin{subfigure}{.50\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./0-Figures/05-Ch5/nmr_au_real0_xz.png}
                \caption{}
            \end{subfigure}%
        }\vfill
        \hbox{%
            \begin{subfigure}{.50\textwidth}
                \centering
                \includegraphics[width=\textwidth]{./0-Figures/05-Ch5/nmr_au_real0_yz.png}
                \caption{}
            \end{subfigure}%
        }\cr
    }
    \caption{Plan (a), east-west (b) and north-south (c) sections through the first \gls{NMR} realization back-transformed to original units.}
    \label{fig:nmr_au_real}
\end{figure}



At this point, the gridded \gls{NMR} realizations should reproduce the continuous variogram model, specified indicator variogram models, cumulative run frequencies and $n$-point connectivity functions. As the gridded models reproduce the data at the data locations (particularly implementations of \gls{SGS} that assign data to grid nodes), an additional step is required to check downhole statistics. For checking runs and $n$-point connectivity reproduction, the entire drillhole configuration is translated within the domain. This translation retains the drillhole configuration but allows sampling simulated values from the grid. The resampled values are then used to check multi-point statistical reproduction.

\FloatBarrier
\subsection{Simulation Checks}
\label{subsec:05simcheck}

The minimum acceptance criteria for checking continuous realizations \citep{leuangthong2004minimum} must be applied, similar to any other geostatistical model. The gridded realizations must reproduce (1) the data values at data locations, (2) the declustered input \gls{CDF} and summary statistics for the latent Gaussian pool and the original variable, and (3) the input covariance model, both for the latent Gaussian pool and the original variable.

Figure \ref{fig:repro_gridded_data_ch5} shows \gls{CDF} reproduction (left) and data reproduction (center) for the gridded \gls{NMR} realizations in Gaussian units. The Gaussian realizations are, on average, standard normal, and the data values are reproduced exactly at collocated grid nodes. In some locations, more than one data value occupies a single grid node, and there is a deviation from the 1:1 line in the scatter plot. In this scenario, the data value closest to the grid node centroid is retained for comparison. Figure \ref{fig:repro_gridded_data_ch5} (right) shows \gls{CDF} reproduction of the gridded realizations back-transformed to original units. The realizations, on average, reproduce the declustered \gls{CDF}. The final component of the traditional model checks is checking continuous variogram reproduction. Figure \ref{fig:repro_gridded_vario_ch5} shows gridded continuous variogram reproduction for the \gls{NMR} realizations in Gaussian units. As the realizations are generated with a mapping function that considers multiple variogram components, there are some deviations from the input continuous model. In this example, the 0.1 and 0.9 quantile indicator variograms influence the continuous variogram reproduction in the major and minor directions. The 0.9 indicator variogram introduces a slight increase in variance in the short-range lags and a slight decrease in the longer-range lags from the 0.1 indicator variogram. Continuous variogram reproduction remains reasonable, though the influence of other objective components is evident in the covariance structure of the final gridded models.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/repro_gridded_data.png}
    \caption{\Gls{CDF} and data reproduction for the \gls{NMR} realizations in Gaussian units (left and center) and \gls{CDF} reproduction in original units (right). The original units reference distribution shown in red considers declustering weights.}
    \label{fig:repro_gridded_data_ch5}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/repro_gridded_vario.png}
    \caption{\gls{NMR} continuous variogram reproduction. The black line is the variogram model; the red dots are the experimental variogram of the imputed data; the red line is the average variogram of the gridded \gls{NMR} realizations, the shaded red area encloses the minimum and maximum gridded \gls{NMR} variogram values. Left to right are the major, minor and vertical directions, respectively.}
    \label{fig:repro_gridded_vario_ch5}
\end{figure}

Beyond the traditional simulation model checks, the \gls{NMR} realizations must be checked to reproduce all the objective function components. These checks include indicator variograms and sequences. Figure \ref{fig:repro_gridded_ivario_ch5} shows gridded reproduction of the strongly asymmetric 0.1 and 0.9 quantile indicator variograms. The gridded models closely reproduce the non-Gaussian indicator structure that is apparent in Figures \ref{fig:nmr_ns_real} and \ref{fig:nmr_au_real}, though there is some deviation at shorter lags. This deviation is attributed to shorter range lags having few pairs given the data configuration. In a mining context, one is predominantly concerned with high values, and in this example, cumulative run frequencies are considered above the 0.9 quantile threshold. When considering sequences above a threshold, the $n$-point connectivity function and cumulative run-length frequencies are analogous; one can be calculated from the other. Only cumulative run-length frequencies are considered for this reason. The chosen simulation algorithm may assign the data values to the grid, resulting in a distribution of runs with zero uncertainty. To overcome this, the entire drillhole configuration is translated within the domain and values are sampled from the grid at the ``new'' data locations. Four translations are performed (25 meters to the NE, SE, SW and NW), and the final result is the expected value across translations. Figure \ref{fig:repro_gridded_runs_ch5} shows gridded cumulative run-length frequencies for the 0.9 quantile indicator. The solid red line is the expected value across translations. The black line is the target value calculated from the drillholes. The expected value deviates slightly from the target, though the uncertainty bandwidth captures the target.

\begin{figure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/repro_gridded_ivario_0_1.png}
        \caption{0.1 Quantile}
    \end{subfigure}
    \begin{subfigure}{1.0\textwidth}
        \centering
        \includegraphics[scale=0.60, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/repro_gridded_ivario_0_9.png}
        \caption{0.9 Quantile}
    \end{subfigure}
    \caption{\gls{NMR} indicator variogram reproduction for the 0.1 (a), 0.5 (b) and 0.9 (c) quantiles. The black line is the variogram model; the red dots are the experimental variogram of the imputed data; the red line is the average variogram of the gridded \gls{NMR} realizations, the shaded red area encloses the minimum and maximum gridded \gls{NMR} variogram values. Left to right are the major, minor and vertical directions, respectively.}
    \label{fig:repro_gridded_ivario_ch5}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/repro_gridded_runs.png}
    \caption{\gls{NMR} 0.9 quantile cumulative run-length frequency reproduction. The black line is the target calculated from the drillholes, the red line is the average gridded value, and the shaded red area encloses the minimum and maximum gridded values. }
    \label{fig:repro_gridded_runs_ch5}
\end{figure}

Suppose the \gls{NMR} realizations meet the minimum acceptance criteria. In that case, the realizations are (1) univariate Gaussian, (2) reproduce the observed data at the data locations, (3) reproduce the continuous variogram model, (4) reproduce the indicator variogram models for chosen thresholds and (5) reproduce chosen multi-point measures of connectivity. Points (4) and (5) ensure the realizations are not multivariate Gaussian.

\FloatBarrier
\section{Discussion}
\label{sec:05discuss}

\Gls{SGRI} is a novel algorithm for imputing latent Gaussian factors for use in the \gls{NMR} simulation framework. \Gls{SGRI} is an alternative to the Gibbs sampler approach and permits the imputation of any number of independent, standard normal Gaussian vectors with the correct spatial structure. When mapped from latent to observed space, the imputed factors reproduce the observed data value. The mapping function $\mathcal{F}_{\theta}$ is highly flexible and incorporates components of the conceptual geological model and features embedded in the design of the Gaussian pool. The algorithm is straightforward and incorporates elements of \gls{SGS} and rejection sampling. Simple kriging calculates the moments of local conditional \glspl{CDF}, and values are drawn with \gls{MCS} subject to rejection. The rejection step ensures that the imputed vector remains within a feasible solution space. The algorithm shows stable convergence in both constrained and unconstrained forms, though heavy constraints may lead to departure from the standard normal distribution. \Gls{SGRI} implements optional constraints by seeding some number of imputation locations with values above or below a threshold, ensuring the imputed values are extreme where observed values are extreme. The constraints are enforced through rejection where $\mathbf{y}_{p}$ values must be above or below the threshold.

\Gls{SGRI} is the second component of the \gls{NMR} suite of programs. \texttt{NMROPT} and \texttt{NMRIMP} form the complete \gls{NMR} framework for simulation of non-Gaussian spatial features. Chapters \ref{ch:04implement} and \ref{ch:05impute} present a synthetic example of the \gls{NMR} workflow highlighting all aspects of latent factor design, parameter inference, factor imputation and simulation, factor mapping and checking \gls{NMR} realizations. The following chapter takes these components to a real \gls{3D} dataset where the connectivity of extreme values is a defining characteristic of the data.