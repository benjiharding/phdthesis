%!TEX root = ../Thesis_example_compile_this.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Latent Factor Imputation}
\label{ch:impute}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter presents a novel algorithm for imputing latent factors called \gls{SGRI}. Imputation of latent factors implies a problem where all values are missing or unknown. Conditioning gridded realizations of latent factors requires assigning valid latent values at the data locations. The goal is to impute latent factors with a unique covariance structure that, when combined with inferred \gls{NMR} mapping, return the true observed values at the data locations within a tolerance. Traditionally, this problem has been approached through Gibbs sampling, though there are noted challenges in achieving stable convergence with spatially correlated variables. \gls{SGRI} is a simple approach that combines \gls{SGS}, rejection sampling and exact data matching. Conditional moments are calculated with \gls{SGS}, ensuring the spatial relationships are correct and iterative rejection sampling ensures the collocated multivariate relationships are correct. Realizations of latent factors are generated and become conditioning data for gridded realizations. When mapped to observed space, the realizations reproduce non-Gaussian spatial features specified by the \gls{NMR} as well as uni- and multivariate statistics. A small synthetic example demonstrates imputing latent factors, conditional simulation and mapping the latent space to observed space.

\FloatBarrier
\section{Imputation Concepts}
\label{sec:impute}

Imputation is a method to ``fill in'' missing values \citep{little2019statistical}. Missing values may be uni- or multivariate and multiple mechanisms or patterns of missingness are possible. Multivariate transformations, common in modern geostatistical workflows, such as \gls{PPMT} \citep{barnett2014projection} require homotopic sampling necessitating imputation methods. Ignoring these missing values results in a loss of information or potential bias if the missingness mechanism is not random. The simplest deterministic approach to imputation is taking the global mean or median of sampled values or employing a regression model. This inference permits the use of all data in subsequent modeling however no uncertainty in the imputed values is captured.

Single imputation involves imputing a single value for each missing data value. \cite{little2019statistical} describe single imputation techniques as (1) mean imputation where the global mean value is substituted; (2) regression imputation where missing values are replaced by predicted value from a regression of missing variable on the observed variables; and (3) stochastic regression imputation where missing values are replaced by a predicted value plus a residual. The single imputation paradigm amounts to saying the missing values are certain or constant. Though all data may be included in statistical analysis, this assumption results in incorrect uncertainty as the true values are unknown. Furthermore, data sets imputed with mean or regression imputation will not have the correct mean or variance \citep{barnett2015multivariate}. For these reasons multiple imputation techniques are preferred when characterizing imputation uncertainty is important.

Multiple imputation involves generating realizations of missing values such that imputation uncertainty can be assessed. A model of the conditional distribution of the missing values given the observed values is inferred and then stochastically sampled resulting in complete data set realizations. Multiple imputation in a geostatistical context is often a constrained problem where the imputed values must (1) reproduce underlying multivariate relationships and (2) reproduce the spatial variability of observed values \citep{barnett2015multivariate}. Collocated variables characterize the multivariate relationships and the covariance structure of the observed variables characterize the spatial variability. The conditional distribution from which the imputed values are drawn is informed by these components \citep{hadavand2023spatial}.

A \textit{latent variable} is a variable that is not directly observed but is assumed related to, and can be inferred from, measured or observed variables \citep{everitt2010cambridge}. Imputation of latent factors is a unique imputation problem where all variables are missing or unsampled \citep{little2019statistical}. The latent factors are not directly observed; they are a synthetic feature of the inferred mathematical model. An example is the \gls{LMR}, which is composed of multiple latent independent random factors operating at different scales \citep{goovaerts1992factorial}. The latent factors are never measured or directly observed but characterize the observed regionalized random variable $Z(\mathbf{u})$. Simulating geologic latent variables subject to other observations or constraints is key component of truncated-Gaussian categorical simulation techniques. The techniques in this chapter do not directly consider categorical values as constraints, though they enforce reproduction of the true continuous values. Latent variable imputation is most commonly performed with a Gibbs Sampler \citep{emery2014simulating,silva2017multiple,arroyo2020iterative,madani2021enhanced} or alternatively the sequential spectral turning bands method \citep{lauzon2020calibration,lauzon2020sequential,lauzon2023joint}. The Gibbs sampler is a Markov chain Monte Carlo method used to sample a multivariate distribution where direct sampling is complex but sampling marginal distributions is simple. The Gibbs sampler is practical for indirectly sampling high-dimensional distributions using univariate conditional distributions, though convergence of the algorithm is a known issue with correlated variables \citep{silva2018enhanced}. Many data necessitate the use of a restricted kriging search which may cause the simulated Gaussian vector to deviate from the desired covariance matrix \citep{emery2014simulating}.


\FloatBarrier
\section{Gibbs Sampler}
\label{sec:gibbs}

The Gibbs sampler \citep{geman1984stochastic} is an iterative simulation algorithm designed to sample an $M$-dimensional multivariate distribution $f(y_{1}, \dots, y_{M})$ of $M$ random variables $\{Y_{1}, \dots, Y_{M}\}$ \citep{little2019statistical}. It is particularly useful where sampling the joint distribution is difficult, but sampling the marginal conditional distributions $f(y_{m}|y_{1},\dots,y_{m-1},y_{m+1},\dots,y_{M}), \ \ m=1,\dots,M$. The Gibbs sampler is summarized in the following general steps:

\begin{enumerate}[noitemsep]
    \item initialize counter $t=0$
    \item initialize a valid arbitrary vector $y^{(0)}$
    \item for each dimension $m=1,\dots,M$:
          \begin{enumerate}[noitemsep]
              \item $t=t+1$
              \item set $y^{(t)}_{j} = y^{(t-1)}_{j} \ \ \forall j \neq m$
              \item draw $y^{(t)}_{m}$ from  $f(y_{m}|y_{1},\dots,y_{m-1},y_{m+1},\dots,y_{M})$
              \item finish if $t=t_{MAX}$
          \end{enumerate}
    \item return $y^{(t)}$
\end{enumerate}

Over a sufficient number of iterations the sampled vector converges on the joint distribution. In a geostatistical context the Gibbs sampler is based on the fact the distribution of a Gaussian vector $Y$ conditioned on other values is Gaussian; the mean and variance of this distribution is calculated by simple kriging \citep{emery2014simulating}. The Gibbs sampler simulates both conditional and unconditional vectors. Conditional Gibbs simulation is commonly used for latent variable assignment in truncated-Gaussian techniques \citep{armstrong2011plurigaussian,silva2017multiple} where latent values must respect both the mapping between categorical and continuous space and match the categorical observations when truncated. Imputation of latent factors in the context of this work does not require satisfying inequality constraints at the data locations but does require satisfying the mapping condition between observed and latent space.

\cite{silva2018enhanced} documents Gibbs sampler convergence issues related to restricting the search neighbourhood for updating the marginal conditional distributions. Considering all $n$ data requires the inversion of a rank $n-1$ covariance matrix which becomes unpractical with increasing $n$. Restricting the search to reduce the size of the covariance results in an approximation of the conditional moments and affects algorithm convergence \citep{emery2014simulating, lauzon2020sequential} as realization quantiles diverge to extreme highs and lows with increasing iterations. \cite{lantuejoul2012simulation} proposed the propagative Gibbs sampler to avoid the matrix inversion requirement though \cite{silva2018enhanced} shows convergence issues are still present with greater than two latent variables and complex truncation rules. These convergence challenges motivate the development of a new simulation algorithm for imputation of latent geologic variables. The \gls{SGRI} algorithm also utilizes a restricted search neighbourhood for calculation of conditional moments though the relaxed constraints relative to imputation for categorical modeling allows for stable convergence.

\FloatBarrier
\section{Rejection Sampling}
\label{sec:reject}


\FloatBarrier
\section{Sequential Gaussian Rejection Imputation}
\label{sec:sgri}

Generating Gaussian \gls{NMR} realizations requires inference of the latent factors at the data locations. These imputed factors become conditioning data for simulation on a regular grid. The problem involves assigning $M$ unknown latent Gaussian values with the correct spatial structure at the data locations. When combined through the \gls{NMR}, these imputed values must also reproduce the observed regionalized random variable $Z(\mathbf{u})$. This solution is non-unique, and multiple combinations of latent factors could reproduce the observed values. In order to correctly transfer this latent uncertainty, a multiple imputation \citep{barnett2015multivariate} approach is adopted where each realization of $Z(\mathbf{u})$ is generated with a unique imputed realization, $\mathbf{y}$.

\Gls{SGRI} is an iterative simulation algorithm for imputing continuous latent Gaussian variables subject to a mapping constraint $\mathcal{F}_{\theta}\left(\mathbf{y}\right)=\mathbf{z} \pm t_{d}$, where $t_{d}$ is a data matching tolerance. \Gls{SGRI} first iteratively samples all univariate conditional distributions of the $M$-dimensional latent distribution until a coarse tolerance with the observed value is met. After the simulation meets the coarse tolerance, a polishing step iteratively perturbs each initial imputed value until a fine tolerance with the observed value is met. At any point during the perturbation, the sample is rejected if the new value does not decrease the error between observed and imputed. Initial sampling of the conditional distributions ensures the correct covariance structure for each latent factor, and iterative polishing ensures data reproduction. Though \cite{armstrong2011plurigaussian} suggests rejection sampling is not a feasible approach for latent imputation, the relaxed constraints relative to imputation for truncated-Gaussian techniques allows for stable convergence of the algorithm.

The \gls{SGRI} algorithm is a straightforward approach to imputation where both spatial and collocated multivariate relationships must be respected. Latent factors are imputed such that when mapped through the \gls{NMR}, the observed values at some location $\mathbf{u}_{i}$ are reproduced exactly, and each latent factor reproduces its covariance structure. Figure \ref{fig:impute_schematic} shows this relationship schematically. A sketch of a drillhole is shown with $i=1,\dots,n$ observed data locations. The latent space consists of $m=1,\dots,M$ factors to be imputed at each observed location. The observed values are a function of the mapped latent factors. It is straightforward to generate independent Gaussian values reproducing observed values; however, latent spatial continuity must also be correct. $F_{m}=1,\dots,M$ \glspl{CDF} must be standard normal and the spatial distribution must reproduce variograms $\gamma_{m}, \ m=1,\dots,M$.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=1.0, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/impute_schematic.png}
    \caption{A schematic drillhole showing the multivariate relationships between observed and latent spaces. The observed data values are a function $\mathcal{F}_{\theta}\left(\mathbf{y}\right)$ of the imputed latent factors at location $\mathbf{u}_{i}$. The imputed values must reproduce the true observed values, and each regionalized latent factor must reproduce its covariance structure $\gamma_{m}$. }
    \label{fig:impute_schematic}
\end{figure}

The three general steps of the \gls{SGRI} algorithm are as follows:
\begin{enumerate}[noitemsep]
    \item Calculate $m=1,\dots,M$ conditional means and standard deviations at the imputation location $\mathbf{u}_{i}$.
    \item Repeatedly sample the Gaussian vector $\tilde{\mathbf{y}}(\mathbf{u}_{i}) = \{\tilde{y}_{m}(\mathbf{u}_{i}), m=1,\dots,M \}$ from the conditional distributions until $\left|\mathcal{F}_{\theta}(\tilde{\mathbf{y}}(\mathbf{u}_{i})) - z(\mathbf{u}_{i})\right|$ is within a specified first tolerance.
    \item Iteratively refine the solution from (2) through stochastic perturbation until $\left|\mathcal{F}_{\theta}(\tilde{\mathbf{y}}(\mathbf{u}_{i})) - z(\mathbf{u}_{i})\right|$ is within a specified second tolerance.
\end{enumerate}

The normal score transform within the latent to observed mapping function $\mathcal{F}_{\theta}$ is slightly different than in the context of network parameter inference in Chapter \ref{ch:implement}. The \gls{SGRI} algorithm constructs a reference distribution for mapping trial latent vectors, $\tilde{\mathbf{y}}_{i}$,  to a scalar value in observed space, $\tilde{z}_{i}$. The reference distribution facilitates the normal score transform of a single NMR output value. The table is constructed by mapping a $1e^{7}$ x $M$ table of standard normal independent factors through  $\mathcal{F}_{\theta}$, resulting in an exhaustive table of outputs in raw activation units. This output table is normal score transformed, and $\tilde{z}_{i}$ values are calculated by linearly interpolating the normal score array. The notation $\mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i}^{(j)})$ implies a scalar, standard normal output. In the following sections the location vector $\mathbf{u}_{i}$ is replaced by the subscript $i$ for notation simplicity.

\subsection{Conditional Moments}
\label{subsec:condmom}

The algorithm begins by determining a random path through all data locations to be imputed. At each imputation location, the conditional moments for each latent covariance structure are solved using the normal equations:
\begin{align}
     & y_{m,i} \sim {\mathcal {N}}\left( \mu_{m,i}, \sigma_{m,i} \right)                           \\
     & \mu_{m,i} = \sum_{j=1}^{N_{i}} \lambda_{j,i} \times y_{m,j}                                 \\
     & \sigma_{m,i}^{2} = 1 - \sum_{j=1}^{N_{i}} \lambda_{m,j,i} \times C_{m,j,i}                  \\
     & \sum_{j=1}^{N_{m,i}} \lambda_{m,j,i} \times C_{m,j,k} = C_{m,k,i} \ \ \forall k \in N_{m,i}
\end{align}

Where $N_{m,i}$ is the neighbourhood about location $i$ using search anisotropy derived from $Y_{m}$; this could either be all neighbours or a restricted search around location $i$. $C_{m,j,k}$ is the covariance between spatial locations $j$ and $k$ for latent variable $m$. The system of equations is solved $M$ times under the assumption of zero mean and unit variance, resulting in a vector of conditional means and standard deviations. Retaining all conditional moments allows for fast, repeated simulation of spatially correlated latent values.

\subsection{Coarse Search}
\label{subsec:coarse}

The coarse search begins once $M$ conditional moments have been calculated at location $i$. The goal of the coarse search is to find $\tilde{\mathbf{y}}_{i}$ that nearly reproduces $z_{i}$ and has the correct spatial covariance. Values are drawn from the valid conditional distributions through Monte Carlo simulation for a set number of iterations, $j$:
\begin{align}
    \tilde{y}_{m, i}^{(j)}       & = r_{\in[0,1]} \times \sigma_{m,i} + \mu_{m,i} \ \ \forall M \\
    \tilde{\mathbf{y}}_{i}^{(j)} & = \{ y_{1, i}^{(j)}, y_{2, i}^{(j)}, \dots, y_{M, i}^{(j)}\}
\end{align}

Where $r_{\in[0,1]}$ is a uniform random number between 0 and 1. Simulated $\tilde{y}_{m, i}^{(j)}$ values are practically constrained $\in[-5, 5]$. The trial value is calculated by mapping $\mathcal{F}_{\theta}(\tilde{y}_{i})$. Coarse imputation error, $d_{1}$, is the absolute difference between the trial value, $\tilde{z}_{i}^{(j)}$, and the true observed data value:
\begin{align}
    \tilde{z}_{i}^{(j)} & = \mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i}^{(j)}) \\
    d_{1}               & = |\tilde{z}_{i}^{(j)} - z_{i}|
\end{align}
\begin{equation}
    \tilde{\mathbf{y}}_{i} =
    \begin{cases}
        \tilde{\mathbf{y}}_{i}^{(j)},   & \text{ if }d_{1} < t_{1} \\
        \tilde{\mathbf{y}}_{i}^{(j-1)}, & \text{ otherwise }
    \end{cases}
\end{equation}

If the difference $d_{1}$ is less than the first rejection tolerance, $t_{1}$, the trial vector $\tilde{\mathbf{y}}_{i}^{(j)}$ is retained as the initial latent vector at the $i^{th}$ location, otherwise it is rejected. If the coarse search fails to converge after the specified number of iterations the sample $\tilde{y}_{m, i}^{(j)}$ is flagged for resimulation. The values of both $\tilde{y}_{m, i}^{(j)}$ and a small neighbourhood of nearest samples are reset. Resimulating the nearest neighbours prevents the situation where local conditioning leads to non-convergence. This scenario is possible with adjacent, opposite, extreme values in a drillhole, such as at the boundary of a vein or high-grade structure. Resimulation uses a default of 10 nearest neighbours. Practice shows that $\approx 0.1$ is a reasonable rejection tolerance for $t_{1}$. Exact reproduction with the observed value is not expected during this step as the multivariate relationship between $M$ collocated factors has not been addressed. Any exact matching now would be disturbed in subsequent polishing steps.


\subsection{Solution Polishing}
\label{subsec:polish}

Once an initial coarse solution is accepted, it can be refined to meet the collocated multivariate requirements. The imputed vector generated in the coarse search ensures $Y_{m}$ reproduces $\gamma_{m}(\mathbf{h})$ however $\mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i})$ must match $z_{i}$ exactly. Solution polishing at the $i^{th}$ location begins by assessing the sensitivity of each component of  $\tilde{\mathbf{y}}_{i}^{(j)}$ on the target observed value $z_{i}$. Due to the nature of the learned mapping function it is not immediately clear how each latent factor influences the observed value. $\tilde{y}_{m, i}^{(j)}$ can either be positively or negatively correlated with $\tilde{z}_{i}$ and the magnitude of the sensitivity depends on $\mathcal{F}_{\theta}$. To assess the sensitivity of each factor a step size of $\Delta_{y} = G^{-1}(0.525)-G^{-1}(0.500)$, or 2.5\% in Gaussian probability space, is chosen. Perturbing a component, $m$, of $\tilde{\mathbf{y}}^{(j)}_{i}$ by $\pm \Delta_{y}$ results in two trial vectors $\tilde{\mathbf{y}}_{m+, i}^{(j)}$, and $\tilde{\mathbf{y}}_{m-,i}^{(j)}$. The trial vectors permit calculation of sensitivity as  $\Delta\tilde{z}_{m+,i} = \tilde{z}_{i} - \mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{m+, i}^{(j)})$ and $\Delta\tilde{z}_{m-,i} = \tilde{z}_{i} - \mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{m-,i}^{(j)})$. Iterating over $m=1,\dots,M$ components establishes the sensitivity and direction of change of $\tilde{z}_{i}$ with respect to $\tilde{y}_{m, i}^{(j)}$. Figure \ref{fig:tornado} shows this sensitivity graphically as a ``tornado'' chart.

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=1.0, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/tornado.png}
    \caption{Schematic representation of latent factor sensitivity as a tornado chart. The x-axis shows $\Delta \tilde{z}$, or how much $\tilde{z}$ changes for a given perturbation of $Y_{m}$. In this scenario the target delta is a positive value. The y-axis shows $m=1,\dots,M$ latent factors sorted by descending sensitivity. $a = \Delta z_{t} - c$; $b=\Delta z_{t}$; $c=$ the largest possible change to $\tilde{z}$.}
    \label{fig:tornado}
\end{figure}

Factors are sorted from most to least sensitive. $\Delta z_{t}$ is the target delta, or the difference between $z_{i}$ and $\tilde{z}_{i}$. $\Delta z_{t}$ tells us which direction we need to move, and the tornado bars tell us which factor(s) is sensitive enough to achieve the target delta. $a$ represents the delta between the most sensitive factor and the target delta, $b$ is equal to the target delta, and $c$ is equal to the largest possible change in $\tilde{z}_{i}$ given the current vector $\tilde{\mathbf{y}}_{i}^{(j)}$. If the target delta lies within one or more of the sensitivity bars, as it does in Figure \ref{fig:tornado}, the algorithm adjusts the least sensitive factor to solve $\mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i}^{(j)}) = z_{i} \pm t_{2}$ by performing a binary search \citep{nowak2008generalized} on the interval $\tilde{y}_{m, i}^{(j)} \pm \Delta_{y}$, where $t_{2}$ is the polishing tolerance. The least sensitive factor is selected to have the smallest possible impact on the initial vector simulated in the coarse search. If the target delta lies outside the maximum sensitivity of any factor the most sensitive factor is perturbed the maximum amount, and the sensitivity of all latent factors is reassessed. This loop of perturbing and assessing sensitivity is repeated until the solution can be solved by binary search. The benefit of this approach is the latent vector $\tilde{\mathbf{y}}_{i}^{(j)}$ is always being perturbed in the correct direction and $\Delta z_{t}$ is always approaching zero. Given a sufficient number of iterations, solution polishing will converge. That being said, there may be situations where the required amount of polishing negatively influences the covariance structure imposed in the coarse search. If at any point $|\tilde{z}_{i}^{(j)} - z_{i}| < t_{2}$ the loop breaks as $\mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i}^{(j)}) \approx z_{i}$. Practice shows that $\approx 0.01$ is a reasonable tolerance for $t_2$.

Assessing latent sensitivity permits calculation of a measure of ``difficulty'' in the polishing phase. Consider the equality:
\begin{equation}
    \frac{a}{b} = \frac{\Delta z_{t} - c}{\Delta z_{t}}
    \label{eq:aoverb}
\end{equation}

If the target delta fall within the sensitivity of one or more factors, $\frac{a}{b}<0$, and the correct vector can be solved with binary search. In this scenario solution polishing converges quickly. If the target delta falls outside the sensitivity of all factors, $\frac{a}{b}>0$, factor sensitivity must be repeatedly assessed, and convergence is more difficult. This ratio provides insight into locations or observed samples that are more difficult to impute. Data locations that are persistently challenging to impute across realization are subject to further investigation. Algorithm \ref{alg:sgri} summarizes the complete \gls{SGRI} algorithm.

\begin{algorithm}
    \caption{SGRI pseudo code.}\label{alg:sgri}
    \begin{algorithmic}[1]
        \State read parameter file
        \State initialize random number generator
        \State initialize search parameters based on covariance
        \For{$\ell = 1, \dots, L$} \Comment{Main loop over realizations}
        \State establish a random path through data
        \For{$i = 1, \dots, ndata$} \Comment{Loop over data locations}
        \For{$m = 1, \dots, M$} \Comment{Loop over factors at $i^{th}$ location}
        \State establish search neighbourhood $N_{m,i}$
        \State calculate conditional moments $\mu_{m,i}$, $\sigma_{m,i}^{2}$
        \EndFor \Comment{End loop over factors}
        \State $j = 0$
        \While{$d_{1} < t_{1}$} \Comment{Start coarse search}
        \State $j = j + 1$
        \For{$m = 1, \dots, M$}
        \State $y_{m, i} = r_{\in[0,1]} \times \sigma_{m,i} + \mu_{m,i}$ \Comment{Monte Carlo simulation}
        \EndFor
        \State $\tilde{z}_{i} = \mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i}^{{(j)}})$
        \State $d_{1} = |\tilde{z}_{i} - z_{i}|$ \Comment{Compare to observed value}
        \If{$j > iter1$}
        \State break
        \EndIf
        \EndWhile \Comment{End coarse search}
        \State $j = 0$
        \State $d2 = d1$
        \While{$d_{2} < t_{2}$} \Comment{Start solution polishing}
        \State $j = j + 1$
        \State assess latent sensitivity
        \If{$\frac{a}{b} < 0$}
        \State solve $\tilde{\mathbf{y}}_{i}^{(j)}$ with binary search
        \Else
        \State set most sensitive factor to its bound
        \State re-assess latent sensitivity
        \EndIf
        \State $\tilde{z}_{i} = \mathcal{F}_{\theta}(\tilde{\mathbf{y}}_{i}^{(j)})$
        \If{$|\tilde{z}_{i}^{(j)} - z_{i}| < d_{2}$} \Comment{Compare to observed value}
        \State $\tilde{y}_{m,i} = \tilde{y}_{m,i}^{(j)}$ \Comment{Retain the perturbed latent value}
        \State $\tilde{z}_{i} =\tilde{z}_{i}^{(j)}$ \Comment{Retain the new observed value}
        \State $d_{2} = |\tilde{z}_{i}^{(j)} - z_{i}|$ \Comment{Retain new absolute difference}
        \EndIf
        \If{$j > iter2$}
        \State break
        \EndIf
        \EndWhile \Comment{End solution polishing}
        \EndFor \Comment{End loop over data locations}
        \EndFor \Comment{End loop over realizations}
    \end{algorithmic}
\end{algorithm}


\FloatBarrier
\section{Practical Details}
\label{sec:details}

There are any number of latent vectors $\tilde{\mathbf{y}}_{i}$ that can reproduce the observed value $z_{i}$. As the solution is highly non-unique, multiple imputed realizations of the latent factors should be considered. Considering a multiple imputation framework transfers the uncertainty in the latent variables to subsequent model realizations \citep{silva2017multiple}. \Gls{SGRI} imputes realizations of latent variables with the correct spatial structure, are standard normal and are independent. The minimum acceptance criteria of the imputed values in practice are:

\begin{enumerate}[noitemsep]
    \item Mapping the latent vector $\mathbf{y}_{i}$ (red in Figure \ref{fig:impute_schematic}) to observed space via $\mathcal{F}_{\theta}$  reproduces the observed value $z_{i} \pm t_{2}$ (blue in Figure \ref{fig:impute_schematic}).
    \item Latent factors are standard normal: $E\left\{Y_{m}\right\}=0$, \ $E\left\{Y_{m}^{2}\right\}=1$
    \item Latent factors are independent: $E\left\{Y_{m,i}Y_{n,j}\right\}=0, \ \ \forall \ m\neq n, \ \forall \ i \neq j$
    \item Latent factors reproduce their respective variogram model $\gamma_m(\mathbf{h}), \ \forall \ \mathbf{h}$
\end{enumerate}

Consider a small synthetic example imputing four latent factors at 746 data locations. Factor four is the nugget effect. The \gls{SGRI} algorithm is run using a search neighbourhood of 40 nodes, a maximum of 20,000 and 10,000 iterations for the coarse search and polishing steps, respectively, and rejection tolerances of 0.1 and 0.01 for the coarse search and polishing steps, respectively. Figure \ref{fig:fact_scatter_marginals} shows a scatter plot matrix between all imputed factors, the mapped values in observed space, $\tilde{z}$, and the true data values, $z$ for a single realization. As expected the correlation between $z$ and $\tilde{z}$ is 1.0. The top row of the matrix in Figure \ref{fig:fact_scatter_marginals} is somewhat redundant due to this perfect correlation however it highlights the exact data matching and validates item (1). The marginal distributions (histograms in \ref{fig:fact_scatter_marginals}) are all standard normal validating item (2). Deviation from the standard normal distribution is possible and one must consider the variogram range relative to the domain size. Long-range variogram structures may generate low variance imputed distributions. Imputed latent factors are clearly uncorrelated with $\lvert \rho \rvert < 0.10$ validating item (3). Figure \ref{fig:factor_varios} shows variogram reproduction for each imputed factor. As there is no nugget effect variogram model the reproduction is not shown. The red shaded area highlights uncertainty in the variogram across all imputed realizations. The expected imputed variograms reproduce the single structure variogram models reasonably well for 746 data.

Something about resim flagging here - show a table with neighbouring samples
Something about average a/b here - show a table with neighbouring samples
Something about using a lookup table here

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/fact_scatter_marginals.png}
    \caption{Scatter plot matrix for a single imputed realization highlighting the perfect correlation between $z$ and $\tilde{z}$, the uncorrelated nature of all latent factors, and the standard normal nature of all marginal distributions. These features are the first three minimum acceptance criteria for \gls{SGRI} imputed realizations.}
    \label{fig:fact_scatter_marginals}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.7, max size={\textwidth}{\textheight}]{./0-Figures/05-Ch5/factor_varios.png}
    \caption{Latent factor variogram reproduction for the first three factors. The expected variograms of the imputed realizations reproduce the models well given the data density. Variogram reproduction is the fourth minimum acceptance criteria for \gls{SGRI} imputed realizations.}
    \label{fig:factor_varios}
\end{figure}

\FloatBarrier
\section{Latent Factor Simulation}
\label{sec:latentsim}


\FloatBarrier
\section{Discussion}
\label{sec:discuss05}