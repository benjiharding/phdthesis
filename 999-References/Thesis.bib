@book{aggarwal2016outlier,
  title = {Outlier {{Analysis}}},
  author = {Aggarwal, Charu C},
  year = {2016},
  publisher = {Springer International Publishing},
  isbn = {978-3-319-47578-3},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Aggarwal\aggarwal2016outlier.pdf}
}

@article{ahmad2022differential,
  title = {Differential Evolution: {{A}} Recent Review Based on State-of-the-Art Works},
  shorttitle = {Differential Evolution},
  author = {Ahmad, Mohamad Faiz and Isa, Nor Ashidi Mat and Lim, Wei Hong and Ang, Koon Meng},
  year = {2022},
  month = may,
  journal = {Alexandria Engineering Journal},
  volume = {61},
  number = {5},
  pages = {3831--3872},
  issn = {1110-0168},
  doi = {10.1016/j.aej.2021.09.013},
  urldate = {2024-04-30},
  abstract = {Differential evolution (DE) is a popular evolutionary algorithm inspired by Darwin's theory of evolution and has been studied extensively to solve different areas of optimisation and engineering applications since its introduction by Storn in 1997. This study aims to review the massive progress of DE in the research community by analysing the 192 articles published on this subject from 1997 to 2021, particularly studies in the past five years. The methodology used to search for relevant DE papers and an overview of the original DE are firstly explained. Recent advances in the modifications proposed to enhance the effectiveness and efficiency of the original DE are reviewed by analysing the strengths and weaknesses of each published work, followed by the potential applications of these DE variants in solving different real-world engineering problems. In contrast to most existing DE review papers, additional analyses are performed in this survey by investigating the impacts of various parameter settings on given DE variants to identify their optimal values required for solving certain problem classes. The qualities of modifications incorporated into selected DE variants are also evaluated by measuring the performance gains achieved in terms of search accuracy and/or efficiency against the original DE. The additional surveys conducted in this study are anticipated to provide more insightful perspectives for both beginners and experts of DE research, enabling their better understanding about current research trends and new motivations to outline appropriate strategic planning for future development works.},
  keywords = {Crossover,Differential Evolution,Initialisation,Metaheuristics,Mutation,Optimisation,Selection},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Ahmad et al\\ahmad2022differential2.pdf;C\:\\Users\\benha\\Zotero\\storage\\JSKCK2K5\\S111001682100613X.html}
}

@book{armstrong2011plurigaussian,
  title = {Plurigaussian {{Simulations}} in {{Geosciences}}},
  author = {Armstrong, Margaret and Galli, Alain and Beucher, H{\'e}l{\`e}ne and Loc'h, Gaelle and Renard, Didier and Doligez, Brigitte and Eschard, R{\'e}mi and Geffroy, Francois},
  year = {2011},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-19607-2},
  urldate = {2024-02-14},
  isbn = {978-3-642-19606-5 978-3-642-19607-2},
  langid = {english},
  keywords = {Geostatistics,Mining,Petroleum,Simulations},
  file = {D:\03 UofA\06 Reading\_zotfile\Armstrong et al\armstrong2011plurigaussian.pdf}
}

@article{arroyo2020iterative,
  title = {Iterative Algorithms for Non-Conditional and Conditional Simulation of {{Gaussian}} Random Vectors},
  author = {Arroyo, Daisy and Emery, Xavier},
  year = {2020},
  month = oct,
  journal = {Stochastic Environmental Research and Risk Assessment},
  volume = {34},
  number = {10},
  pages = {1523--1541},
  issn = {1436-3259},
  doi = {10.1007/s00477-020-01875-0},
  urldate = {2024-02-13},
  abstract = {The conditional simulation of Gaussian random vectors is widely used in geostatistical~applications to quantify uncertainty in regionalized phenomena that have been observed at finitely many sampling locations. Two iterative algorithms are presented to deal with such a simulation. The first one is a variation of the propagative version of the Gibbs sampler aimed at simulating the random vector without any conditioning data. The novelty of the presented algorithm stems from the introduction of a relaxation parameter that, if adequately chosen, allows quickening the rates of convergence and mixing of the sampler. The second algorithm is meant to convert the non-conditional simulation into a conditional one, based on the successive over-relaxation method. Again, a relaxation parameter allows quickening the convergence in distribution to the desired conditional random vector. Both algorithms are applicable in a very general setting and avoid the pivoting, inversion, square rooting or decomposition of the variance-covariance matrix of the vector to be simulated, thus reduce the computation costs and memory requirements with respect to other discrete~geostatistical simulation approaches.},
  langid = {english},
  keywords = {Gauss-Seidel method,Gaussian random fields,Gibbs sampler,Mixing,Successive over-relaxation method},
  file = {D:\03 UofA\06 Reading\_zotfile\Arroyo_Emery\arroyo2020iterative.pdf}
}

@techreport{artemis2020,
  title = {Blackwater Gold Project British Columbia - {{NI}} 43-101 Technical Report on Pre-Feasibility Study},
  author = {{Artemis Gold Inc.}},
  year = {2020},
  keywords = {thesis_02}
}

@article{athens2022stochastic,
  title = {Stochastic {{Inversion}} of {{Gravity Data Accounting}} for {{Structural Uncertainty}}},
  author = {Athens, Noah and Caers, Jef},
  year = {2022},
  month = feb,
  journal = {Mathematical Geosciences},
  volume = {54},
  number = {2},
  pages = {413--436},
  issn = {1874-8953},
  doi = {10.1007/s11004-021-09978-2},
  urldate = {2024-04-04},
  abstract = {Conventional gravity inversion techniques have limited ability to quantify structural uncertainty in geologic models. In this paper, a stochastic framework is proposed that directly incorporates fault-related and density-related uncertainty into the inversion process. The approach uses Monte Carlo simulation to generate model realizations and the gradual deformation method to further refine models to match observed data. To guarantee that model realizations are structurally restorable, fault displacements are generated using a kinematic modeling approach in which fault model properties such as the number of faults, location, dip, slip, and orientation are considered uncertain. Using a synthetic case study problem, a reference gravity field was inverted to generate a suite of posterior model realizations. Analysis of the posterior models was used to create a fault probability map as well as quantify the distribution of slip and dip of faults in three zones of deformation. Uncertainty in density values was found to be greatly reduced in the top 250~m depth, suggesting limited sensitivity to deeper sources in this example. Following the synthetic case study problem, the inversion approach was applied to a field-observed gravity profile in Dixie Valley, Nevada, and the inversion results were compared to a previously published forward gravity model. By generating a suite of posterior models, structural uncertainty can be better assessed to make more informed decisions in a host of subsurface modeling problems.},
  langid = {english},
  keywords = {Gravity modeling,Inverse modeling,Structural modeling,Uncertainty},
  file = {D:\03 UofA\06 Reading\_zotfile\Athens_Caers\athens2022stochastic.pdf}
}

@phdthesis{babakhani2014geostatistical,
  title = {Geostatistical {{Modeling}} in {{Presence}} of {{Extreme Values}}},
  author = {Babakhani, Mahshid},
  year = 2014,
  doi = {10.7939/R3VQ2SJ85},
  urldate = {2023-11-15},
  abstract = {Geostatistical modeling in presence of extreme values needs special attention. Certain extreme high values known as outliers require...},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Babakhani\babakhani2014geostatistical.pdf}
}

@article{balkaya20173d,
  title = {{{3D}} Non-Linear Inversion of Magnetic Anomalies Caused by Prismatic Bodies Using Differential Evolution Algorithm},
  author = {Balkaya, {\c C}a{\u g}layan and Ekinci, Yunus Levent and G{\"o}kt{\"u}rkler, G{\"o}khan and Turan, Se{\c c}il},
  year = {2017},
  month = jan,
  journal = {Journal of Applied Geophysics},
  volume = {136},
  pages = {372--386},
  issn = {0926-9851},
  doi = {10.1016/j.jappgeo.2016.10.040},
  urldate = {2024-04-04},
  abstract = {3D non-linear inversion of total field magnetic anomalies caused by vertical-sided prismatic bodies has been achieved by differential evolution (DE), which is one of the population-based evolutionary algorithms. We have demonstrated the efficiency of the algorithm on both synthetic and field magnetic anomalies by estimating horizontal distances from the origin in both north and east directions, depths to the top and bottom of the bodies, inclination and declination angles of the magnetization, and intensity of magnetization of the causative bodies. In the synthetic anomaly case, we have considered both noise-free and noisy data sets due to two vertical-sided prismatic bodies in a non-magnetic medium. For the field case, airborne magnetic anomalies originated from intrusive granitoids at the eastern part of the Biga Peninsula (NW Turkey) which is composed of various kinds of sedimentary, metamorphic and igneous rocks, have been inverted and interpreted. Since the granitoids are the outcropped rocks in the field, the estimations for the top depths of two prisms representing the magnetic bodies were excluded during inversion studies. Estimated bottom depths are in good agreement with the ones obtained by a different approach based on 3D modelling of pseudogravity anomalies. Accuracy of the estimated parameters from both cases has been also investigated via probability density functions. Based on the tests in the present study, it can be concluded that DE is a useful tool for the parameter estimation of source bodies using magnetic anomalies.},
  keywords = {Differential evolution,Granitoids,Magnetic anomaly,Metaheuristic,Non-linear inversion,Prismatic bodies},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Balkaya et al\\balkaya20173d.pdf;C\:\\Users\\benha\\Zotero\\storage\\3T2CP34V\\S0926985116304347.html}
}

@techreport{banyan2020,
  title = {{{TECHNICAL REPORT ON THE AURMAC PROPERTY}}, {{MAYO MINING DISTRICT YUKON TERRITORY}}, {{CANADA}}},
  author = {{Banyan Gold Corp.}},
  year = {2020},
  keywords = {thesis_02}
}

@article{bardossy2016random,
  title = {Random {{Mixing}}: {{An Approach}} to {{Inverse Modeling}} for {{Groundwater Flow}} and {{Transport Problems}}},
  shorttitle = {Random {{Mixing}}},
  author = {B{\'a}rdossy, Andr{\'a}s and H{\"o}rning, Sebastian},
  year = {2016},
  month = sep,
  journal = {Transport in Porous Media},
  volume = {114},
  number = {2},
  pages = {241--259},
  issn = {1573-1634},
  doi = {10.1007/s11242-015-0608-4},
  urldate = {2024-02-15},
  abstract = {This paper presents a novel methodology for inverse modeling of groundwater flow and transport problems in a Monte Carlo framework, i.e., multiple solutions to the inverse problem are generated. The methodology is based on the concept of random mixing of spatial random fields. The conditional target hydraulic transmissivity field is obtained as a linear combination of unconditional spatial random fields. The corresponding weights of the linear combination are selected such that the spatial variability of the hydraulic transmissivities as well as the actual observed transmissivity values are reproduced. The constraints related to the hydraulic head and contaminant concentration observations are nonlinear. In order to fulfill these constraints, a specific property of the presented approach is used. A connected domain of fields fulfilling all linear constraints is identified. This domain includes an infinite number of realizations, and in this domain, the head and concentration deviations are minimized using standard continuous optimization techniques. The methodology uses spatial copulas to describe the spatial dependence structure. A combination with multiple point statistics allows inversion under specific structural constraints.},
  langid = {english},
  keywords = {Copula,Inverse modeling,Multiple point statistics,Random mixing},
  file = {D:\03 UofA\06 Reading\_zotfile\Bárdossy_Hörning\bardossy2016random.pdf}
}

@book{barnett1984outliers,
  title = {Outliers in Statistical Data},
  author = {Barnett, Vic and Lewis, Toby},
  year = {1984},
  journal = {Wiley Series in Probability and Mathematical Statistics. Applied Probability and Statistics},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Barnett_Lewis\barnett1984outliers.pdf}
}

@article{barnett2014projection,
  title = {Projection {{Pursuit Multivariate Transform}}},
  author = {Barnett, Ryan M. and Manchuk, John G. and Deutsch, Clayton V.},
  year = {2014},
  month = apr,
  journal = {Mathematical Geosciences},
  volume = {46},
  number = {3},
  pages = {337--359},
  issn = {1874-8953},
  doi = {10.1007/s11004-013-9497-7},
  urldate = {2022-09-20},
  abstract = {Transforming complex multivariate geological data to a Gaussian distribution is an important and challenging problem in geostatistics. A~variety of transforms are available for this goal, but struggle with high dimensional data sets. Projection pursuit density estimation (PPDE) is a well-established nonparametric method for estimating the joint density of multivariate data. A~central component of the PPDE algorithm transforms the original data toward a multivariate Gaussian distribution. The PPDE approach is modified to map complex data to a multivariate Gaussian distribution within a geostatistical modeling context. Traditional modeling may then take place on the transformed Gaussian data, with a back-transform used to return simulated variables to their original units. This approach is referred to as the projection pursuit multivariate transform (PPMT). The PPMT shows the potential to be an effective means for modeling high dimensional and complex geologic data. The PPMT algorithm is developed before discussing considerations and limitations. A~case study compares modeling results against more common techniques to demonstrate the value and place of the PPMT within geostatistics.},
  langid = {english},
  keywords = {Geostatistical Modeling,Kernel Density Estimation,Projection Index,Projection Pursuit,Radial Point Interpolation Method},
  file = {D:\03 UofA\06 Reading\_zotfile\Barnett et al\barnett2014projection.pdf}
}

@article{barnett2015multivariate,
  title = {Multivariate {{Imputation}} of {{Unequally Sampled Geological Variables}}},
  author = {Barnett, Ryan M. and Deutsch, Clayton V.},
  year = {2015},
  month = oct,
  journal = {Mathematical Geosciences},
  volume = {47},
  number = {7},
  pages = {791--817},
  issn = {1874-8953},
  doi = {10.1007/s11004-014-9580-8},
  urldate = {2023-08-14},
  abstract = {Unequally sampled data pose a practical and significant problem for geostatistical modeling. Multivariate transformations are frequently applied in modeling workflows to reproduce the multivariate relationships of geological data. Unfortunately, these transformations may only be applied to data observations that sample all of the variables. In the case of unequal sampling, practitioners must decide between excluding incomplete observations and imputing (inferring) the missing values. While imputation is recommended by missing data theorists, the use of deterministic methods such as regression is generally discouraged. Instead, techniques such as multiple imputation (MI) are advocated to increase the accuracy, decrease the bias, and capture the uncertainty of imputed values. As missing data theory has received little attention within geostatistical literature and practice, MI has not been adapted from its conventional form to be suitable for geological data. To address this, geostatistical algorithms are integrated within an MI framework to produce parametric and non-parametric methods. Synthetic and geometallurgical case studies are used to demonstrate the feasibility of each method, where techniques that use both spatial and colocated information are shown to outperform the alternatives.},
  langid = {english},
  keywords = {Geostatistics,Missing data analysis,Modeling,Statistics,thesis_05},
  file = {D:\03 UofA\06 Reading\_zotfile\Barnett_Deutsch\barnett2015multivariate.pdf}
}

@book{bellman1961adaptive,
  title = {Adaptive Control Processes: {{A}} Guided Tour},
  author = {Bellman, Richard E.},
  year = {1961},
  publisher = {Princeton University Press},
  address = {Princeton},
  doi = {doi:10.1515/9781400874668},
  urldate = {2024-04-19},
  isbn = {978-1-4008-7466-8},
  file = {D:\03 UofA\06 Reading\_zotfile\Bellman\bellman1961adaptive.pdf}
}

@article{bilal2020differential,
  title = {Differential {{Evolution}}: {{A}} Review of More than Two Decades of Research},
  shorttitle = {Differential {{Evolution}}},
  author = {{Bilal} and Pant, Millie and Zaheer, Hira and {Garcia-Hernandez}, Laura and Abraham, Ajith},
  year = {2020},
  month = apr,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {90},
  pages = {103479},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2020.103479},
  urldate = {2024-03-15},
  abstract = {Since its inception in 1995, Differential Evolution (DE) has emerged as one of the most frequently used algorithms for solving complex optimization problems. Its flexibility and versatility have prompted several customized variants of DE for solving a variety of real life and test problems. The present study, surveys the near 25 years of existence of DE. In this extensive survey, 283 research articles have been covered and the journey of DE is shown through its basic aspects like population generation, mutation schemes, crossover schemes, variation in parameters and hybridized variants along with various successful applications of DE. This study also provides some key bibliometric indicators like highly cited papers having citations more than 500, publication trend since 1996, journal citations etc. The main aim of the present document is to serve as an extended summary of 25 years of existence of DE, intended for dissemination to interested parties. It is expected that the present survey would generate interest among the new users towards the philosophy of DE and would also guide the experience researchers.},
  keywords = {Crossover,Differential evolution,Meta-heuristics,Mutation,Selection},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Bilal et al\\bilal2020differential.pdf;C\:\\Users\\benha\\Zotero\\storage\\BHNXG4EA\\S095219762030004X.html}
}

@article{boisvert2007multiplepoint,
  title = {Multiple-{{Point Statistics}} for {{Training Image Selection}}},
  author = {Boisvert, Jeff B. and Pyrcz, Michael J. and Deutsch, Clayton V.},
  year = {2007},
  month = dec,
  journal = {Natural Resources Research},
  volume = {16},
  number = {4},
  pages = {313--321},
  issn = {1573-8981},
  doi = {10.1007/s11053-008-9058-9},
  urldate = {2022-05-26},
  abstract = {Selecting a training image (TI) that is representative of the target spatial phenomenon (reservoir, mineral deposit, soil type, etc.) is essential for an effective application of multiple-point statistics (MPS) simulation. It is often possible to narrow potential TIs to a general subset based on the available geological knowledge; however, this is largely subjective. A method is presented that compares the distribution of runs and the multiple-point density function from available exploration data and TIs. The difference in the MPS can be used to select the TI that is most representative of the data set. This tool may be applied to further narrow a suite of TIs for a more realistic model of spatial uncertainty. In addition, significant differences between the spatial statistics of local conditioning data and a TI may lead to artifacts in MPS. The utilization of this tool will identify contradictions between conditioning data and TIs. TI selection is demonstrated for a deepwater reservoir with 32 wells.},
  langid = {english},
  keywords = {Estimation,Geostatistics,Reserves,Runs,Uncertainty},
  file = {D:\03 UofA\06 Reading\_zotfile\Boisvert et al\boisvert2007multiplepoint.pdf}
}

@article{boukerche2021outlier,
  title = {Outlier {{Detection}}: {{Methods}}, {{Models}}, and {{Classification}}},
  shorttitle = {Outlier {{Detection}}},
  author = {Boukerche, Azzedine and Zheng, Lining and Alfandi, Omar},
  year = {2021},
  month = may,
  journal = {ACM Computing Surveys},
  volume = {53},
  number = {3},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3381028},
  urldate = {2024-04-29},
  abstract = {Over the past decade, we have witnessed an enormous amount of research effort dedicated to the design of efficient outlier detection techniques while taking into consideration efficiency, accuracy, high-dimensional data, and distributed environments, among other factors. In this article, we present and examine these characteristics, current solutions, as well as open challenges and future research directions in identifying new outlier detection strategies. We propose a taxonomy of the recently designed outlier detection strategies while underlying their fundamental characteristics and properties. We also introduce several newly trending outlier detection methods designed for high-dimensional data, data streams, big data, and minimally labeled data. Last, we review their advantages and limitations and then discuss future and new challenging issues.},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Boukerche et al\boukerche2021outlier.pdf}
}

@article{caers1999statistics,
  title = {Statistics for Modeling Heavy Tailed Distributions in Geology: {{Part I}}. {{Methodology}}},
  author = {Caers, Jef and Beirlant, Jan and Maes, Marc A},
  year = {1999},
  journal = {Mathematical geology},
  volume = {31},
  number = {4},
  pages = {391--410},
  publisher = {Springer},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Caers et al\caers1999statistics.pdf}
}

@article{caers1999statisticsa,
  title = {Statistics for Modeling Heavy Tailed Distributions in Geology: {{Part II}}. {{Applications}}},
  author = {Caers, Jef and Beirlant, Jan and Maes, Marc A},
  year = {1999},
  journal = {Mathematical geology},
  volume = {31},
  number = {4},
  pages = {411--434},
  publisher = {Springer},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Caers et al\caers1999statisticsa.pdf}
}

@book{carbas2021nature,
  title = {Nature-Inspired Metaheuristic Algorithms for Engineering Optimization Applications},
  author = {Carbas, Serdar and Toktas, Abdurrahim and Ustun, Deniz},
  year = {2021},
  publisher = {Springer},
  file = {D:\03 UofA\06 Reading\_zotfile\Carbas et al\carbas2021nature.pdf}
}

@techreport{cardinal2019,
  title = {Namdini Gold Project Feasibility Study {{NI}} 43-101 Technical Report, Ghana, West Africa},
  author = {{Cardinal Resources}},
  year = {2019},
  keywords = {thesis_02}
}

@techreport{cartier2020,
  title = {{{NI}} 43-101 Technical Report and Mineral Resource Estimate for the Central, North and South Gold Corridors on the Chimo Mine Project, Qu{\'e}bec, Canada},
  author = {{Cartier Resources Inc.}},
  year = {2020},
  keywords = {thesis_02}
}

@misc{carvalho2017overview,
  title = {An {{Overview}} of {{Multiple Indicator Kriging}}},
  author = {Carvalho, Dhaniel and Deutsch, Clayton V},
  year = {2017},
  month = jan,
  urldate = {2022-10-03},
  howpublished = {https://geostatisticslessons.com/lessons/mikoverview},
  keywords = {thesis_02},
  file = {C:\Users\benha\Zotero\storage\J53QYQC4\mikoverview.html}
}

@article{chen2008detecting,
  title = {On Detecting Spatial Outliers},
  author = {Chen, Dechang and Lu, Chang-Tien and Kou, Yufeng and Chen, Feng},
  year = {2008},
  journal = {Geoinformatica},
  volume = {12},
  number = {4},
  pages = {455--475},
  publisher = {Springer},
  file = {D\:\\03 UofA\\04 Research\\02 PhD\\_zotfile\\Chen et al\\chen2008detecting.pdf;C\:\\Users\\benha\\Zotero\\storage\\25ZTKSQN\\s10707-007-0038-8.html}
}

@book{chiles2012geostatistics,
  title = {Geostatistics: {{Modeling Spatial Uncertainty}}},
  author = {Chil{\`e}s, Jean-Paul and Delfiner, Pierre},
  year = {2012},
  publisher = {John Wiley \& Sons},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Chilès_Delfiner\chiles2012geostatistics.pdf}
}

@book{chilès2012geostatistics,
  title = {Geostatistics: {{Modeling Spatial Uncertainty}}, {{Second Edition}}},
  author = {Chil{\`e}s, Jean-Paul and Delfiner, Pierre},
  year = {2012},
  edition = {2nd},
  publisher = {John Wiley \& Sons, Inc.},
  isbn = {978-0-470-18315-1},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Chilès_Delfiner\chilès2012geostatistics.pdf}
}

@techreport{cim2019,
  title = {{{CIM}} Estimation of Mineral Resources \& Mineral Reserves Best Practice Guidelines},
  author = {{CIM Mineral Resource \& Mineral Reserve Committee}},
  year = {2019},
  month = nov,
  institution = {{Canadian Institute of Mining, Metallurgy and Petroleum}},
  keywords = {thesis_02}
}

@article{clifton2014extending,
  title = {Extending the {{Generalised Pareto Distribution}} for {{Novelty Detection}} in {{High-Dimensional Spaces}}},
  author = {Clifton, David A. and Clifton, Lei and Hugueny, Samuel and Tarassenko, Lionel},
  year = {2014},
  month = mar,
  journal = {Journal of Signal Processing Systems},
  volume = {74},
  number = {3},
  pages = {323--339},
  issn = {1939-8115},
  doi = {10.1007/s11265-013-0835-2},
  urldate = {2022-01-14},
  abstract = {Novelty detection involves the construction of a ``model of normality'', and then classifies test data as being either ``normal'' or ``abnormal'' with respect to that model. For this reason, it is often termed one-class classification. The approach is suitable for cases in which examples of ``normal'' behaviour are commonly available, but in which cases of ``abnormal'' data are comparatively rare. When performing novelty detection, we are typically most interested in the tails of the normal model, because it is in these tails that a decision boundary between ``normal'' and ``abnormal'' areas of data space usually lies. Extreme value statistics provides an appropriate theoretical framework for modelling the tails of univariate (or low-dimensional) distributions, using the generalised Pareto distribution (GPD), which can be demonstrated to be the limiting distribution for data occurring within the tails of most practically-encountered probability distributions. This paper provides an extension of the GPD, allowing the modelling of probability distributions of arbitrarily high dimension, such as occurs when using complex, multimodel, multivariate distributions for performing novelty detection in most real-life cases. We demonstrate our extension to the GPD using examples from patient physiological monitoring, in which we have acquired data from hospital patients in large clinical studies of high-acuity wards, and in which we wish to determine ``abnormal'' patient data, such that early warning of patient physiological deterioration may be provided.},
  langid = {english},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Clifton et al\clifton2014extending.pdf}
}

@book{coles2001introduction,
  title = {An Introduction to Statistical Modeling of Extreme Values},
  author = {Coles, Stuart and Bawa, Joanna and Trenner, Lesley and Dorazio, Pat},
  year = {2001},
  volume = {208},
  publisher = {Springer},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Coles et al\coles2001introduction.pdf}
}

@book{conn2009introduction,
  title = {Introduction to Derivative-Free Optimization},
  author = {Conn, Andrew R and Scheinberg, Katya and Vicente, Luis N},
  year = {2009},
  publisher = {SIAM}
}

@article{costa2003reducing,
  title = {Reducing the Impact of Outliers in Ore Reserves Estimation},
  author = {Costa, Joao Felipe},
  year = {2003},
  journal = {Mathematical geology},
  volume = {35},
  number = {3},
  pages = {323--345},
  publisher = {Springer},
  file = {D\:\\03 UofA\\04 Research\\02 PhD\\_zotfile\\Costa\\costa2003reducing.pdf;C\:\\Users\\benha\\Zotero\\storage\\VBTPGIRW\\A1023822315523.html}
}

@article{cui2024applications,
  title = {Applications of Nature-Inspired Metaheuristic Algorithms for Tackling Optimization Problems across Disciplines},
  author = {Cui, Elvis Han and Zhang, Zizhao and Chen, Culsome Junwen and Wong, Weng Kee},
  year = {2024},
  month = apr,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {9403},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-56670-6},
  urldate = {2024-04-30},
  abstract = {Nature-inspired metaheuristic algorithms are important components of artificial intelligence, and are increasingly used across disciplines to tackle various types of challenging optimization problems. This paper demonstrates the usefulness of such algorithms for solving a variety of challenging optimization problems in statistics using a nature-inspired metaheuristic algorithm called competitive swarm optimizer with mutated agents (CSO-MA). This algorithm was proposed by one of the authors and its superior performance relative to many of its competitors had been demonstrated in earlier work and again in this paper. The main goal of this paper is to show a typical nature-inspired metaheuristic algorithmi, like CSO-MA, is efficient for tackling many different types of optimization problems in statistics. Our applications are new and include finding maximum likelihood estimates of parameters in a single cell generalized trend model to study pseudotime in bioinformatics, estimating parameters in the commonly used Rasch model in education research, finding M-estimates for a Cox regression in a Markov renewal model, performing matrix completion tasks to impute missing data for a two compartment model, and selecting variables optimally in an ecology problem in China. To further demonstrate the flexibility of metaheuristics, we also find an optimal design for a car refueling experiment in the auto industry using a logistic model with multiple interacting factors. In addition, we show that metaheuristics can sometimes outperform optimization algorithms commonly used in statistics.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational science,Statistics},
  file = {D:\03 UofA\06 Reading\_zotfile\Cui et al\cui2024applications.pdf}
}

@article{dasilva2019treatment,
  title = {The Treatment of Missing `Not at Random' Geological Data for Ore Grade Modelling},
  author = {{da Silva}, Camilla Z. and Costa, Jo{\~a}o Felipe C. L.},
  year = {2019},
  month = jan,
  journal = {Applied Earth Science},
  volume = {128},
  number = {1},
  pages = {15--26},
  publisher = {Taylor \& Francis},
  issn = {2572-6838},
  doi = {10.1080/25726838.2018.1547508},
  urldate = {2024-04-29},
  abstract = {Geostatistics aims to spatially model multiple correlated attributes with some under-sampled variables. Missing samples can distort the data set statistics, and for the model to be consistent, it is necessary to understand the mechanisms that cause the missing data (MD). There are three mechanisms that can govern MD: Missing Completely at Random (MCAR), Missing at Random (MAR) and Missing Not At Random (MNAR). More specifically, when there is a systematic difference between measured and the missing samples, the data is subject to the MNAR mechanism, which leads to biases in the final model if not dealt with adequately. A case is presented where an under-sampled attribute, U (ppm), is modelled to infer its values over the study area through co-simulation approach and an univariate simulation performed on an artificially complete set, considering the MNAR mechanism, which shows a 10\% reduction in the relative estimation errors compared to the co-simulated model.},
  keywords = {Co-simulation,data imputation,grade modelling,MNAR,simulation},
  file = {D:\03 UofA\06 Reading\_zotfile\da Silva_Costa\dasilva2019treatment.pdf}
}

@article{davilarodriguez2024threedimensional,
  title = {Three-{{Dimensional Inversion}} of {{Magnetic Anomalies Using}} a {{Low-Level Representation}} and an {{Evolution Strategy}} for {{Archaeological Studies}}},
  author = {D{\'a}vila Rodr{\'i}guez, Israel Alberto and Palafox Gonz{\'a}lez, Abel and Guerrero Arroyo, Edgar Alejandro and Becerra L{\'o}pez, Fernando I. and Fregoso Becerra, Emilia},
  year = {2024},
  month = apr,
  journal = {Mathematical Geosciences},
  volume = {56},
  number = {3},
  pages = {511--539},
  issn = {1874-8953},
  doi = {10.1007/s11004-023-10090-w},
  urldate = {2024-04-04},
  abstract = {Geophysical data inversion typically involves the numerical solution of high-dimensional ill-posed problems. To reduce the non-uniqueness, prior information in the form of appropriate regularization schemes, together with a proper representation of the quantities of interest, is helpful. To this end, methods are recommended that take into account a low-dimensional formulation of the inverse problem, a suitable representation for the quantities of interest, and a helpful numerical procedure. Structural aspects of the objects of interest, such as the dimensions of structures, are often available as prior knowledge in archaeological magnetic surveys. However, they are not easily exploited by classical geophysical data inversion applications; for example, a typical representation together with Tikhonov-like regularization schemes or total variation, which promote smoothness in the solutions, occlude underlying structural information or retrieve fuzzy boundaries. In this work, a three-dimensional inversion of magnetic data is developed based on an evolution strategy. It is adapted to a numerical representation that easily incorporates aspects achievable at archaeological sites. Synthetic test cases and a magnetic dataset corresponding to an archaeological site are used to report the results.},
  langid = {english},
  keywords = {Dimension reduction,Evolution strategies,Geophysical inverse problems,Low-level representation,Magnetic data inversion,Stochastic methods},
  file = {D:\03 UofA\06 Reading\_zotfile\Dávila Rodríguez et al\davilarodriguez2024threedimensional.pdf}
}

@article{davis1987production,
  title = {Production of Conditional Simulations via the {{LU}} Triangular Decomposition of the Covariance Matrix},
  author = {Davis, Michael W.},
  year = {1987},
  month = feb,
  journal = {Mathematical Geology},
  volume = {19},
  number = {2},
  pages = {91--98},
  issn = {1573-8868},
  doi = {10.1007/BF00898189},
  urldate = {2023-08-17},
  abstract = {This paper reviews the turning band method and fast Fourier transform method of producing a nonconditional simulation of a multinormal random function with a given covariance structure. A review of the two common methods of conditioning the simulation to honor the data shows that they are formally equivalent. Another method for directly pondering a conditional simulation based on the LU triangular decomposition of the covariance matrix is presented. Computational and implementation difficulties are discussed.},
  langid = {english},
  keywords = {conditional simulation,fast Fourier transform,geostatistics,kriging},
  file = {D:\03 UofA\06 Reading\_zotfile\Davis\davis1987productiona.pdf}
}

@article{davison2015statistics,
  title = {Statistics of Extremes},
  author = {Davison, Anthony C and Huser, Rapha{\"e}l},
  year = {2015},
  journal = {Annual Review of Statistics and its Application},
  volume = {2},
  pages = {203--235},
  publisher = {Annual Reviews},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Davison_Huser\davison2015statistics.pdf}
}

@article{decarvalho2019highorder,
  title = {High-{{Order Block Support Spatial Simulation Method}} and {{Its Application}} at a {{Gold Deposit}}},
  author = {{de Carvalho}, Joao Pedro and Dimitrakopoulos, Roussos and Minniakhmetov, Ilnur},
  year = {2019},
  month = aug,
  journal = {Mathematical Geosciences},
  volume = {51},
  number = {6},
  pages = {793--810},
  issn = {1874-8953},
  doi = {10.1007/s11004-019-09784-x},
  urldate = {2024-04-25},
  abstract = {High-order sequential simulation methods have been developed as an alternative to existing frameworks to facilitate the modeling of the spatial complexity of non-Gaussian spatially distributed variables of interest. These high-order simulation approaches address the modeling of the curvilinear features and spatial connectivity of extreme values that are common in mineral deposits, petroleum reservoirs, water aquifers, and other geological phenomena. This paper presents a new high-order simulation method that generates realizations directly at the block support scale conditioned to the available data at point support scale. In the context of sequential high-order simulation, the method estimates, at each block location, the cross-support joint probability density function using Legendre-like splines as the set of basis functions needed. The proposed method adds previously simulated blocks to the set of conditioning data, which initially contains the available data at point support scale. A spatial template is defined by the configuration of the block to be simulated and related conditioning values at both support scales, and is used to infer additional high-order statistics from a training image. Testing of the proposed method with an exhaustive dataset shows that simulated realizations reproduce major structures and high-order relations of data. The practical intricacies of the proposed method are demonstrated in an application at a gold deposit.},
  langid = {english},
  keywords = {Block support,Cross-support joint probability density function,High-order spatial statistics,Sequential high-order simulation},
  file = {D:\03 UofA\06 Reading\_zotfile\de Carvalho et al\decarvalho2019highorder.pdf}
}

@book{dehaan2007extreme,
  title = {Extreme Value Theory: An Introduction},
  author = {De Haan, Laurens and Ferreira, Ana},
  year = {2007},
  publisher = {Springer Science \& Business Media},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\De Haan_Ferreira\dehaan2007extreme.pdf}
}

@article{deligne2010recurrence,
  title = {Recurrence Rates of Large Explosive Volcanic Eruptions},
  author = {Deligne, N. I. and Coles, S. G. and Sparks, R. S. J.},
  year = {2010},
  journal = {Journal of Geophysical Research: Solid Earth},
  volume = {115},
  number = {B6},
  issn = {2156-2202},
  doi = {10.1029/2009JB006554},
  urldate = {2022-02-15},
  abstract = {A global database of large explosive volcanic eruptions has been compiled for the Holocene and analyzed using extreme value theory to estimate magnitude-frequency relationships. The database consists of explosive eruptions with magnitude (M) greater than or equal to 4. Two models are applied to the data, one assuming no underreporting of eruptions and the other taking underreporting into consideration. Results from the latter indicate that the level of underreporting is high and fairly constant from the start of the Holocene until about 1 A.D. and then decreases dramatically toward the present. Results indicate there is only a {$\sim$}20\% probability that an explosive eruption of M = 6 occurring prior to 1 A.D. is recorded. Analysis of the data set in the time periods 1750 A.D. and 1900 A.D. to present (assuming no underreporting) suggests that that these periods are likely to be too short to give reliable estimates of return periods for explosive eruptions with M {$>$} 6. Analysis of the Holocene data set with corrections for underreporting bias provide robust magnitude-frequency relationships up to M = 7. Extrapolation of the model to greater magnitudes (M {$>$} 8) gives results inconsistent with geological data, predicting eruption size upper limits much smaller than known eruptions such as the Fish Canyon Tuff. We interpret this result as the consequence of different mechanisms operating for explosive eruptions with M {$>$} 7.},
  langid = {english},
  keywords = {explosive volcanism,extreme value theory,Holocene},
  file = {D\:\\03 UofA\\04 Research\\02 PhD\\_zotfile\\Deligne et al\\deligne2010recurrence.pdf;C\:\\Users\\benha\\Zotero\\storage\\VNY8M2PD\\2009JB006554.html}
}

@phdthesis{deutsch1992annealing,
  title = {Annealing Techniques Applied to Reservoir Modeling and the Integration of Geological and Engineering (Well Test) Data},
  author = {Deutsch, Clayton Vernon},
  year = {1992},
  school = {stanford university},
  file = {D:\03 UofA\06 Reading\_zotfile\Deutsch\deutsch1992annealing.pdf}
}

@article{deutsch1992geostatistical,
  title = {Geostatistical Software Library and User's Guide},
  author = {Deutsch, Clayton V. and Journel, Andre G.},
  year = {1992},
  journal = {New York},
  volume = {119},
  number = {147}
}

@article{deutsch2007recall,
  title = {A {{Recall}} of {{Factorial Kriging}} with {{Examples}} and a {{Modified Version}} of Kt3d},
  author = {Deutsch, Clayton V},
  year = {2007},
  pages = {8},
  langid = {english},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Deutsch\deutsch2007recall.pdf}
}

@article{deutsch2010display,
  title = {Display of Cross Validation/Jackknife Results},
  author = {Deutsch, Clayton V},
  year = {2010},
  journal = {Centre for Computational Geostatistics Annual Report},
  volume = {12},
  number = {406},
  pages = {1--4},
  file = {D:\03 UofA\06 Reading\_zotfile\Deutsch\deutsch2010display.pdf}
}

@article{dimitrakopoulos2009highorder,
  title = {High-Order {{Statistics}} of {{Spatial Random Fields}}: {{Exploring Spatial Cumulants}} for {{Modeling Complex Non-Gaussian}} and {{Non-linear Phenomena}}},
  shorttitle = {High-Order {{Statistics}} of {{Spatial Random Fields}}},
  author = {Dimitrakopoulos, Roussos and Mustapha, Hussein and Gloaguen, Erwan},
  year = {2009},
  month = dec,
  journal = {Mathematical Geosciences},
  volume = {42},
  number = {1},
  pages = {65},
  issn = {1874-8953},
  doi = {10.1007/s11004-009-9258-9},
  urldate = {2022-08-30},
  abstract = {The spatial distributions of earth science and engineering phenomena under study are currently predicted from finite measurements and second-order geostatistical models. The latter models can be limiting, as geological systems are highly complex, non-Gaussian, and exhibit non-linear patterns of spatial connectivity. Non-linear and non-Gaussian high-order geostatistics based on spatial connectivity measures, namely spatial cumulants, are proposed as a new alternative modeling framework for spatial data. This framework has two parts. The first part is the definition, properties, and inference of spatial cumulants---including understanding the interrelation of cumulant characteristics with the in-situ behavior of geological entities or processes, as examined in this paper. The second part is the research on a random field model for simulation based on its high-order spatial cumulants.},
  langid = {english},
  keywords = {Complex geology,High-order statistics,Non-Gaussian spatial random functions,Spatial cumulants},
  file = {D:\03 UofA\06 Reading\_zotfile\Dimitrakopoulos et al\dimitrakopoulos2009highorder.pdf}
}

@article{drumond2019using,
  title = {Using {{Mahalanobis Distance}} to {{Detect}} and {{Remove Outliers}} in {{Experimental Covariograms}}},
  author = {Drumond, David Alvarenga and Rolo, Roberto Mentzingen and Costa, Jo{\~a}o Felipe Coimbra Leite},
  year = {2019},
  month = jan,
  journal = {Natural Resources Research},
  volume = {28},
  number = {1},
  pages = {145--152},
  issn = {1573-8981},
  doi = {10.1007/s11053-018-9399-y},
  urldate = {2022-01-13},
  abstract = {Experimental variograms are crucial for most geostatistical studies.In kriging, for example, the variography has a direct influence on the interpolation weights. Despite the great importance of variogram estimators in predicting geostatistical features, they are commonly influenced by outliers in the dataset. The effect of some randomly spatially distributed outliers can mask the pattern of the experimental variogram and produce a destructuration effect, implying that the true data spatial continuity cannot be reproduced. In this paper, an algorithm to detect and remove the effect of outliers in experimental variograms using the Mahalanobis distance is proposed. An example of the algorithm's application is presented, showing that the developed technique is able to satisfactorily detect and remove outliers from a variogram.},
  langid = {english},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Drumond et al\drumond2019using.pdf}
}

@article{dubey2022activation,
  title = {Activation Functions in Deep Learning: {{A}} Comprehensive Survey and Benchmark},
  shorttitle = {Activation Functions in Deep Learning},
  author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
  year = {2022},
  month = sep,
  journal = {Neurocomputing},
  volume = {503},
  pages = {92--108},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.06.111},
  urldate = {2024-03-13},
  abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: https://github.com/shivram1987/ActivationFunctions.},
  keywords = {Activation Functions,Convolutional neural networks,Deep learning,Neural networks,Overview,Recurrent Neural Networks},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Dubey et al\\dubey2022activation.pdf;C\:\\Users\\benha\\Zotero\\storage\\ZZFULDLK\\S0925231222008426.html}
}

@article{dutaut2021new,
  title = {A New Grade-Capping Approach Based on Coarse Duplicate Data Correlation},
  author = {Dutaut, R. V. and Marcotte, D.},
  year = {2021},
  month = may,
  journal = {Journal of the Southern African Institute of Mining and Metallurgy},
  volume = {121},
  number = {5},
  pages = {193--200},
  publisher = {{The Southern African Institute of Mining and Metallurgy}},
  issn = {2225-6253},
  doi = {10.17159/2411-9717/1379/2021},
  urldate = {2023-11-19},
  file = {D:\03 UofA\06 Reading\_zotfile\Dutaut_Marcotte\dutaut2021new.pdf}
}

@book{eidsvik2015value,
  title = {Value of Information in the Earth Sciences: {{Integrating}} Spatial Modeling and Decision Analysis},
  author = {Eidsvik, Jo and Mukerji, Tapan and Bhattacharjya, Debarun},
  year = {2015},
  publisher = {Cambridge University Press}
}

@techreport{eldorado2020,
  title = {Technical Report Ki{\c s}lada{\u g} Gold Mine Turkey},
  author = {{Eldorado Gold Corporation}},
  year = {2020},
  keywords = {thesis_02}
}

@article{emery2014simulating,
  title = {Simulating {{Large Gaussian Random Vectors Subject}} to {{Inequality Constraints}} by {{Gibbs Sampling}}},
  author = {Emery, Xavier and Arroyo, Daisy and Pel{\'a}ez, Mar{\'i}a},
  year = {2014},
  month = apr,
  journal = {Mathematical Geosciences},
  volume = {46},
  number = {3},
  pages = {265--283},
  issn = {1874-8953},
  doi = {10.1007/s11004-013-9495-9},
  urldate = {2023-08-14},
  abstract = {The Gibbs sampler is an iterative algorithm used to simulate Gaussian random vectors subject to inequality constraints. This algorithm relies on the fact that the distribution of a vector component conditioned by the other components is Gaussian, the mean and variance of which are obtained by solving a kriging system. If the number of components is large, kriging is usually applied with a moving search neighborhood, but this practice can make the simulated vector not reproduce the target correlation matrix. To avoid these problems, variations of the Gibbs sampler are presented. The conditioning to inequality constraints on the vector components can be achieved by simulated annealing or by restricting the transition matrix of the iterative algorithm. Numerical experiments indicate that both approaches provide realizations that reproduce the correlation matrix of the Gaussian random vector, but some conditioning constraints may not be satisfied when using simulated annealing. On the contrary, the restriction of the transition matrix manages to satisfy all the constraints, although at the cost of a large number of iterations.},
  langid = {english},
  keywords = {Gaussian random field,Gibbs sampler,Kriging neighborhood,Markov chain,Restriction of transition matrix,Simulated annealing,thesis_05},
  file = {D:\03 UofA\06 Reading\_zotfile\Emery et al\emery2014simulating.pdf}
}

@article{ernst2017comparison,
  title = {Comparison of Local Outlier Detection Techniques in Spatial Multivariate Data},
  author = {Ernst, Marie and Haesbroeck, Gentiane},
  year = {2017},
  journal = {Data mining and knowledge discovery},
  volume = {31},
  number = {2},
  pages = {371--399},
  publisher = {Springer},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Ernst_Haesbroeck\ernst2017comparison.pdf}
}

@book{everitt2010cambridge,
  title = {The Cambridge Dictionary of Statistics},
  author = {Everitt, B.S. and Skrondal, A.},
  year = {2010},
  publisher = {Cambridge University Press},
  isbn = {978-0-521-76699-9},
  lccn = {2010502891},
  keywords = {thesis_05},
  file = {D:\03 UofA\06 Reading\_zotfile\Everitt_Skrondal\everitt2010cambridge.pdf}
}

@article{filzmoser2005multivariate,
  title = {Multivariate Outlier Detection in Exploration Geochemistry},
  author = {Filzmoser, Peter and Garrett, Robert G and Reimann, Clemens},
  year = {2005},
  journal = {Computers \& geosciences},
  volume = {31},
  number = {5},
  pages = {579--587},
  publisher = {Elsevier},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Filzmoser et al\filzmoser2005multivariate.pdf}
}

@article{filzmoser2014identification,
  title = {Identification of Local Multivariate Outliers},
  author = {Filzmoser, Peter and {Ruiz-Gazen}, Anne and {Thomas-Agnan}, Christine},
  year = {2014},
  month = feb,
  journal = {Statistical Papers},
  volume = {55},
  number = {1},
  pages = {29--47},
  issn = {1613-9798},
  doi = {10.1007/s00362-013-0524-z},
  urldate = {2024-04-30},
  abstract = {The Mahalanobis distance between pairs of multivariate observations is used as a measure of similarity between the observations. The theoretical distribution is derived, and the result is used for judging on the degree of isolation of an observation. In case of spatially dependent data where spatial coordinates are available, different exploratory tools are introduced for studying the degree of isolation of an observation from a fraction of its neighbors, and thus to identify local multivariate outliers.},
  langid = {english},
  keywords = {Mahalanobis distance,MCD estimator,Outliers,Robust statistics,Spatial dependence},
  file = {D:\03 UofA\06 Reading\_zotfile\Filzmoser et al\filzmoser2014identification.pdf}
}

@article{filzmoser2020multivariate,
  title = {Multivariate {{Outlier Detection}} in {{Applied Data Analysis}}: {{Global}}, {{Local}}, {{Compositional}} and {{Cellwise Outliers}}},
  shorttitle = {Multivariate {{Outlier Detection}} in {{Applied Data Analysis}}},
  author = {Filzmoser, Peter and Gregorich, Mariella},
  year = {2020},
  month = nov,
  journal = {Mathematical Geosciences},
  volume = {52},
  number = {8},
  pages = {1049--1066},
  issn = {1874-8953},
  doi = {10.1007/s11004-020-09861-6},
  urldate = {2024-03-12},
  abstract = {Outliers are encountered in all practical situations of data analysis, regardless of the discipline of application. However, the term outlier is not uniformly defined across all these fields since the differentiation between regular and irregular behaviour is naturally embedded in the subject area under consideration. Generalized approaches for outlier identification have to be modified to allow the diligent search for potential outliers. Therefore, an overview of different techniques for multivariate outlier detection is presented within the scope of selected kinds of data frequently found in the field of geosciences. In particular, three common types of data in geological studies are explored: spatial, compositional and flat data. All of these formats motivate new outlier concepts, such as local outlyingness, where the spatial information of the data is used to define a neighbourhood structure. Another type are compositional data, which nicely illustrate the fact that some kinds of data require not only adaptations to standard outlier approaches, but also transformations of the data itself before conducting the outlier search. Finally, the very recently developed concept of cellwise outlyingness, typically used for high-dimensional data, allows one to identify atypical cells in a data matrix. In practice, the different data formats can be mixed, and it is demonstrated in various examples how to proceed in such situations.},
  langid = {english},
  keywords = {Cellwise outliers,Compositional data analysis,Local outlyingness,Multivariate outlier detection,Robust statistics},
  file = {D:\03 UofA\06 Reading\_zotfile\Filzmoser_Gregorich\filzmoser2020multivariate.pdf}
}

@techreport{fiore2021,
  title = {{{NI}} 43-101 Updated Technical Report on Resources and Reserves Pan Gold Project White Pine County, Nevada},
  author = {{Fiore Gold Ltd.}},
  year = {2021},
  keywords = {thesis_02}
}

@inproceedings{fisher1928limiting,
  title = {Limiting Forms of the Frequency Distribution of the Largest or Smallest Member of a Sample},
  booktitle = {Mathematical Proceedings of the {{Cambridge}} Philosophical Society},
  author = {Fisher, Ronald Aylmer and Tippett, Leonard Henry Caleb},
  year = {1928},
  volume = {24},
  pages = {180--190},
  publisher = {Cambridge University Press},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Fisher_Tippett\fisher1928limiting.pdf}
}

@misc{fisher2019all,
  title = {All {{Models}} Are {{Wrong}}, but {{Many}} Are {{Useful}}: {{Learning}} a {{Variable}}'s {{Importance}} by {{Studying}} an {{Entire Class}} of {{Prediction Models Simultaneously}}},
  shorttitle = {All {{Models}} Are {{Wrong}}, but {{Many}} Are {{Useful}}},
  author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
  year = {2019},
  month = dec,
  number = {arXiv:1801.01489},
  eprint = {1801.01489},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.01489},
  urldate = {2023-08-10},
  abstract = {Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model \$f({\textbackslash}mathbf\{x\})={\textbackslash}mathbf\{x\}\^{}\{T\}{\textbackslash}beta\$ with a fixed coefficient vector \${\textbackslash}beta\$) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Fisher et al\\fisher2019all.pdf;C\:\\Users\\benha\\Zotero\\storage\\48WPPFJ8\\1801.html}
}

@article{fourie2019limiting,
  title = {Limiting the Influence of Extreme Grades in Ordinary Kriged Estimates},
  author = {Fourie, A. and Morgan, C. and Minnitt, R.C.A.},
  year = {2019},
  journal = {Journal of the Southern African Institute of Mining and Metallurgy},
  volume = {119},
  number = {4},
  issn = {22256253, 24119717},
  doi = {10.17159/2411-9717/18/090/2019},
  urldate = {2021-10-14},
  langid = {english},
  file = {C:\Users\benha\Zotero\storage\HAKN5DWN\Fourie et al. - 2019 - Limiting the influence of extreme grades in ordina.pdf}
}

@article{frechet1927loi,
  title = {Sur La Loi de Probabilit{\'e} de l'{\'e}cart Maximum},
  author = {Fr{\'e}chet, Maurice},
  year = {1927},
  journal = {Ann. Soc. Math. Polon.},
  volume = {6},
  pages = {93--116}
}

@book{fu2003distribution,
  title = {Distribution {{Theory}} of {{Runs}} and {{Patterns}} and {{Its Applications}}: {{A Finite Markov Chain Imbedding Approach}}},
  shorttitle = {Distribution {{Theory}} of {{Runs}} and {{Patterns}} and {{Its Applications}}},
  author = {Fu, James C and Lou, W Y Wendy},
  year = {2003},
  month = jul,
  publisher = {WORLD SCIENTIFIC},
  doi = {10.1142/4669},
  urldate = {2022-04-27},
  isbn = {978-981-02-4587-0 978-981-277-920-5},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Fu_Lou\fu2003distribution.pdf}
}

@article{geman1984stochastic,
  title = {Stochastic {{Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}},
  author = {Geman, Stuart and Geman, Donald},
  year = {1984},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-6},
  number = {6},
  pages = {721--741},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1984.4767596},
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  keywords = {Additive noise,Annealing,Bayesian methods,Deformable models,Degradation,Energy states,Gibbs distribution,image restoration,Image restoration,line process,MAP estimate,Markov random field,Markov random fields,relaxation,scene modeling,spatial degradation,Stochastic processes,Temperature distribution,thesis_05},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Geman_Geman\\geman1984stochastic.pdf;C\:\\Users\\benha\\Zotero\\storage\\YY8YSXS4\\4767596.html}
}

@article{georgioudakis2020comparative,
  title = {A {{Comparative Study}} of {{Differential Evolution Variants}} in {{Constrained Structural Optimization}}},
  author = {Georgioudakis, Manolis and Plevris, Vagelis},
  year = {2020},
  journal = {Frontiers in Built Environment},
  volume = {6},
  issn = {2297-3362},
  urldate = {2023-08-08},
  abstract = {Differential evolution (DE) is a population-based metaheuristic search algorithm that optimizes a problem by iteratively improving a candidate solution based on an evolutionary process. Such algorithms make few or no assumptions about the underlying optimization problem and can quickly explore very large design spaces. DE is arguably one of the most versatile and stable population-based search algorithms that exhibits robustness to multi-modal problems. In the field of structural engineering, most practical optimization problems are associated with one or several behavioral constraints. Constrained optimization problems are quite challenging to solve due to their complexity and high nonlinearity. In this work we examine the performance of several DE variants, namely the standard DE, the composite DE (CODE), the adaptive DE with optional external archive (JADE) and the self-adaptive DE (JDE and SADE), for handling constrained structural optimization problems associated with truss structures. The performance of each DE variant is evaluated by using five well-known benchmark structures in 2D and 3D. The evaluation is done on the basis of final optimum result and the rate of convergence. Valuable conclusions are obtained from the statistical analysis which can help a structural engineer in practice to choose the suitable algorithm for such kind of problems.},
  file = {D:\03 UofA\06 Reading\_zotfile\Georgioudakis_Plevris\georgioudakis2020comparative.pdf}
}

@book{geron2019hands,
  title = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}: {{Concepts}}, Tools, and Techniques to Build Intelligent Systems},
  author = {G{\'e}ron, Aur{\'e}lien},
  year = {2019},
  publisher = {" O'Reilly Media, Inc."}
}

@article{ghorbanidehno2020recent,
  title = {Recent Developments in Fast and Scalable Inverse Modeling and Data Assimilation Methods in Hydrology},
  author = {Ghorbanidehno, Hojat and Kokkinaki, Amalia and Lee, Jonghyun and Darve, Eric},
  year = {2020},
  month = dec,
  journal = {Journal of Hydrology},
  volume = {591},
  pages = {125266},
  issn = {0022-1694},
  doi = {10.1016/j.jhydrol.2020.125266},
  urldate = {2024-04-30},
  abstract = {The last twenty years have brought significant advances in hydrology and hydrogeology, especially in the area of data availability and predictive modeling capabilities. Remote sensing, imaging technology and in situ continuous monitoring devices have allowed the collection of large and complex datasets. At the same time, the computational power, efficiency and parallelization capabilities of numerical models have been constantly increasing, making possible simulations at larger scales and finer resolutions. Harnessing these new capabilities can result in better characterization of complex heterogeneous systems through inverse modeling, and better accuracy in predicting the behavior of dynamic systems, through data assimilation. Keeping up with developments in data collection and simulation capabilities, significant research has been conducted on developing computationally efficient inverse modeling and data assimilation algorithms. These new algorithms need to be able to handle expensive linear algebra computations, excessive data storage needs, and large numbers of repeated numerical simulations. The algorithms that have been developed aim to achieve a reasonable trade-off between computational cost and estimation accuracy. Two of the most common techniques used to handle this trade-off are ensemble-based methods and eigenspectrum-based compression of covariances. Both of these approaches can be combined with fast linear algebra techniques, further improving their computational efficiency. This focused review discusses computationally efficient inverse modeling and data assimilation methods developed in the past two decades in the context of their predecessor algorithms, outlining their similarities and differences and the applications they have been used for. By presenting a conceptual map of approaches that have been used to reduce inverse modeling and data assimilation costs, we hope to increase the visibility and understanding of these computational tools, as they are becoming more available to the broader scientific community and a bigger part of data-driven decision making in hydrological and hydrogeological applications.},
  file = {C:\Users\benha\Zotero\storage\VWT6Q9QU\S0022169420307265.html}
}

@article{gilli2006application,
  title = {An Application of Extreme Value Theory for Measuring Financial Risk},
  author = {Gilli, Manfred and Kellezi, Evis},
  year = {2006},
  journal = {Computational Economics},
  volume = {27},
  number = {2},
  pages = {207--228},
  publisher = {Springer},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Gilli_others\gilli2006application.pdf}
}

@article{giraud2019integration,
  title = {Integration of Geoscientific Uncertainty into Geophysical Inversion by Means of Local Gradient Regularization},
  author = {Giraud, Jeremie and Lindsay, Mark and Ogarko, Vitaliy and Jessell, Mark and Martin, Roland and {Pakyuz-Charrier}, Evren},
  year = {2019},
  month = jan,
  journal = {Solid Earth},
  volume = {10},
  number = {1},
  pages = {193--210},
  publisher = {Copernicus GmbH},
  issn = {1869-9510},
  doi = {10.5194/se-10-193-2019},
  urldate = {2024-04-10},
  abstract = {We introduce a workflow integrating geological modelling uncertainty information to constrain gravity inversions. We test and apply this approach to the Yerrida Basin (Western Australia), where we focus on prospective greenstone belts beneath sedimentary cover. Geological uncertainty information is extracted from the results of a probabilistic geological modelling process using geological field data and their inferred accuracy as inputs. The uncertainty information is utilized to locally adjust the weights of a minimum-structure gradient-based regularization function constraining geophysical inversion. Our results demonstrate that this technique allows geophysical inversion to update the model preferentially in geologically less certain areas. It also indicates that inverted models are consistent with both the probabilistic geological model and geophysical data of the area, reducing interpretation uncertainty. The interpretation of inverted models reveals that the recovered greenstone belts may be shallower and thinner than previously thought.},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Giraud et al\giraud2019integration.pdf}
}

@article{gnedenko1943distribution,
  title = {Sur {{La Distribution Limite Du Terme Maximum D}}'{{Une Serie Aleatoire}}},
  author = {Gnedenko, B.},
  year = {1943},
  journal = {Annals of Mathematics},
  volume = {44},
  number = {3},
  eprint = {1968974},
  eprinttype = {jstor},
  pages = {423--453},
  publisher = {Annals of Mathematics},
  issn = {0003-486X},
  doi = {10.2307/1968974},
  urldate = {2021-10-28},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Gnedenko\gnedenko1943distribution.pdf}
}

@incollection{gomez-hernandez1993joint,
  title = {Joint {{Sequential Simulation}} of {{MultiGaussian Fields}}},
  booktitle = {Geostatistics {{Tr{\'o}ia}} '92: {{Volume}} 1},
  author = {{G{\'o}mez-Hern{\'a}ndez}, J. Jaime and Journel, Andr{\'e} G.},
  editor = {Soares, Amilcar},
  year = {1993},
  series = {Quantitative {{Geology}} and {{Geostatistics}}},
  pages = {85--94},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-011-1739-5_8},
  urldate = {2023-08-17},
  abstract = {The sequential simulation algorithm can be used for the generation of conditional realizations from either a multiGaussian random function or any non-Gaussian random function as long as its conditional distributions can be derived. The multivariate probability density function (pdf) that fully describes a random function can be written as the product of a set of univariate conditional pdfs. Drawing realizations from the multivariate pdf amounts to drawing sequentially from that series of univariate conditional pdfs. Similarly, the joint multivariate pdf of several random functions can be written as the product of a series of univariate conditional pdfs. The key step consists of the derivation of the conditional pdfs. In the case of a multiGaussian fields, these univariate conditional pdfs are known to be Gaussian with mean and variance given by the solution of a set of normal equations also known as simple cokriging equations. Sequential simulation is preferred to other techniques, such as turning bands, because of its ease of use and extreme flexibility.},
  isbn = {978-94-011-1739-5},
  langid = {english},
  keywords = {Conditional Distribution,Conditioning Data,Exponential Type,Search Neighborhood,Sequential Simulation},
  file = {D:\03 UofA\06 Reading\_zotfile\Gómez-Hernández_Journel\gomez-hernandez1993joint.pdf}
}

@article{gomez-hernandez1998be,
  title = {To Be or Not to Be Multi-{{Gaussian}}? {{A}} Reflection on Stochastic Hydrogeology},
  shorttitle = {To Be or Not to Be Multi-{{Gaussian}}?},
  author = {{G{\'o}mez-Hern{\'a}ndez}, J. Jaime and Wen, Xian-Huan},
  year = {1998},
  month = feb,
  journal = {Advances in Water Resources},
  volume = {21},
  number = {1},
  pages = {47--61},
  issn = {0309-1708},
  doi = {10.1016/S0309-1708(96)00031-0},
  urldate = {2024-02-14},
  abstract = {The multivariate Gaussian random function model is commonly used in stochastic hydrogeology to model spatial variability of log-conductivity. The multi-Gaussian model is attractive because it is fully characterized by an expected value and a covariance function or matrix, hence its mathematical simplicity and easy inference. Field data may support a Gaussian univariate distribution for log hydraulic conductivity, but, in general, there are not enough field data to support a multi-Gaussian distribution. A univariate Gaussian distribution does not imply a multi-Gaussian model. In fact, many multivariate models can share the same Gaussian histogram and covariance function, yet differ by their patterns of spatial continuity at different threshold values. Hence the decision to use a multi-Gaussian model to represent the uncertainty associated with the spatial heterogeneity of log-conductivity is not databased. Of greatest concern is the fact that a multi-Gaussian model implies the minimal spatial correlation of extreme values, a feature critical for mass transport and a feature that may be in contradiction with some geological settings, e.g. channeling. The possibility for high conductivity values to be spatially correlated should not be discarded by adopting a congenial model just because data shortage prevents refuting it. In this study, three alternatives to a multi-Gaussian model, all sharing the same Gaussian histogram and the same covariance function, but with different continuity patterns for extreme values, were considered to model the spatial variability of log-conductivity. The three alternative models, plus the traditional multi-Gaussian model, are used to perform Monte Carlo analyses of groundwater travel times from a hypothetical nuclear repository to the ground surface through a synthetic formation similar to the Finnsj{\"o}n site in Sweden. The results show that the groundwater travel times predicted by the multi-Gaussian model could be ten times slower than those predicted by the other models. The probabilities of very short travel times could be severely underestimated using the multi-Gaussian model. Consequently, if field measured data are not sufficient to determine the higher-order moments necessary to validate the multi-Gaussian model --- which is the usual situation in practice --- other alternative models to the multi-Gaussian one ought to be considered.},
  keywords = {geostatistics,Heterogeneity,mass transport,Monte Carlo simulation,non-multi-Gaussian,risk analysis,stochastic simulation,travel time,uncertainty},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Gómez-Hernández_Wen\\gomez-hernandez1998be.pdf;C\:\\Users\\benha\\Zotero\\storage\\RK8KZZ6J\\S0309170896000310.html}
}

@article{gomez-hernandez2021one,
  title = {One {{Step}} at a {{Time}}: {{The Origins}} of {{Sequential Simulation}} and {{Beyond}}},
  shorttitle = {One {{Step}} at a {{Time}}},
  author = {{G{\'o}mez-Hern{\'a}ndez}, J. Jaime and Srivastava, R. Mohan},
  year = {2021},
  month = feb,
  journal = {Mathematical Geosciences},
  volume = {53},
  number = {2},
  pages = {193--209},
  issn = {1874-8953},
  doi = {10.1007/s11004-021-09926-0},
  urldate = {2022-08-30},
  abstract = {In the mid-1980s, still in his young 40s, Andr{\'e} Journel was already recognized as one of the giants of geostatistics. Many of the contributions from his new research program at Stanford University had centered around the indicator methods that he developed: indicator kriging and multiple indicator kriging. But when his second crop of graduate students arrived at Stanford, indicator methods still lacked an approach to conditional simulation that was not tainted by what Andr{\'e} called the `Gaussian disease'; early indicator simulations went through the tortuous path of converting all indicators to Gaussian variables, running a turning bands simulation, and truncating the resulting multi-Gaussian realizations. When he conceived of sequential indicator simulation (SIS), even Andr{\'e} likely did not recognize the generality of an approach to simulation that tackled the simulation task one step at a time. The early enthusiasm for SIS was its ability, in its multiple-indicator form, to cure the Gaussian disease and to build realizations in which spatial continuity did not deteriorate in the extreme values. Much of Stanford's work in the 1980s focused on petroleum geostatistics, where extreme values (the high-permeability fracture zones and the low-permeability shale barriers) have much stronger anisotropy, and much longer ranges of correlation in the maximum continuity direction, than mid-range values. With multi-Gaussian simulations necessarily imparting weaker continuity to the extremes, SIS was an important breakthrough. The generality of the sequential approach was soon recognized, first through its analogy with multi-variate unconditional simulation achieved using the lower triangular matrix of an LU decomposition of the covariance matrix as the multiplier of random normal deviates. Modifying LU simulation so that it became conditional gave rise to sequential Gaussian simulation (SGS), an algorithm that shared much in common with SIS. With nagging implementation details like the sequential path and the search neighborhood being common to both methods, improvements in either SIS or SGS often became improvements to the other. Almost half of the contributors to this Special Issue became students of Andr{\'e} in the classes of 1984--1988, and several are the pioneers of SIS and SGS. Others who studied later with Andr{\'e} explored and developed the first multipoint statistics simulation procedures, which are based on the same concept that underlies sequential simulation. Among his many significant intellectual accomplishments, one of the cornerstones of Andr{\'e} Journel's legacy was sequential simulation, built one step at a time.},
  langid = {english},
  keywords = {Large grids,Random functions,Stochastic processes},
  file = {D:\03 UofA\06 Reading\_zotfile\Gómez-Hernández_Srivastava\gomez-hernandez2021one.pdf}
}

@article{goovaerts1992factorial,
  title = {Factorial Kriging Analysis: A Useful Tool for Exploring the Structure of Multivariate Spatial Soil Information},
  shorttitle = {Factorial Kriging Analysis},
  author = {Goovaerts, P.},
  year = {1992},
  journal = {Journal of Soil Science},
  volume = {43},
  number = {4},
  pages = {597--619},
  issn = {1365-2389},
  doi = {10.1111/j.1365-2389.1992.tb00163.x},
  urldate = {2021-10-16},
  abstract = {Most studies of relations between soil properties fail to take account of their regionalized nature because of the lack of appropriate methods. This paper describes a geostatistical technique, factorial kriging analysis, that bridges the gap between classical multivariate analysis and a univariate geostatistical approach. The basic feature of the method is the fitting of a linear model of coregionalization, i.e. all experimental simple and cross-variograms are modelled with a linear combination of basic variogram functions. A particular variance-covariance matrix, the coregionalization matrix, can then be associated with each spatial scale defined by the range of the basic variogram function. Each coregionalization matrix describes relationships between variables at a given spatial scale. A principal component analysis of these matrices produces a set of components, the regionalized factors, that reflect the main features of the multivariate information for each spatial scale and whose scores are estimated by cokriging. The technique is described and illustrated with three case studies based on a simulated data set and soil survey data. The results are compared with those of the principal component analysis of the variance-covariance matrix and the variogram matrices.},
  langid = {english},
  keywords = {thesis_05},
  file = {D\:\\03 UofA\\04 Research\\02 PhD\\_zotfile\\Goovaerts\\goovaerts1992factorial.pdf;C\:\\Users\\benha\\Zotero\\storage\\RI54W7CF\\j.1365-2389.1992.tb00163.html}
}

@book{goovaerts1997geostatistics,
  title = {Geostatistics for Natural Resources Evaluation},
  author = {Goovaerts, Pierre},
  year = {1997},
  publisher = {Oxford University Press on Demand},
  file = {C:\Users\benha\Zotero\storage\6WBXREIM\books.html}
}

@article{grana2022probabilistic,
  title = {Probabilistic Inversion of Seismic Data for Reservoir Petrophysical Characterization: {{Review}} and Examples},
  shorttitle = {Probabilistic Inversion of Seismic Data for Reservoir Petrophysical Characterization},
  author = {Grana, Dario and Azevedo, Leonardo and {de Figueiredo}, Leandro and Connolly, Patrick and Mukerji, Tapan},
  year = {2022},
  month = jul,
  journal = {Geophysics},
  volume = {87},
  number = {5},
  pages = {M199-M216},
  issn = {0016-8033},
  doi = {10.1190/geo2021-0776.1},
  urldate = {2024-04-30},
  abstract = {The physics that describes the seismic response of an interval of saturated porous rocks with known petrophysical properties is relatively well understood and includes rock physics, petrophysics, and wave propagation models. The main goal of seismic reservoir characterization is to predict the rock and fluid properties given a set of seismic measurements by combining geophysical models and mathematical methods. This modeling challenge is generally formulated as an inverse problem. The most common geophysical inverse problem is the seismic (or elastic) inversion, i.e.,~the estimation of elastic properties, such as seismic velocities or impedances, from seismic amplitudes and traveltimes. The estimation of petrophysical properties, such as porosity, lithology, and fluid saturations, also can be formulated as an inverse problem and is generally referred to as rock-physics (or petrophysical) inversion. Several deterministic and probabilistic methods can be applied to solve seismic inversion problems. Deterministic algorithms predict a single solution, which is a ``best'' estimate or the most likely value of the model variables of interest. In probabilistic algorithms, on the other hand, the solution is the probability distribution of the model variables of interest, which can be expressed as a conditional probability density function or a set of model realizations conditioned on the data. The probabilistic approach provides a quantification of the uncertainty of the solution in addition to the most likely model. Our goal is to define the terminology, present an overview of probabilistic seismic and rock-physics inversion methods for the estimation of petrophysical properties, demonstrate the fundamental concepts with illustrative examples, and discuss the recent research developments.},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Grana et al\\grana2022probabilistic.pdf;C\:\\Users\\benha\\Zotero\\storage\\4EMME5GY\\Probabilistic-inversion-of-seismic-data-for.html}
}

@incollection{guardiano1993multivariate,
  title = {Multivariate {{Geostatistics}}: {{Beyond Bivariate Moments}}},
  shorttitle = {Multivariate {{Geostatistics}}},
  booktitle = {Geostatistics {{Tr{\'o}ia}} '92},
  author = {Guardiano, Felipe B. and Srivastava, R. Mohan},
  editor = {Gradstein, F. M. and Soares, Amilcar},
  year = {1993},
  volume = {5},
  pages = {133--144},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-011-1739-5_12},
  urldate = {2022-05-26},
  isbn = {978-0-7923-2157-6 978-94-011-1739-5},
  langid = {english},
  file = {C:\Users\benha\Zotero\storage\Z8UQKRWQ\Guardiano and Srivastava - 1993 - Multivariate Geostatistics Beyond Bivariate Momen.pdf}
}

@book{gumbel1958statistics,
  title = {Statistics of Extremes},
  author = {Gumbel, Emil Julius},
  year = {1958},
  publisher = {Courier Corporation},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Gumbel\gumbel1958statistics.pdf}
}

@phdthesis{guthke2013non,
  title = {Non-Multi-{{Gaussian}} Spatial Structures: Process-Driven Natural Genesis, Manifestation, Modeling Approaches, and Influences on Dependent Processes},
  author = {Guthke, Philipp},
  year = {2013},
  file = {D:\03 UofA\06 Reading\_zotfile\Guthke\guthke2013non.pdf}
}

@article{guthke2017link,
  title = {On the Link between Natural Emergence and Manifestation of a Fundamental Non-{{Gaussian}} Geostatistical Property: {{Asymmetry}}},
  shorttitle = {On the Link between Natural Emergence and Manifestation of a Fundamental Non-{{Gaussian}} Geostatistical Property},
  author = {Guthke, Philipp and B{\'a}rdossy, Andr{\'a}s},
  year = {2017},
  month = may,
  journal = {Spatial Statistics},
  volume = {20},
  pages = {1--29},
  issn = {2211-6753},
  doi = {10.1016/j.spasta.2017.01.003},
  urldate = {2024-02-14},
  abstract = {The geostatistical workflow of data analysis, model fitting, and subsequent interpolation or simulation has recently been enhanced by several methods. These methods can be summarized under the terms 'rank-order geostatistics', for the empirical analysis part of the workflow, and 'copula geostatistics', for the theoretical foundation and the modeling (interpolation or simulation). Besides taking into account non-Gaussianity, the main advantage of this alternative way to treat geostatistical problems is the descriptive power and standardized interpretability. This paper addresses the empirical analysis part of the workflow. We investigate the interplay between structural features, statistical properties, and underlying dynamic processes of a realization of a spatial field. (1) In the first part of this paper we recapitulate and consolidate the advances in the empirical analysis part of the geostatistical workflow and put them into context of 'classical' geostatistics. We place particular emphasis on the theoretical foundation of so called asymmetry functions, because in our opinion they are the necessary first step away from Gaussian geostatistics. (2) In the second part, we rigorously analyze how specific types of structural features are related to the asymmetry of a spatial field. In the geostatistical tradition of interpreting the shape of variograms or correlation functions, we give examples of how to interpret different shapes of the asymmetry function. (3) We subsequently report how dynamic processes naturally lead to the emergence of asymmetrical (non-Gaussian) spatial structures. To demonstrate these findings, we investigate the manifestation of different spatial data sets (land surface elevation, groundwater contamination, grades of an undergraduate exam) and relate the non-Gaussian structural features to the underlying dynamic processes. Numerical process models are utilized to manifest evidence of how realistic processes naturally lead to complex non-Gaussian structures. The key purpose of this paper is to show that asymmetry is a fundamental geostatistical property and a result of different kinds of spatial processes.},
  keywords = {Asymmetry function,Copula geostatistics,Manifestation,Non-multi-Gaussian,Rank-order geostatistics,Spatial process},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Guthke_Bárdossy\\guthke2017link.pdf;C\:\\Users\\benha\\Zotero\\storage\\7USWKD8Y\\S2211675317300258.html}
}

@article{hadavand2023spatial,
  title = {Spatial Multivariate Data Imputation Using Deep Learning and Lambda Distribution},
  author = {Hadavand, Mostafa and Deutsch, Clayton V.},
  year = {2023},
  month = aug,
  journal = {Computers \& Geosciences},
  volume = {177},
  pages = {105376},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2023.105376},
  urldate = {2024-02-14},
  abstract = {Artificial neural networks (ANNs) are often used to establish a mapping between an input data set and a corresponding output. There are many applications that rely on quantifying the conditional distribution of the output given the input data set. This is often referred to as aleatoric uncertainty associated with variability of the outcome due to inherently random effects. In this paper, deep learning is used to quantify moments of the conditional distribution of a missing variable based on homotopic multivariate observations. The lambda distribution is then used to parametrize the conditional distribution based on the provided moments. Geostatistical quantification of spatial continuity complements the multivariate conditional distribution through Bayesian updating to inform multiple data imputation that accounts for the uncertainty associated with the missing variable(s). Geological data are often incomplete, and data imputation is an essential step to avoid excluding heterotopic data. The proposed data imputation framework trains multi layer perceptron (MLP) neural networks to characterize multivariate relationships inferred from homotopic training data. A case study is conducted using geological data from a lateritic Nickle deposit to demonstrate application of the proposed methodology.},
  keywords = {Gaussian mixture model,Geostatistics,Simulation},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Hadavand_Deutsch\\hadavand2023spatial.pdf;C\:\\Users\\benha\\Zotero\\storage\\ZEKJPSLQ\\S0098300423000808.html}
}

@article{harding2023probabilistic,
  title = {Probabilistic {{Modeling}} of the {{Round Mountain Gold Deposit}}: A {{Case Study}}},
  shorttitle = {Probabilistic {{Modeling}} of the {{Round Mountain Gold Deposit}}},
  author = {Harding, Ben and Lagos, Rodolfo and Pfeiffer, Nicos and Deutsch, Clayton V.},
  year = {2023},
  month = oct,
  journal = {Mining, Metallurgy \& Exploration},
  volume = {40},
  number = {5},
  pages = {1987--2006},
  issn = {2524-3470},
  doi = {10.1007/s42461-023-00787-1},
  urldate = {2024-04-23},
  abstract = {This paper documents a probabilistic resource modeling workflow for the Round Mountain epithermal gold deposit. The set of categorical and continuous variable simulated realizations provides a model of variability and uncertainty given the current orebody knowledge. These realizations can be used to quantify and understand expected deviations in reconciliation during mining and provide a more accurate measure of expected gold than an estimated model. Gold mineralization at Round Mountain is primarily contained within tertiary tuff units, which dip gently to the west and thicken towards the southern part of the orebody. Domain boundaries are modeled as combinations of lithology and alteration; alteration domains are constrained to be within the tuffs. Large-scale geologic uncertainty is strongly influenced by parameter uncertainty, particularly the input histogram. The spatial bootstrap characterizes prior global uncertainty in the input histogram for categorical and continuous variables. Categorical models are generated using hierarchical truncated pluri-Gaussian (HTPG) simulation. Continuous variable simulation is carried out within the simulated domain boundaries using trend models to account for non-stationarity.},
  langid = {english},
  keywords = {Gaussian simulation,Geostatistics,Hierarchical truncated pluri-Gaussian,Resources},
  file = {D:\03 UofA\06 Reading\_zotfile\Harding et al\harding2023probabilistic.pdf}
}

@article{harris2014multivariate,
  title = {Multivariate Spatial Outlier Detection Using Robust Geographically Weighted Methods},
  author = {Harris, Paul and Brunsdon, Chris and Charlton, Martin and Juggins, Steve and Clarke, Annemarie},
  year = {2014},
  journal = {Mathematical Geosciences},
  volume = {46},
  number = {1},
  pages = {1--31},
  publisher = {Springer},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Harris et al\harris2014multivariate.pdf}
}

@article{hawkins1984robust,
  title = {Robust Kriging---{{A}} Proposal},
  author = {Hawkins, Douglas M. and Cressie, Noel},
  year = {1984},
  month = jan,
  journal = {Journal of the International Association for Mathematical Geology},
  volume = {16},
  number = {1},
  pages = {3--18},
  issn = {1573-8868},
  doi = {10.1007/BF01036237},
  urldate = {2023-11-15},
  abstract = {Geological data frequently have a heavy-tailed normal-in-the-middle distribution, which gives rise to grade distributions that appear to be normal except for the occurrence of a few outliers. This same situation also applies to log-transformed data to which lognormal kriging is to be applied. For such data, linear kriging is nonrobust in that (1)kriged estimates tend to infinity as the outliers do, and (2)it is also not minimum mean squared error. The more general nonlinear method of disjunctive kriging is even more nonrobust, computationally more laborious, and in the end need not produce better practical answers. We propose a robust kriging method for such nearly normal data based on linear kriging of an editing of the data. It is little more laborious than conventional linear kriging and, used in conjunction with a robust estimator of the variogram, provides good protection against the effects of data outliers. The method is also applicable to time series analysis.},
  langid = {english},
  keywords = {Geostatistics,kriging,robust estimation,time series},
  file = {D:\03 UofA\06 Reading\_zotfile\Hawkins_Cressie\hawkins1984robust.pdf}
}

@inproceedings{hazan2012extreme,
  title = {Extreme Value Statistics for Vibration Spectra Outlier Detection},
  booktitle = {International Conference on Condition Monitoring and Machinery Failure Prevention Technologies},
  author = {Hazan, Aur{\'e}lien and Lacaille, J{\'e}r{\^o}me and Madani, Kurosh},
  year = {2012},
  pages = {p--1},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Hazan et al\hazan2012extreme.pdf}
}

@article{hodge2004survey,
  title = {A Survey of Outlier Detection Methodologies},
  author = {Hodge, Victoria and Austin, Jim},
  year = {2004},
  journal = {Artificial intelligence review},
  volume = {22},
  number = {2},
  pages = {85--126},
  publisher = {Springer},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Hodge_Austin\hodge2004survey.pdf}
}

@article{holland1992genetic,
  title = {Genetic {{Algorithms}}},
  author = {Holland, John H.},
  year = {1992},
  journal = {Scientific American},
  volume = {267},
  number = {1},
  eprint = {24939139},
  eprinttype = {jstor},
  pages = {66--73},
  publisher = {Scientific American, a division of Nature America, Inc.},
  issn = {0036-8733},
  urldate = {2024-04-30},
  file = {D:\03 UofA\06 Reading\_zotfile\Holland\holland1992genetic.pdf}
}

@article{hong2007improved,
  title = {Improved {{Factorial Kriging}} for {{Feature Identification}} and {{Extraction}}},
  author = {Hong, Sahyun and Deutsch, Clayton V},
  year = {2007},
  pages = {16},
  langid = {english},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Hong_Deutsch\hong2007improved.pdf}
}

@phdthesis{isaaks1990application,
  title = {The Application of {{Monte Carlo}} Methods to the Analysis of Spatially Correlated Data},
  author = {Isaaks, Edward Harold},
  year = {1990},
  address = {United States -- California},
  urldate = {2021-10-22},
  abstract = {Monte Carlo methods based on multiple equiprobable conditional simulations can be particularly informative in assessing the impact of spatial heterogeneities. For example, multiple equiprobable simulations, conditional to sample data can be generated using a stochastic imaging or conditional simulation algorithm. Consider a logical equivalent of some real system or operation: whereas the input to the real system is the spatially distributed attribute, the input to the logical equivalent of that system is the conditional simulation of that attribute. By processing a number of equiprobable conditional simulations through the logical equivalent of the real system, an equivalent number of responses are obtained, i.e. a response distribution. The response distribution provides a quantitative measure of the uncertainty in the output associated with that of the input. However, to date the generation of multiple conditional simulations has required large main frame computers and thus such methods using multiple conditional simulations are not widely used, for example, by the mining industry. To address this problem a C computer program, GSIM3D.C, has been developed to enable the generation of multiple conditional simulations using a 80386 DOS based personal computer. This program enables the generation of multiple conditional simulations sufficiently large for practical applications within a reasonable period of time. With the capability of this program, Monte Carlo methods are applied directly to the problems of change of support and the misclassification of selective mining units (SMU) at the time of mining (grade control). The classification of SMU at the time of mining is posed as an optimization problem where the objective function is to minimize the dollars lost due to classification errors. This approach is demonstrated through a case study based on an exhaustive data set. A typical mining scenario is synthesized using realistic recovery factors, costs, and 195 blast hole assays drawn from the exhaustive data set. The grade of each SMU is characterized using a conditional probability distribution obtained from multiple conditional simulations. The dollar loss associated with each possible classification error is established as a function of the unknown SMU grade. The SMU are then classified as either waste, run of the mine heap leach, agglomeration, or milling ore, according to the classification providing the minimum expected dollar loss. A comparison is made between the optimally recovered value (\$) and the value recovered using ordinary kriging. Optimized grade control is shown to increase the net revenue by more than 7{\textbackslash}\% over that obtained using ordinary kriging.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  langid = {english},
  school = {Stanford University},
  keywords = {conditional simulation,Earth sciences,grade control,Pure sciences,stochastic imaging},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Isaaks\isaaks1990application2.pdf}
}

@article{jha2022comparative,
  title = {A Comparative Study on Outlier Detection Techniques for Noisy Production Data from Unconventional Shale Reservoirs},
  author = {Jha, H. S. and Khanal, A. and Seikh, H. M. D. and Lee, W. J.},
  year = {2022},
  month = sep,
  journal = {Journal of Natural Gas Science and Engineering},
  volume = {105},
  pages = {104720},
  issn = {1875-5100},
  doi = {10.1016/j.jngse.2022.104720},
  urldate = {2023-11-20},
  abstract = {Decline curve analyses (DCA) and rate transient analyses (RTA) are widely used to characterize the fluid flow through porous media and forecast future production. Oil and gas production data are routinely analyzed for history matching and optimizing the well stimulation methods in hydrocarbon exploration and production lifecycle. However, outliers add significant uncertainty and non-uniqueness to results from production data analysis. This study provides a structured and comprehensive overview of five widely used outlier detection (OD) techniques for identifying and removing outliers in production data. Each OD technique measures deviation differently and, therefore, has a different outcome even when applied to the same dataset, creating the need to test several methods and find the optimal technique for identifying and removing outliers from production data. First, we generated production data from a typical multi-fractured horizontal well using a numerical reservoir simulator and added random noise to the data. Then, we used five different OD techniques to identify the prelabeled outliers from the synthetic production data. Finally, we identified the best-performing OD algorithm by comparing the various evaluation metrics such as the mean absolute error (MAE), precision, sensitivity, and F1 score. Results showed that the angle-based OD (ABOD) had the best MAE, precision, sensitivity, and F1 score of 8\%, 85\%, 98\%, and 0.90, respectively. The next best-performing OD technique was distance-based OD (DBOD), with MAE, precision, sensitivity, and F1 score of 16\%, 71\%, 100\%, and 0.83, respectively. We tested the ABOD method on several field production datasets by assuming different outlier thresholds (fraction of the data points likely to be outliers). Visual inspection of the processed data showed that the ABOD method effectively identified and removed the outliers from a relatively clean dataset (outlier threshold of 20\%) and a highly noisy dataset (outlier threshold of 80\%). This algorithm is intuitive and can effectively identify and remove outliers from the field production data to improve production forecasting, reserves estimation, and rate transient analysis for multi-fractured horizontal oil and gas reservoirs.},
  keywords = {Noisy data,Outlier detection,Production data analysis,Production forecasting,Shale Gas,Unconventional Reservoirs},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Jha et al\\jha2022comparative.pdf;C\:\\Users\\benha\\Zotero\\storage\\854V3YY2\\S1875510022003080.html}
}

@article{journel1974geostatistics,
  title = {Geostatistics for {{Conditional Simulation}} of {{Ore Bodies}}},
  author = {Journel, A. G.},
  year = {1974},
  month = aug,
  journal = {Economic Geology},
  volume = {69},
  number = {5},
  pages = {673--687},
  issn = {0361-0128},
  doi = {10.2113/gsecongeo.69.5.673},
  urldate = {2022-08-30},
  abstract = {Simulation techniques are frequently used to solve various problems of operational research for the mining industry and more generally in earth sciences (hydrogeology, gravimetry, meteorology, etc.). First, the model to be simulated is characterized; for example, the spatial dispersion of grades in an ore body. Then a simulation technique is devised, which must be operational, particularly in terms of computer time. The efficiency of the simulation produced is obviously linked to the capacity of the model to fit the main characteristics of the revealed reality. One of the most important of these characteristics, namely the spatial autocorrelation of variables, is often ignored by the models commonly presented in classical literature.The originality of conditional simulation derives: (1) from the fact that these simulations meet the particular spatial autocorrelation function (covariance or variogram) which characterizes the reality observed; (2) from the conditionalization of the experimental data, i.e., the simulated values at data locations equal the experimental values; and (3) from the possibility of working in real three-dimensional space. The simulation technique proposed (turning-bands method) consists of simulating on lines (one-dimensional space) and then turning these lines in three-dimensional space. This procedure avoids the well-known explosion of computer time and memories involved in classical procedures extended to several dimensions. The originality of using conditional simulation techniques with regard to spectral analysis techniques is presented in the third point.},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Journel\\journel1974geostatistics.pdf;C\:\\Users\\benha\\Zotero\\storage\\DGCFIMF5\\Geostatistics-for-Conditional-Simulation-of-Ore.html}
}

@article{journel1983nonparametric,
  title = {Nonparametric Estimation of Spatial Distributions},
  author = {Journel, A. G.},
  year = {1983},
  month = jun,
  journal = {Journal of the International Association for Mathematical Geology},
  volume = {15},
  number = {3},
  pages = {445--468},
  issn = {1573-8868},
  doi = {10.1007/BF01031292},
  urldate = {2021-10-22},
  abstract = {The indicator approach, whereby the data are used through their rank order, allows a nonparametric approach to the data bivariate distribution. Such rich structural information allows a nonparametric risk-qualified, estimation of local and global spatial distributions.},
  langid = {english},
  keywords = {thesis_02},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Journel\journel1983nonparametric.pdf}
}

@article{journel1989nongaussian,
  title = {Non-{{Gaussian}} Data Expansion in the {{Earth Sciences}}},
  author = {Journel, A. G. and Alabert, F.},
  year = {1989},
  journal = {Terra Nova},
  volume = {1},
  number = {2},
  pages = {123--134},
  issn = {1365-3121},
  doi = {10.1111/j.1365-3121.1989.tb00344.x},
  urldate = {2021-10-22},
  abstract = {A formalism is proposed to generate alternative equiprobable images of an underlying population spatial distribution. The resulting images honour data values at their locations and reflect important characteristics of the data such as patterns of spatial connectivity of extreme-values. The formalism capitalizes on a coding of all information available into bits (0-l), which are then processed all together accounting for their patterns of correlation in space. Such common coding allows accounting for qualitative information, possibly of an interpretative nature, to complement the usually sparse hard data available in Earth Sciences applications. The approach proposed, although of a probabilistic nature, does not call for any Gaussian-type modelling or hypothesis.},
  langid = {english},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Journel_Alabert\\journel1989nongaussian.pdf;C\:\\Users\\benha\\Zotero\\storage\\BCVDMGVT\\j.1365-3121.1989.tb00344.html}
}

@article{journel1993entropy,
  title = {Entropy and Spatial Disorder},
  author = {Journel, Andr{\'e} G. and Deutsch, Clayton V.},
  year = {1993},
  month = apr,
  journal = {Mathematical Geology},
  volume = {25},
  number = {3},
  pages = {329--355},
  issn = {1573-8868},
  doi = {10.1007/BF00901422},
  urldate = {2021-10-22},
  abstract = {The majority of geostatistical estimation and simulation algorithms rely on a covariance model as the sole characteristic of the spatial distribution of the attribute under study. The limitation to a single covariance implicitly calls for a multivariate Gaussian model for either the attribute itself or for its normal scores transform. The Gaussian model could be justified on the basis that it is both analytically simple and it is a maximum entropy model, i.e., a model that minimizes unwarranted structural properties. As a consequence, the Gaussian model also maximizes spatial disorder (beyond the imposed covariance) which can cause flow simulation results performed on multiple stochastic images to be very similar; thus, the space of response uncertainty could be too narrow entailing a misleading sense of safety. The ability of the sole covariance to adequately describe spatial distributions for flow studies, and the assumption that maximum spatial disorder amounts to either no additional information or a safe prior hypothesis are questioned. This paper attempts to clarify the link between entropy and spatial disorder and to provide, through a detailed case study, an appreciation for the impact of entropy of prior random function models on the resulting response distributions.},
  langid = {english},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Journel_Deutsch\journel1993entropy.pdf}
}

@incollection{journel2005covariance,
  title = {Beyond {{Covariance}}: {{The Advent}} of {{Multiple-Point Geostatistics}}},
  shorttitle = {Beyond {{Covariance}}},
  booktitle = {Geostatistics {{Banff}} 2004},
  author = {Journel, Andre G.},
  editor = {Leuangthong, Oy and Deutsch, Clayton V.},
  year = {2005},
  series = {Quantitative {{Geology}} and {{Geostatistics}}},
  pages = {225--233},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-1-4020-3610-1_23},
  urldate = {2022-08-30},
  abstract = {In any estimation or simulation endeavor there are two types of information, the conditioning data typically numerical, most often location-specific, and the structural model which relates deterministically or stochastically the conditioning data to the unknown(s). Adding conditioning data is valuable only inasmuch as the structural model that links them to the unknowns is accurate and reflects data redundancy.},
  isbn = {978-1-4020-3610-1},
  langid = {english},
  keywords = {Conditioning Data,High Order Statistic,Sequential Simulation,Training Image,Variogram Model},
  file = {D:\03 UofA\06 Reading\_zotfile\Journel\journel2005covariance.pdf}
}

@article{kerrou2008issues,
  title = {Issues in Characterizing Heterogeneity and Connectivity in Non-{{multiGaussian}} Media},
  author = {Kerrou, Jaouher and Renard, Philippe and Hendricks Franssen, Harrie-Jan and Lunati, Ivan},
  year = {2008},
  month = jan,
  journal = {Advances in Water Resources},
  volume = {31},
  number = {1},
  pages = {147--159},
  issn = {0309-1708},
  doi = {10.1016/j.advwatres.2007.07.002},
  urldate = {2024-04-11},
  abstract = {The performances of kriging, stochastic simulations and sequential self-calibration inversion are assessed when characterizing a non-multiGaussian synthetic 2D braided channel aquifer. The comparison is based on a series of criteria such as the reproduction of the original reference transmissivity or head fields, but also in terms of accuracy of flow and transport (capture zone) forecasts when the flow conditions are modified. We observe that the errors remain large even for a dense data network. In addition some unexpected behaviours are observed when large transmissivity datasets are used. In particular, we observe an increase of the bias with the number of transmissivity data and an increasing uncertainty with the number of head data. This is interpreted as a consequence of the use of an inadequate multiGaussian stochastic model that is not able to reproduce the connectivity of the original field.},
  keywords = {Aquifer characterization,Connectivity,Inverse,Kriging,Multiple-point statistics,Stochastic simulations,Uncertainty,Well-capture zones},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Kerrou et al\\kerrou2008issues.pdf;C\:\\Users\\benha\\Zotero\\storage\\XLBNEDXZ\\S0309170807001236.html}
}

@inproceedings{lantuejoul2012simulation,
  title = {Simulation of a {{Gaussian}} Random Vector: A Propagative Version of the {{Gibbs}} Sampler},
  booktitle = {The 9th International Geostatistics Congress},
  author = {Lantu{\'e}joul, Christian and Desassis, Nicolas},
  year = {2012},
  pages = {174--181},
  file = {D:\03 UofA\06 Reading\_zotfile\Lantuéjoul_Desassis\lantuejoul2012simulation.pdf}
}

@article{lauzon2020calibration,
  title = {Calibration of Random Fields by a Sequential Spectral Turning Bands Method},
  author = {Lauzon, Dany and Marcotte, Denis},
  year = {2020},
  month = feb,
  journal = {Computers \& Geosciences},
  volume = {135},
  pages = {104390},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2019.104390},
  urldate = {2024-02-13},
  abstract = {A new algorithm for calibration of conditional realizations to measured or desired response functions is presented. The Sequential-Spectral Turning Bands Method (S-STBM) builds the field by choosing the phase of each new cosine function such that the observed field response functions become increasingly calibrated. The phase selection has little influence on the spatial correlation structure but can help to meet other objectives. Conditioning by kriging is used in the algorithm main loop to impose exact hard data reproduction. A first case study illustrates the performance of the algorithm for a cyclic and asymmetric field. S-STBM is shown to reproduce similarly or better the directional asymmetry than calibrated realizations obtained by FFTMA-SA. A training image (TI) with connected low values provides the second case study where the target is the reproduction of non-centered third-order spatial moments. A third case study shows the effectiveness of the S-STBM algorithm to calibrate a Gaussian field to tracer tests. Contrary to FFTMA-SA, S-STBM works on irregular grids. Its computational complexity of O(n) and small memory requirement makes it an attractive method for calibration.},
  keywords = {Conditional simulation,Constructive calibration,High-order statistics,Spectral turning bands},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Lauzon_Marcotte\\lauzon2020calibration.pdf;C\:\\Users\\benha\\Zotero\\storage\\WKMIAIZG\\S0098300419306752.html}
}

@article{lauzon2020sequential,
  title = {The Sequential Spectral Turning Band Simulator as an Alternative to {{Gibbs}} Sampler in Large Truncated- or Pluri- {{Gaussian}} Simulations},
  author = {Lauzon, Dany and Marcotte, Denis},
  year = {2020},
  month = nov,
  journal = {Stochastic Environmental Research and Risk Assessment},
  volume = {34},
  number = {11},
  pages = {1939--1951},
  issn = {1436-3259},
  doi = {10.1007/s00477-020-01850-9},
  urldate = {2024-02-13},
  abstract = {The Sequential Spectral Turning Bands Method (S-STBM) builds Gaussian random fields (GRF) calibrated to desired response functions. An interesting application of S-STBM concerns the simulation of GRF subject to inequality constraints. S-STBM works by choosing the phase of each cosine function of the STBM algorithm instead of perturbating nodes of the GRF many thousand times using conditional distributions as in Gibbs sampler. Each chosen phase increasingly constrains the nodes to the desired inequalities. A method based on the sequential Gaussian simulation is introduced to accelerate convergence at the end of the process. Examples shown compare S-STBM approach to Gibbs sampler. Orders of magnitude reduction in computation time is achieved with our spectral method. Furthermore, examples show that the phase selection has no significant influence on the spatial correlation. Our approach is easily generalized to pluriGaussian simulations. Compared to Gibbs sampler, S-STBM is not limited to small systems (no memory limitation) and its complexity of O(n) makes it an efficient tool to simulate large GRF subject to inequality constraints.},
  langid = {english},
  keywords = {Gibbs Sampler,Inequality constraints,PluriGaussian simulation,Sequential Gaussian simulation,Spectral simulation,Truncated Gaussian random field},
  file = {D:\03 UofA\06 Reading\_zotfile\Lauzon_Marcotte\lauzon2020sequential.pdf}
}

@article{lauzon2023joint,
  title = {Joint Hydrofacies-Hydraulic Conductivity Modeling Based on a Constructive Spectral Algorithm Constrained by Transient Head Data},
  author = {Lauzon, Dany and Marcotte, Denis},
  year = {2023},
  month = sep,
  journal = {Hydrogeology Journal},
  volume = {31},
  number = {6},
  pages = {1647--1664},
  issn = {1435-0157},
  doi = {10.1007/s10040-023-02638-1},
  urldate = {2024-02-15},
  abstract = {A constructive spectral method is presented to jointly calibrate hydrofacies and hydraulic conductivity to transient pressure heads. The method iteratively constructs Gaussian random fields to model the spatial correlation of hydraulic conductivity and hydrofacies using pluriGaussian simulation. Borehole conditioning is done quickly by replacing the slow Gibbs sampler method with an approach that is based on calibrating the underlying Gaussian fields that are subject to inequality constraints. Calibration to transient pressure heads is performed by shallow optimization of the phase vectors of the continuous spectral method. A parameterization technique makes it possible to reduce phase vector optimization from multivariate to univariate. The algorithm is tested on two-dimensional (2D) and 3D synthetic regional aquifers made of three hydrofacies. It reduced the objective function by one order of magnitude in one hundred iterations. The tests on the 2D aquifers indicated that the transient hydraulic heads alone cannot provide much information about hydrofacies. However, combining them with hydrofacies observations from boreholes results in improved hydrofacies identification compared to when only borehole data are used. Similar results were obtained in the 3D aquifer case, although the improvement in aquifer identification was less pronounced. The spectral method presented makes it possible to calibrate complex aquifers to transient heads using a limited number of calls to the flow simulator. Doing so helps to characterize sub-surface heterogeneity and assess the uncertainty and geological risks associated with groundwater flow.},
  langid = {english},
  keywords = {Data assimilation,Geostatistics,Inverse modeling,Parameter uncertainty assessment,Stochastic hydrogeology},
  file = {D:\03 UofA\06 Reading\_zotfile\Lauzon_Marcotte\lauzon2023joint.pdf}
}

@article{lee2021temporal,
  title = {Temporal Prediction Modeling for Rainfall-Induced Shallow Landslide Hazards Using Extreme Value Distribution},
  author = {Lee, Jung-Hyun and Kim, Hanbeen and Park, Hyuck-Jin and Heo, Jun-Haeng},
  year = {2021},
  month = jan,
  journal = {Landslides},
  volume = {18},
  number = {1},
  pages = {321--338},
  issn = {1612-5118},
  doi = {10.1007/s10346-020-01502-7},
  urldate = {2024-05-01},
  abstract = {As the frequency and intensity of heavy rainfall increase, the frequency of extreme rainfall-induced landslides also increases. Thus, the importance of accurate assessment of extreme rainfall-induced landslide hazard increases. Landslide hazard assessment requires estimations of two components: spatial probability and temporal probability. While various approaches have been successfully used to estimate spatial landslide susceptibility, fewer studies have addressed temporal probability and, consequently, a commonly accepted method does not exist. Prior approaches have estimated temporal probability using frequency analysis of past landslides or landslide triggering rainfall events. Hence, a large amount of information was required: sufficiently complete historical data on recurrent landslides and repetitive rainfall events. However, in many cases, it is difficult to obtain such complete historical data. Therefore, this study developed a new approach that can be applied to an area where incomplete data are available or where nonrepetitive landslide events have occurred. To evaluate the temporal probability of landslide occurrence, the developed approach adopted extreme value analysis using the Gumbel distribution. The exceedance probability of a rainfall threshold was evaluated, using the Gumbel model, with 72-h antecedent rainfall threshold. This probability was then considered to be the temporal probability of landslide occurrence. The temporal probability of landslides was then integrated with landslide susceptibility results from a multi-layer perceptron model. Consequently, the landslide hazards for different future time periods, from 1 to 200~years, were estimated.},
  langid = {english},
  keywords = {Extreme value distribution,Gumbel distribution,Landslide hazard,Multi-layer perceptron,Rainfall-induced landslides,Temporal probability},
  file = {D:\03 UofA\06 Reading\_zotfile\Lee et al\lee2021temporal.pdf}
}

@article{leuangthong2004minimum,
  title = {Minimum {{Acceptance Criteria}} for {{Geostatistical Realizations}}},
  author = {Leuangthong, Oy and McLennan, Jason A. and Deutsch, Clayton V.},
  year = {2004},
  month = sep,
  journal = {Natural Resources Research},
  volume = {13},
  number = {3},
  pages = {131--141},
  issn = {1573-8981},
  doi = {10.1023/B:NARR.0000046916.91703.bb},
  urldate = {2024-03-07},
  abstract = {Geostatistical simulation is being used increasingly for numerical modeling of natural phenomena. The development of simulation as an alternative to kriging is the result of improved characterization of heterogeneity and a model of joint uncertainty. The popularity of simulation has increased in both mining and petroleum industries. Simulation is widely available in commercial software. Many of these software packages, however, do not necessarily provide the tools for careful checking of the geostatistical realizations prior to their use in decision-making. Moreover, practitioners may not understand all that should be checked. There are some basic checks that should be performed on all geostatistical models. This paper identifies (1) the minimum criteria that should be met by all geostatistical simulation models, and (2) the checks required to verify that these minimum criteria are satisfied. All realizations should honor the input information including the geological interpretation, the data values at their locations, the data distribution, and the correlation structure, within ``acceptable'' statistical fluctuations. Moreover, the uncertainty measured by the differences between simulated realizations should be a reasonable measure of uncertainty. A number of different applications are shown to illustrate the various checks. These checks should be an integral part of any simulation modeling work flow.},
  langid = {english},
  keywords = {Model validation,simulation,verification},
  file = {D:\03 UofA\06 Reading\_zotfile\Leuangthong et al\leuangthong2004minimum.pdf}
}

@book{leuangthong2008solved,
  title = {Solved Problems in Geostatistics},
  author = {Leuangthong, Oy and Khan, K Daniel and Deutsch, Clayton V},
  year = {2008},
  publisher = {John Wiley \& Sons},
  isbn = {978-0-470-17792-1},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Leuangthong et al\leuangthong2011solved.pdf}
}

@article{leuangthong2015dealing,
  title = {Dealing with High-Grade Data in Resource Estimation},
  author = {Leuangthong, O. and Nowak, M.},
  year = {2015},
  journal = {Journal of the Southern African Institute of Mining and Metallurgy},
  volume = {115},
  number = {1},
  pages = {27--36},
  publisher = {{The Southern African Institute of Mining and Metallurgy}},
  keywords = {thesis_02},
  file = {D\:\\03 UofA\\04 Research\\02 PhD\\_zotfile\\Leuangthong_Nowak\\leuangthong2015dealing.pdf;C\:\\Users\\benha\\Zotero\\storage\\MIJ9JM8Z\\scielo.html}
}

@article{leung2021sample,
  title = {Sample {{Truncation Strategies}} for {{Outlier Removal}} in {{Geochemical Data}}: {{The MCD Robust Distance Approach Versus}} t-{{SNE Ensemble Clustering}}},
  shorttitle = {Sample {{Truncation Strategies}} for {{Outlier Removal}} in {{Geochemical Data}}},
  author = {Leung, Raymond and Balamurali, Mehala and Melkumyan, Arman},
  year = {2021},
  month = jan,
  journal = {Mathematical Geosciences},
  volume = {53},
  number = {1},
  pages = {105--130},
  issn = {1874-8953},
  doi = {10.1007/s11004-019-09839-z},
  urldate = {2022-04-26},
  abstract = {The presence of outliers in geochemical data can impact the accuracy of grade models and influence the interpretation of mine assay data. Removal of outliers is therefore an important consideration in grade estimation work. This paper presents two sample truncation strategies which have been devised to reject outliers in multivariate geochemical data. In essence, a data-dependent threshold is applied to the robust distances of sorted samples to discard outliers within a given class. For robust distances based on the minimum covariance determinant (MCD) where sample deviations from the cluster centre are computed using robust estimates, the inverse chi-square cumulative distribution function is often used to compute the cutoff point, \$\${\textbackslash}chi \_\{1-{\textbackslash}alpha ,{\textbackslash}nu \}\$\$, under the assumption of multivariate normality. In this work, it has been observed that this approach consistently underestimates the true extent of outliers. The proposed alternatives consist of a geometric and an analytic approach. The former defines the sample truncation point as the knee of the robust distance curve in an approximately chi-square-distributed quantile--quantile plot. The latter uses the silhouette and likelihood functions to consider the degree of cohesion in the resultant inlier/outlier clusters. Both techniques significantly reduce the scatter amongst the samples retained in each domain/class. For validation, ensemble clustering based on t-distributed stochastic neighbour embedding (t-SNE) is used to study the outlier recall rate, the effects of feature selection, and spatial correlation with MCD-based outlier rejection. Visual and quantitative analyses show that the proposed methods are superior to the baseline method which rejects samples using chi-square critical values.},
  langid = {english},
  keywords = {Geochemical data,Outlier detection,Robust distance,Sample truncation strategies,Subdomain identification},
  file = {D:\03 UofA\06 Reading\_zotfile\Leung et al\leung2021sample.pdf}
}

@article{li2022ecod,
  title = {{{ECOD}}: {{Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions}}},
  shorttitle = {{{ECOD}}},
  author = {Li, Zheng and Zhao, Yue and Hu, Xiyang and Botta, Nicola and Ionescu, Cezar and Chen, George H.},
  year = {2022},
  month = mar,
  journal = {arXiv:2201.00382 [cs, stat]},
  eprint = {2201.00382},
  primaryclass = {cs, stat},
  urldate = {2022-05-06},
  abstract = {Outlier detection refers to the identification of data points that deviate from a general data distribution. Existing unsupervised approaches often suffer from high computational cost, complex hyperparameter tuning, and limited interpretability, especially when working with large, high-dimensional datasets. To address these issues, we present a simple yet effective algorithm called ECOD (Empirical-Cumulative-distribution-based Outlier Detection), which is inspired by the fact that outliers are often the "rare events" that appear in the tails of a distribution. In a nutshell, ECOD first estimates the underlying distribution of the input data in a nonparametric fashion by computing the empirical cumulative distribution per dimension of the data. ECOD then uses these empirical distributions to estimate tail probabilities per dimension for each data point. Finally, ECOD computes an outlier score of each data point by aggregating estimated tail probabilities across dimensions. Our contributions are as follows: (1) we propose a novel outlier detection method called ECOD, which is both parameter-free and easy to interpret; (2) we perform extensive experiments on 30 benchmark datasets, where we find that ECOD outperforms 11 state-of-the-art baselines in terms of accuracy, efficiency, and scalability; and (3) we release an easy-to-use and scalable (with distributed support) Python implementation for accessibility and reproducibility.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Li et al\\li2022ecod.pdf;C\:\\Users\\benha\\Zotero\\storage\\PUJ47SQ5\\2201.html}
}

@article{linde2015geological,
  title = {Geological Realism in Hydrogeological and Geophysical Inverse Modeling: {{A}} Review},
  shorttitle = {Geological Realism in Hydrogeological and Geophysical Inverse Modeling},
  author = {Linde, Niklas and Renard, Philippe and Mukerji, Tapan and Caers, Jef},
  year = {2015},
  month = dec,
  journal = {Advances in Water Resources},
  volume = {86},
  pages = {86--101},
  issn = {0309-1708},
  doi = {10.1016/j.advwatres.2015.09.019},
  urldate = {2024-04-10},
  abstract = {Scientific curiosity, exploration of georesources and environmental concerns are pushing the geoscientific research community toward subsurface investigations of ever-increasing complexity. This review explores various approaches to formulate and solve inverse problems in ways that effectively integrate geological concepts with geophysical and hydrogeological data. Modern geostatistical simulation algorithms can produce multiple subsurface realizations that are in agreement with conceptual geological models and statistical rock physics can be used to map these realizations into physical properties that are sensed by the geophysical or hydrogeological data. The inverse problem consists of finding one or an ensemble of such subsurface realizations that are in agreement with the data. The most general inversion frameworks are presently often computationally intractable when applied to large-scale problems and it is necessary to better understand the implications of simplifying (1) the conceptual geological model (e.g., using model compression); (2) the physical forward problem (e.g., using proxy models); and (3) the algorithm used to solve the inverse problem (e.g., Markov chain Monte Carlo or local optimization methods) to reach practical and robust solutions given today's computer resources and knowledge. We also highlight the need to not only use geophysical and hydrogeological data for parameter estimation purposes, but also to use them to falsify or corroborate alternative geological scenarios.},
  keywords = {Geophysics,Geostatistics,Hydrogeology,Hydrogeophysics,Inverse problems,Markov chain Monte Carlo},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Linde et al\\linde2015geological2.pdf;C\:\\Users\\benha\\Zotero\\storage\\G29QIUAJ\\S0309170815002262.html}
}

@book{little2002statistical,
  title = {Statistical {{Analysis}} with {{Missing Data}}},
  author = {Little, Roderick J. A. and Rubin, Donald B.},
  year = {2002},
  edition = {1},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781119013563},
  urldate = {2023-08-14},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Little_Rubin\\little2002statistical.pdf;C\:\\Users\\benha\\Zotero\\storage\\T5FRK54J\\9781119013563.html}
}

@book{little2019statistical,
  title = {Statistical Analysis with Missing Data},
  author = {Little, Roderick J. A. and Rubin, Donald B.},
  year = {2019},
  series = {Wiley Series in Probability and Statistics},
  edition = {3},
  publisher = {Wiley},
  isbn = {0-470-52679-3 978-1-118-59601-2 978-1-118-59569-5 978-0-470-52679-8},
  file = {D:\03 UofA\06 Reading\_zotfile\Roderick J. A. Little\little2019statistical.pdf}
}

@incollection{machado2012field,
  title = {Field {{Parametric Geostatistics}}---{{A Rigorous Theory}} to {{Solve Problems}} of {{Highly Skewed Distributions}}},
  booktitle = {Geostatistics {{Oslo}} 2012},
  author = {Machado, Rochana S and Armony, Miguel and Costa, Jo{\~a}o Felipe Coimbra Leite},
  year = {2012},
  pages = {383--395},
  publisher = {Springer},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Machado et al\machado2012field.pdf}
}

@article{madani2021enhanced,
  title = {Enhanced Conditional {{Co-Gibbs}} Sampling Algorithm for Data Imputation},
  author = {Madani, Nasser and Bazarbekov, Talgatbek},
  year = {2021},
  month = mar,
  journal = {Computers \& Geosciences},
  volume = {148},
  pages = {104655},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2020.104655},
  urldate = {2024-02-14},
  abstract = {The Gibbs sampler is an iterative algorithm for data imputation of a random vector at locations where values of the variable of interest are missing. In this algorithm, the simulated values converge to a Gaussian random vector distribution with zero mean and a given covariance matrix obtained by solving a simple kriging system through several iterations. In a bivariate dataset, if the principal variable for imputation depends on an auxiliary variable that is more abundant at the sample locations, this algorithm fails to produce the local and spatial cross-correlation structures. To overcome this impediment, a variant of the Gibbs sampler, the conditional Co-Gibbs sampler, has been proposed in this study, where simple kriging is replaced by three alternative cokriging paradigms: multicollocated cokriging, collocated cokriging, and homotopic cokriging. The algorithm was examined for an actual case study to statistically evaluate its performance. The results indicate that the conditional Co-Gibbs sampler with multicollocated cokriging outperformed the alternatives, including simple kriging where data imputation occurred as a consequence of ignoring the influence of the auxiliary variable, partially or totally. In addition, a computer software, provided as an open-source executable file, was used to implement the proposed algorithm for data imputation in bivariate cases.},
  keywords = {Algorithms,Data processing,Geology,Geostatistics,Spatial statistics},
  file = {D:\03 UofA\06 Reading\_zotfile\Madani_Bazarbekov\madani2021enhanced.pdf}
}

@article{mahalanobis2018generalized,
  title = {On the {{Generalized Distance}} in {{Statistics}}},
  author = {Mahalanobis, P. C.},
  year = {2018},
  journal = {Sankhy{\=a}: The Indian Journal of Statistics, Series A (2008-)},
  volume = {80},
  eprint = {48723335},
  eprinttype = {jstor},
  pages = {S1-S7},
  publisher = {[Springer, Indian Statistical Institute]},
  issn = {0976-836X},
  urldate = {2024-04-30},
  file = {D:\03 UofA\06 Reading\_zotfile\Mahalanobis\mahalanobis2018generalized.pdf}
}

@article{maleki2014capping,
  title = {Capping and Kriging Grades with Long-Tailed Distributions},
  author = {Maleki, M. and Madani, N. and Emery, X.},
  year = {2014},
  journal = {Journal of the Southern African Institute of Mining and Metallurgy},
  volume = {114},
  number = {3},
  pages = {255--263},
  publisher = {{The Southern African Institute of Mining and Metallurgy}},
  file = {D\:\\03 UofA\\04 Research\\02 PhD\\_zotfile\\Maleki et al\\maleki2014capping.pdf;C\:\\Users\\benha\\Zotero\\storage\\UDIIMC5J\\scielo.html}
}

@book{mariethoz2014multiplepoint,
  title = {Multiple-{{Point Geostatistics}}},
  author = {Mariethoz, Gregoire and Caers, Jef},
  year = {2014},
  edition = {1},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781118662953},
  urldate = {2024-04-26},
  abstract = {This book provides a comprehensive introduction to multiple-point geostatistics, where spatial continuity is described using training images. Multiple-point geostatistics aims at bridging the gap between physical modelling/realism and spatio-temporal stochastic modelling. The book provides an overview of this new field in three parts. Part I presents a conceptual comparison between traditional random function theory and stochastic modelling based on training images, where random function theory is not always used. Part II covers in detail various algorithms and methodologies starting from basic building blocks in statistical science and computer science. Concepts such as non-stationary and multi-variate modeling, consistency between data and model, the construction of training images and inverse modelling are treated. Part III covers three example application areas, namely, reservoir modelling, mineral resources modelling and climate model downscaling. This book will be an invaluable reference for students, researchers and practitioners of all areas of the Earth Sciences where forecasting based on spatio-temporal data is performed.},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Mariethoz_Caers\\mariethoz2014multiplepoint.pdf;C\:\\Users\\benha\\Zotero\\storage\\NAYY9W6Y\\9781118662953.html}
}

@article{matheron1963principles,
  title = {Principles of Geostatistics},
  author = {Matheron, Georges},
  year = {1963},
  journal = {Economic geology},
  volume = {58},
  number = {8},
  pages = {1246--1266},
  publisher = {Society of Economic Geologists},
  file = {D\:\\03 UofA\\04 Research\\02 PhD\\_zotfile\\Matheron\\matheron1963principles.pdf;C\:\\Users\\benha\\Zotero\\storage\\FHBA3RRH\\Principles-of-geostatistics.html}
}

@techreport{matheron1982factorial,
  title = {Pour Une Analyse Krigeante Des Donn{\'e}es R{\'e}gionalis{\'e}es},
  author = {Matheron, Georges},
  year = {1982},
  number = {N-732},
  institution = {Ecole des Mines de Paris},
  urldate = {2021-10-28},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Matheron\matheron1982factorial.pdf}
}

@inproceedings{matheron1987conditional,
  title = {Conditional Simulation of the Geometry of Fluvio-Deltaic Reservoirs},
  booktitle = {{{SPE}} Annual Technical Conference and Exhibition?},
  author = {Matheron, Georges and Beucher, H{\'e}l{\`e}ne and {de Fouquet}, Chantal and Galli, A and Gu{\'e}rillot, Dominique and Ravenne, Ch},
  year = {1987},
  pages = {SPE--16753},
  publisher = {Spe},
  file = {D:\03 UofA\06 Reading\_zotfile\Matheron et al\matheron1987conditional.pdf}
}

@book{matheron2019matheron,
  title = {Matheron's {{Theory}} of {{Regionalised Variables}}},
  author = {Matheron, Georges},
  editor = {{Pawlowsky-Glahn}, Vera and Serra, Jean},
  year = {2019},
  series = {International {{Association}} for {{Mathematical Geology Studies}} in {{Mathematical Geology}}},
  publisher = {Oxford University Press},
  address = {Oxford},
  doi = {10.1093/oso/9780198835660.001.0001},
  urldate = {2021-10-26},
  abstract = {This book has never been published before, although its contents have provided the basis for hundreds of papers, theses, and books on geostatistics. The chapters are based on the lectures of a summer course given by Georges Matheron in 1970; initially written in French, they were translated into English by Charles Huijbregts. They do not contain mathematical technicalities or practical case studies; instead, they present major topics like estimation variances, kriging systems, mining estimation, and intrinsic theory, all of which are established by simple proofs. The reader is invited to wonder about the physical meaning of the notions Matheron deals with. When Matheron wrote these lectures, he considered the theory of linear geostatistics complete; however, what was an ending for Matheron has been the starting point for most geostatisticians. Many discovered the book's content indirectly, via the many borrowings one can find in several books; in such a situation, it is always instructive to come back to the original document, where the author's motivations, his physical intuitions, and his thoughts on the meaning of what he does are detailed. The decision to publish this book was motivated by the desire to introduce Matheron's work to a larger audience. The book has remained faithful to the original notes while introducing a common structure for the chapters and sections, numbering equations sequentially within each chapter, numbering the figures (most of which were redrawn) sequentially, and adding captions. In addition, Matheron's comments on the exercises, or suggestions for solutions, have been added.},
  isbn = {978-0-19-883566-0},
  langid = {english},
  keywords = {extension variance,intrinsic theory,kriging,linear geostatistics,mining estimation},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Matheron et al\matheron2019matheron2.pdf}
}

@article{mclachlan2019finite,
  title = {Finite {{Mixture Models}}},
  author = {McLachlan, Geoffrey J. and Lee, Sharon X. and Rathnayake, Suren I.},
  year = {2019},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {6},
  number = {Volume 6, 2019},
  pages = {355--378},
  publisher = {Annual Reviews},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-031017-100325},
  urldate = {2024-04-03},
  abstract = {The important role of finite mixture models in the statistical analysis of data is underscored by the ever-increasing rate at which articles on mixture applications appear in the statistical and general scientific literature. The aim of this article is to provide an up-to-date account of the theory and methodological developments underlying the applications of finite mixture models. Because of their flexibility, mixture models are being increasingly exploited as a convenient, semiparametric way in which to model unknown distributional shapes. This is in addition to their obvious applications where there is group-structure in the data or where the aim is to explore the data for such structure, as in a cluster analysis. It has now been three decades since the publication of the monograph by McLachlan \&amp; Basford (1988) with an emphasis on the potential usefulness of mixture models for inference and clustering. Since then, mixture models have attracted the interest of many researchers and have found many new and interesting fields of application. Thus, the literature on mixture models has expanded enormously, and as a consequence, the bibliography here can only provide selected coverage.},
  langid = {english},
  file = {C:\Users\benha\Zotero\storage\4RTEK94L\annurev-statistics-031017-100325.html}
}

@techreport{medgold2021,
  title = {{{PRELIMINARY ECONOMIC ASSESSMENT AND NI}} 43-101 {{TECHNICAL REPORT FOR THE MEDGOLD TLAMINO PROJECT LICENCES}}, {{SERBIA}}},
  author = {{Medgold Resources Corp.}},
  year = {2021},
  keywords = {thesis_02}
}

@article{meng2020enhancing,
  title = {Enhancing {{Differential Evolution With Novel Parameter Control}}},
  author = {Meng, Zhenyu and Chen, Yuxin and Li, Xiaoqing},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {51145--51167},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2979738},
  urldate = {2023-12-06},
  abstract = {In this paper, we proposed a novel DE variant named DE-NPC for real parameter single objective optimization. In DE-NPC algorithm, a novel adaptation scheme for the scale factor F is first proposed, which is based on the location information of the population rather than the fitness difference. The adaptation scheme of crossover rate CR in our DE-NPC is based on its success probability. Furthermore, a novel population size reduction scheme is also employed in DE-NPC, which can get a better perception of the landscape of objectives and consequently obtain an overall better performance. The algorithm validation is conducted under our test suite containing 88 benchmarks from CEC2013, CEC2014 and CEC2017 in comparison with several state-of-the-art DE variants. The experiment results show that our novel DE-NPC algorithm is competitive with these state-of-the-art DE variants.},
  langid = {english},
  file = {C:\Users\benha\Zotero\storage\PUARZI7A\Meng et al. - 2020 - Enhancing Differential Evolution With Novel Parame.pdf}
}

@article{miniussi2020metastatistical,
  title = {Metastatistical {{Extreme Value Distribution}} Applied to Floods across the Continental {{United States}}},
  author = {Miniussi, Arianna and Marani, Marco and Villarini, Gabriele},
  year = {2020},
  month = feb,
  journal = {Advances in Water Resources},
  volume = {136},
  pages = {103498},
  issn = {0309-1708},
  doi = {10.1016/j.advwatres.2019.103498},
  urldate = {2024-05-01},
  abstract = {This study analyzes daily mean streamflow records from 5,311~U.S. Geological Survey stream gages in the continental United States and develops a Metastatistical Extreme Value Distribution (MEVD) tailored for flood frequency analysis. We compare the new tool with the Generalized Extreme Value (GEV) and Log-Pearson Type III (LP3) distributions and investigate the role of El Ni{\~n}o Southern Oscillation (ENSO) in the generation of floods. Hence, we formulate the MEVD in terms of mixture of distributions to describe the occurrence of flood peaks generated under different ENSO phases. We find that the MEVD outperforms GEV and LP3 distributions respectively in about 76\% and 86\% of the stations, with a significant improvement in the accuracy of quantiles corresponding to return periods much larger than the calibration sample size. The ENSO signature detected in the distributions of the daily peak flows does not necessarily improve the estimation of high return period flow values.},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Miniussi et al\\miniussi2020metastatistical.pdf;C\:\\Users\\benha\\Zotero\\storage\\SPZJAZAI\\S0309170819306852.html}
}

@article{minniakhmetov2017joint,
  title = {Joint {{High-Order Simulation}} of {{Spatially Correlated Variables Using High-Order Spatial Statistics}}},
  author = {Minniakhmetov, Ilnur and Dimitrakopoulos, Roussos},
  year = {2017},
  month = jan,
  journal = {Mathematical Geosciences},
  volume = {49},
  number = {1},
  pages = {39--66},
  issn = {1874-8953},
  doi = {10.1007/s11004-016-9662-x},
  urldate = {2024-04-25},
  abstract = {Joint geostatistical simulation techniques are used to quantify uncertainty for spatially correlated attributes, including mineral deposits, petroleum reservoirs, hydrogeological horizons, environmental contaminants. Existing joint simulation methods consider only second-order spatial statistics and Gaussian processes. Motivated by the presence of relatively large datasets for multiple correlated variables that typically are available from mineral deposits and the effects of complex spatial connectivity between grades on the subsequent use of simulated realizations, this paper presents a new approach for the joint high-order simulation of spatially correlated random fields. First, a vector random function is orthogonalized with a new decorrelation algorithm into independent factors using the so-termed diagonal domination condition of high-order cumulants. Each of the factors is then simulated independently using a high-order univariate simulation method on the basis of high-order spatial cumulants and Legendre polynomials. Finally, attributes of interest are reconstructed through the back-transformation of the simulated factors. In contrast to state-of-the-art methods, the decorrelation step of the proposed approach not only considers the covariance matrix, but also high-order statistics to obtain independent non-Gaussian factors. The intricacies of the application of the proposed method are shown with a dataset from a multi-element iron ore deposit. The application shows the reproduction of high-order spatial statistics of available data by the jointly simulated attributes.},
  langid = {english},
  keywords = {Correlated variables,Decorrelation,Diagonal domination,High-order simulation,Joint simulation,Multi-element mineral deposits,Non-Gaussian variables},
  file = {D:\03 UofA\06 Reading\_zotfile\Minniakhmetov_Dimitrakopoulos\minniakhmetov2017joint.pdf}
}

@article{minniakhmetov2018highorder,
  title = {High-{{Order Spatial Simulation Using Legendre-Like Orthogonal Splines}}},
  author = {Minniakhmetov, Ilnur and Dimitrakopoulos, Roussos and Godoy, Marcelo},
  year = {2018},
  month = oct,
  journal = {Mathematical Geosciences},
  volume = {50},
  number = {7},
  pages = {753--780},
  issn = {1874-8953},
  doi = {10.1007/s11004-018-9741-2},
  urldate = {2022-08-30},
  abstract = {High-order sequential simulation techniques for complex non-Gaussian spatially distributed variables have been developed over the last few years. The high-order simulation approach does not require any transformation of initial data and makes no assumptions about any probability distribution function, while it introduces complex spatial relations to the simulated realizations via high-order spatial statistics. This paper presents a new extension where a conditional probability density function (cpdf) is approximated using Legendre-like orthogonal splines. The coefficients of spline approximation are estimated using high-order spatial statistics inferred from the available sample data, additionally complemented by a training image. The advantages of using orthogonal splines with respect to the previously used Legendre polynomials include their ability to better approximate a multidimensional probability density function, reproduce the high-order spatial statistics, and provide a generalization of high-order simulations using Legendre polynomials. The performance of the new method is first tested with a completely known image and compared to both the high-order simulation approach using Legendre polynomials and the conventional sequential Gaussian simulation method. Then, an application in a gold deposit demonstrates the advantages of the proposed method in terms of the reproduction of histograms, variograms, and high-order spatial statistics, including connectivity measures. The C++ course code of the high-order simulation implementation presented herein, along with an example demonstrating its utilization, are provided online as supplementary material.},
  langid = {english},
  keywords = {High-order spatial statistics,Non-Gaussian distribution,Orthogonal splines,Spatial complexity,Stochastic simulation},
  file = {D:\03 UofA\06 Reading\_zotfile\Minniakhmetov et al\minniakhmetov2018highorder.pdf}
}

@article{minniakhmetov2022highorder,
  title = {High-{{Order Data-Driven Spatial Simulation}} of {{Categorical Variables}}},
  author = {Minniakhmetov, Ilnur and Dimitrakopoulos, Roussos},
  year = {2022},
  month = jan,
  journal = {Mathematical Geosciences},
  volume = {54},
  number = {1},
  pages = {23--45},
  issn = {1874-8953},
  doi = {10.1007/s11004-021-09943-z},
  urldate = {2022-04-26},
  abstract = {Modern approaches for the spatial simulation of categorical variables are largely based on multi-point statistical methods, where a training image is used to derive complex spatial relationships using relevant patterns. In these approaches, simulated realizations are driven by the training image utilized, while the spatial statistics of the actual sample data are ignored. This paper presents a data-driven, high-order simulation approach based on the approximation of high-order spatial indicator moments. The high-order spatial statistics are expressed as functions of spatial distances that are similar to variogram models for two-point methods, while higher-order statistics are connected with lower-orders via boundary conditions. Using an advanced recursive B-spline approximation algorithm, the high-order statistics are reconstructed from the available data and are subsequently used for the construction of conditional distributions using Bayes' rule. Random values are subsequently simulated for all unsampled grid nodes. The main advantages of the proposed technique are its ability to (a) simulate without a training image to reproduce the high-order statistics of the data, and (b) adapt the model's complexity to the information available in the data. The practical intricacies and effectiveness of the proposed approach are demonstrated through applications at two copper deposits.},
  langid = {english},
  keywords = {Categorical variables,Data-driven,High-order spatial statistics,Spatial model,Stochastic simulation},
  file = {D:\03 UofA\06 Reading\_zotfile\Minniakhmetov_Dimitrakopoulos\minniakhmetov2022highorder.pdf}
}

@book{mirjalili2019evolutionary,
  title = {Evolutionary {{Algorithms}} and {{Neural Networks}}},
  author = {Mirjalili, Seyedali},
  year = {2019},
  series = {Studies in {{Computational Intelligence}}},
  volume = {780},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-93025-1},
  urldate = {2023-04-25},
  isbn = {978-3-319-93024-4 978-3-319-93025-1},
  keywords = {Applied Neural Networks,Backpropagation Algorithms,Binary PSO Algorithms,Deep Neural Networks for Image Classification,Estimation of a Global Optimum,Evolutionary Computation for Real-world Problems,Evolutionary Operators,Hand Posture/Gesture Detection Using Neural Networks,Mathematical Model of PSO,Optimal Set of Features,Optimization for Real World Problems,Population-based Optimization Algorithms,Single-objective Optimization Algorithm,Stochastic Optimization Algorithms,Training Algorithms for Neural Networks,Training Neural Networks with Genetic Algorithms},
  file = {D:\03 UofA\06 Reading\_zotfile\Mirjalili\mirjalili2019evolutionary.pdf}
}

@article{mises1936distribution,
  title = {La Distribution de La plus Grande de n Valeurs},
  author = {von Mises, R},
  year = {1936},
  journal = {Rev. Math. Interbalcanic Union},
  volume = {1},
  pages = {141--160}
}

@article{mohamed2014rdel,
  title = {{{RDEL}}: {{Restart Differential Evolution}} Algorithm with {{Local Search Mutation}} for Global Numerical Optimization},
  shorttitle = {{{RDEL}}},
  author = {Mohamed, Ali Wagdy},
  year = {2014},
  month = nov,
  journal = {Egyptian Informatics Journal},
  volume = {15},
  number = {3},
  pages = {175--188},
  issn = {1110-8665},
  doi = {10.1016/j.eij.2014.07.001},
  urldate = {2023-08-08},
  abstract = {In this paper, a novel version of Differential Evolution (DE) algorithm based on a couple of local search mutation and a restart mechanism for solving global numerical optimization problems over continuous space is presented. The proposed algorithm is named as Restart Differential Evolution algorithm with Local Search Mutation (RDEL). In RDEL, inspired by Particle Swarm Optimization (PSO), a novel local mutation rule based on the position of the best and the worst individuals among the entire population of a particular generation is introduced. The novel local mutation scheme is joined with the basic mutation rule through a linear decreasing function. The proposed local mutation scheme is proven to enhance local search tendency of the basic DE and speed up the convergence. Furthermore, a restart mechanism based on random mutation scheme and a modified Breeder Genetic Algorithm (BGA) mutation scheme is combined to avoid stagnation and/or premature convergence. Additionally, an exponent increased crossover probability rule and a uniform scaling factors of DE are introduced to promote the diversity of the population and to improve the search process, respectively. The performance of RDEL is investigated and compared with basic differential evolution, and state-of-the-art parameter adaptive differential evolution variants. It is discovered that the proposed modifications significantly improve the performance of DE in terms of quality of solution, efficiency and robustness.},
  langid = {english},
  keywords = {Differential evolution,Evolutionary computation,Global numerical optimization,Local search mutation,Restart mechanism},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Mohamed\\mohamed2014rdel.pdf;C\:\\Users\\benha\\Zotero\\storage\\4D5QJGVC\\S1110866514000279.html}
}

@article{mohammadi2021direct,
  title = {A Direct Sampling Multiple Point Statistical Approach for Multivariate Imputation of Unequally Sampled Compositional Variables and Categorical Data},
  author = {Mohammadi, Hamed and Hosseini, Sajjad Talesh and Asghari, Omid and {Zacche da Silva}, Camilla and Boisvert, Jeff B.},
  year = {2021},
  month = nov,
  journal = {Computers \& Geosciences},
  volume = {156},
  pages = {104911},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2021.104911},
  urldate = {2024-04-29},
  abstract = {Exploration datasets are often unequally sampled and have missing values for select variables of interest at select locations. Many state-of-the-art joint multivariate modeling workflows cannot consider missing data. One solution is to exclude incomplete samples from multivariate geostatistical modeling; however, this leads to a loss of information, increases uncertainty, and may introduce a bias in subsequent spatial numerical modeling workflows. Alternatives include (1) impute the missing values to generate a complete data set, termed single imputation, or (2) generate multiple realizations of the data that account for uncertainty in the missing values, termed multiple imputation (MI). MI is preferred as it quantifies uncertainty in missing values and transfers that uncertainty through spatial numerical modeling workflows. A new algorithm is proposed for the imputation of unequally sampled continuous, compositional, and categorical variables. A modified version of multiple-point direct sampling is used to impute missing values using multivariate multiple-point patterns from nearby completely sampled observations. Drillhole data are used as the `training data' for direct sampling, with preference given to training data with similar co-located values to the imputation sample to account for non-stationarities common in mineral deposits. Advantages of the algorithm include: (1) alignment with the current best practice of MI, data uncertainty is incorporated through multiple realizations of missing data and can be carried through further geomodelling workflows; (2) using multivariate multiple-point patterns honors spatial and multivariate relationships in the data; (3) can be applied to joint imputation of categorical and continuous variables; (4) better reproduces input proportions and compositional data, and (5) can explicitly incorporate non-stationarities. The proposed methodology is compared to multiple imputation by chained equations (MICE) and Bayesian updating (BU) using two Iranian case studies; samples from these complete datasets are removed based on missing at random and missing not at random mechanisms. The third case study is a South American Iron deposit with compositional data that was originally incompletely sampled, the mechanism of missingness is unknown. Comparisons between imputation methodologies over the three case studies show that the proposed algorithm reduces prediction error, generates accurate and unbiased imputed values that reproduce multivariate relationships, reproduces multiple-point statistics patterns, and is robust in non-stationary data sets.},
  keywords = {Direct sampling,Mining,Missing data imputation,Multiple imputation,Multiple point statistics,Multivariate geostatistical modeling,Spatial modeling,Training data},
  file = {C:\Users\benha\Zotero\storage\ID2WCQKY\S0098300421002028.html}
}

@article{mood1940distribution,
  title = {The {{Distribution Theory}} of {{Runs}}},
  author = {Mood, A. M.},
  year = {1940},
  journal = {The Annals of Mathematical Statistics},
  volume = {11},
  number = {4},
  eprint = {2235718},
  eprinttype = {jstor},
  pages = {367--392},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851},
  urldate = {2022-04-21},
  file = {D:\03 UofA\06 Reading\_zotfile\Mood\mood1940distribution.pdf}
}

@misc{mullner2011modern,
  title = {Modern Hierarchical, Agglomerative Clustering Algorithms},
  author = {M{\"u}llner, Daniel},
  year = {2011},
  month = sep,
  number = {arXiv:1109.2378},
  eprint = {1109.2378},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-12-11},
  abstract = {This paper presents algorithms for hierarchical, agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a "stepwise dendrogram", a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting efficiently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes.},
  archiveprefix = {arxiv},
  keywords = {62H30,Computer Science - Data Structures and Algorithms,I.5.3,Statistics - Machine Learning,thesis_06},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Müllner\\mullner2011modern.pdf;C\:\\Users\\benha\\Zotero\\storage\\RBT9X7UZ\\1109.html}
}

@article{mustapha2010highorder,
  title = {High-Order {{Stochastic Simulation}} of {{Complex Spatially~Distributed Natural Phenomena}}},
  author = {Mustapha, Hussein and Dimitrakopoulos, Roussos},
  year = {2010},
  month = jul,
  journal = {Mathematical Geosciences},
  volume = {42},
  number = {5},
  pages = {457--485},
  issn = {1874-8953},
  doi = {10.1007/s11004-010-9291-8},
  urldate = {2022-08-30},
  abstract = {Spatially distributed and varying natural phenomena encountered in geoscience and engineering problem solving are typically incompatible with Gaussian models, exhibiting nonlinear spatial patterns and complex, multiple-point connectivity of extreme values. Stochastic simulation of such phenomena is historically founded on second-order spatial statistical approaches, which are limited in their capacity to model complex spatial uncertainty. The newer multiple-point (MP) simulation framework addresses past limits by establishing the concept of a training image, and, arguably, has its own drawbacks. An alternative to current MP approaches is founded upon new high-order measures of spatial complexity, termed ``high-order spatial cumulants.'' These are combinations of moments of statistical parameters that characterize non-Gaussian random fields and can describe complex spatial information. Stochastic simulation of complex spatial processes is developed based on high-order spatial cumulants in the high-dimensional space of Legendre polynomials. Starting with discrete Legendre polynomials, a set of discrete orthogonal cumulants is introduced as a tool to characterize spatial shapes. Weighted orthonormal Legendre polynomials define the so-called Legendre cumulants that are high-order conditional spatial cumulants inferred from training images and are combined with available sparse data sets. Advantages of the high-order sequential simulation approach developed herein include the absence of any distribution-related assumptions and pre- or post-processing steps. The method is shown to generate realizations of complex spatial patterns, reproduce bimodal data distributions, data variograms, and high-order spatial cumulants of the data. In addition, it is shown that the available hard data dominate the simulation process and have a definitive effect on the simulated realizations, whereas the training images are only used to fill in high-order relations that cannot be inferred from data. Compared to the MP framework, the proposed approach is data-driven and consistently reconstructs the lower-order spatial complexity in the data used, in addition to high order.},
  langid = {english},
  keywords = {Conditional sequential simulation,High-order spatial cumulants,Legendre polynomials},
  file = {D:\03 UofA\06 Reading\_zotfile\Mustapha_Dimitrakopoulos\mustapha2010highorder.pdf}
}

@article{mustapha2011hosim,
  title = {{{HOSIM}}: {{A}} High-Order Stochastic Simulation Algorithm for Generating Three-Dimensional Complex Geological Patterns},
  shorttitle = {{{HOSIM}}},
  author = {Mustapha, Hussein and Dimitrakopoulos, Roussos},
  year = {2011},
  month = sep,
  journal = {Computers \& Geosciences},
  volume = {37},
  number = {9},
  pages = {1242--1253},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2010.09.007},
  urldate = {2022-09-14},
  abstract = {The three-dimensional high-order simulation algorithm HOSIM is developed to simulate complex non-linear and non-Gaussian systems. HOSIM is an alternative to the current MP approaches and it is based upon new high-order spatial connectivity measures, termed high-order spatial cumulants. The HOSIM algorithm implements a sequential simulation process, where local conditional distributions are generated using weighted orthonormal Legendre polynomials, which in turn define the so-called Legendre cumulants. The latter are high-order conditional spatial cumulants inferred from both the available data and training images. This approach is data-driven and reconstructs both high and lower-order spatial complexity in simulated realizations, while it only borrows from training images information that is not available in the data used. However, the three-dimensional implementation of the algorithm is computationally very intensive. To address his topic, the contribution of high-order conditional spatial cumulants is assessed in this paper through the number of Legendre cumulants with respect to the order of approximation used to estimate a conditional distribution and the number of data used within the respective neighbourhood. This leads to discarding the terms of Legendre cumulants with negligible contributions and allows an efficient simulation algorithm to be developed. The current version of the HOSIM algorithm is several orders of magnitude faster than the original version of the algorithm. Application and comparisons in a controlled environment show the excellent performance and efficiency of the HOSIM algorithm.},
  langid = {english},
  keywords = {High-order spatial cumulants,Legendre polynomials,Sequential simulation},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Mustapha_Dimitrakopoulos\\mustapha2011hosim.pdf;C\:\\Users\\benha\\Zotero\\storage\\WSQK9QXE\\S009830041000316X.html}
}

@article{nava-flores2023high,
  title = {High {{Resolution Model}} of the {{Vinton Salt-Dome Cap Rock}} by {{Joint Inversion}} of the {{Full Tensor Gravity Gradient Data}} with the {{Simulated Annealing Global Optimization Method}}},
  author = {{Nava-Flores}, Mauricio and {Ortiz-Alem{\'a}n}, Carlos and {Urrutia-Fucugauchi}, Jaime},
  year = {2023},
  month = mar,
  journal = {Pure and Applied Geophysics},
  volume = {180},
  number = {3},
  pages = {983--1014},
  issn = {1420-9136},
  doi = {10.1007/s00024-023-03227-9},
  urldate = {2024-04-04},
  abstract = {We present a 3D high-resolution modeling methodology based on the interpretation of gravity gradient data and its joint inversion with the simulated annealing (SA) global optimization method. The geometry of the model, used as computational domain in the solution of the forward and inverse problems, is defined with an irregular ensemble of cubic prisms that recreates the interpreted shape of the target, derived from the results of applying different interpretation methods to the gravity gradient data. In our inversion approach, the linear inverse problem resulting from the domain discretization is not solved. Instead, the cost function is explored with the SA algorithm, its low misfit region is identified, and models belonging to it are selected for obtaining the mean model, which represents the most likely model among them, as well as for estimating its uncertainty. The SA inversion algorithm we applied was numerically optimized to reduce the computational burden required by the forward problem, and it was driven by optimal tuning parameters, determined by a parametric analysis. Tests on synthetic data show the efficiency of our methodology to obtain a model that approximates the synthetic target and the usefulness of the estimated uncertainty to complement the interpretation. Finally, by applying our methodology to gravity gradient data acquired over the Vinton dome located in Louisiana, USA, we obtained results that are in agreement with geological information and previous studies.},
  langid = {english},
  keywords = {3D gravity gradient modeling,gravity gradient data processing,joint inversion,simulated annealing},
  file = {D:\03 UofA\06 Reading\_zotfile\Nava-Flores et al\nava-flores2023high.pdf}
}

@article{neves2015geostatistical,
  title = {Geostatistical {{Analysis}} in {{Extremes}}: {{An Overview}}},
  author = {Neves, M Manuela},
  year = {2015},
  journal = {Mathematics of Energy and Climate Change},
  pages = {229--245},
  publisher = {Springer},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Neves\neves2015geostatistical.pdf}
}

@techreport{ngm2020,
  title = {{{TECHNICAL REPORT ON THE CARLIN COMPLEX}}, {{EUREKA AND ELKO COUNTIES}}, {{STATE OF NEVADA}}, {{USA}}},
  author = {{Nevada Gold Mines LLC}},
  year = {2020},
  keywords = {thesis_02}
}

@article{nguyen2023dynamic,
  title = {A {{Dynamic Extreme Value Model}} with {{Application}} to {{Volcanic Eruption Forecasting}}},
  author = {Nguyen, Michele and Veraart, Almut E. D. and Taisne, Benoit and Tan, Chiou Ting and Lallemant, David},
  year = {2023},
  month = oct,
  journal = {Mathematical Geosciences},
  issn = {1874-8953},
  doi = {10.1007/s11004-023-10109-2},
  urldate = {2023-11-15},
  abstract = {Extreme events such as natural and economic disasters leave lasting impacts on society and motivate the analysis of extremes from data. While classical statistical tools based on Gaussian distributions focus on average behaviour and can lead to persistent biases when estimating extremes, extreme value theory (EVT) provides the mathematical foundations to accurately characterise extremes. This motivates the development of extreme value models for extreme event forecasting. In this paper, a dynamic extreme value model is proposed for forecasting volcanic eruptions. This is inspired by one recently introduced for financial risk forecasting with high-frequency data. Using a case study of the Piton de la Fournaise volcano, it is shown that the modelling framework is widely applicable, flexible and holds strong promise for natural hazard forecasting. The value of using EVT-informed thresholds to identify and model extreme events is shown through forecast performance, and considerations to account for the range of observed events are discussed.},
  langid = {english},
  keywords = {Extreme value theory,Forecasting,High-frequency data,Seismic data},
  file = {D:\03 UofA\06 Reading\_zotfile\Nguyen et al\nguyen2023dynamic.pdf}
}

@inproceedings{nowak2008generalized,
  title = {Generalized Binary Search},
  booktitle = {2008 46th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}}},
  author = {Nowak, Robert},
  year = {2008},
  month = sep,
  pages = {568--574},
  doi = {10.1109/ALLERTON.2008.4797609},
  urldate = {2024-02-17},
  abstract = {This paper studies a generalization of the classic binary search problem of locating a desired value within a sorted list. The classic problem can be viewed as determining the correct one-dimensional, binary-valued threshold function from a finite class of such functions based on queries taking the form of point samples of the function. The classic problem is also equivalent to a simple binary encoding of the threshold location. This paper extends binary search to learning more general binary-valued functions. Specifically, if the set of target functions and queries satisfy certain geometrical relationships, then an algorithm, based on selecting a query that is maximally discriminating at each step, will determine the correct function in a number of steps that is logarithmic in the number of functions under consideration. Examples of classes satisfying the geometrical relationships include linear separators in multiple dimensions. Extensions to handle noise are also discussed. Possible applications include machine learning, channel coding, and sequential experimental design.},
  keywords = {Channel coding,Design for experiments,Feedback,Machine learning,Particle separators,Probability distribution,Sampling methods,Search problems,thesis_05,Uncertainty},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Nowak\\nowak2008generalized.pdf;C\:\\Users\\benha\\Zotero\\storage\\4QDUPKVN\\4797609.html}
}

@inproceedings{nowak2013suggestions,
  title = {Suggestions for Good Capping Practices from Historical Literature},
  booktitle = {Proceedings of the 23rd {{World Mining Congress}} 2013},
  author = {Nowak, M. and Leuangthong, O. and Srivastava, R. M.},
  year = {2013},
  publisher = {{Canadian Institute of Mining, Metallurgy and Petroleum Montreal}},
  keywords = {thesis_02},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Nowak et al\nowak2013suggestions.pdf}
}

@incollection{nowak2019optimal,
  title = {Optimal Drill Hole Spacing for Resource Classification},
  booktitle = {Mining {{Goes Digital}}},
  author = {Nowak, Marek and Leuangthong, Oy},
  year = {2019},
  publisher = {CRC Press},
  abstract = {Regulatory bodies around the world have relied on similar definitions for mineral resource categories, and subdivide mineral resources in order of increasing geological confidence: Inferred, Indicated and Measured. Major investment and development decisions are based on a correct assignment of a resource to a proper category. The progression to mineral reserves reporting requires at least the categorization of Indicated and/or Measured resources. As such, the assignment of a mineral resource to an Indicated category should be recognized as a necessary step to advance towards the feasibility of a project.                      Allocation of mineral resource categories often relies on some geostatistical tools, of which drill hole spacing is perhaps most common. This paper looks at procedures to determine the optimal drill hole spacing required for allocation of a portion of resources to an Indicated category in metal deposits. These procedures are differentiated into two main groups based on their simplicity and stage of application: (1) general assessment of adequate drill hole spacing; and (2) local assessment of uncertainty on estimated grade within large panels representing monthly or quarterly production. At an early exploration stage general assessment of optimal drill hole spacing for an assignment to an Indicated category is quite adequate. On the other hand, at a pre-feasibility stage simulation studies represent the best tool for resource categorization. Practical examples from a case study the authors have worked on will be presented.},
  isbn = {978-0-429-32077-4},
  file = {D:\03 UofA\06 Reading\_zotfile\Nowak_Leuangthong\nowak2019optimal.pdf}
}

@article{ortiz2002calculation,
  title = {Calculation of {{Uncertainty}} in the {{Variogram}}},
  author = {Ortiz, Juli{\'a}n and Deutsch, Clayton V.},
  year = {2002},
  month = feb,
  journal = {Mathematical Geology},
  volume = {34},
  number = {2},
  pages = {169--183},
  issn = {1573-8868},
  doi = {10.1023/A:1014412218427},
  urldate = {2024-03-26},
  abstract = {There are often limited data available in early stages of geostatistical modeling. This leads to considerable uncertainty in statistical parameters including the variogram. This article presents an approach to calculate the uncertainty in the variogram. A methodology to transfer this uncertainty through geostatistical simulation and decision making is also presented.},
  langid = {english},
  keywords = {decision making,multi-Gaussian,multipoint statistics},
  file = {D:\03 UofA\06 Reading\_zotfile\Ortiz_Deutsch\ortiz2002calculation.pdf}
}

@phdthesis{ortiz2003characterization,
  title = {Characterization of High Order Correlation for Enhanced Indicator Simulation.},
  author = {Ortiz, Juli{\'a}n},
  year = {2003},
  file = {D:\03 UofA\06 Reading\_zotfile\Ortiz\ortiz2003characterization2.pdf}
}

@techreport{osiko2020,
  title = {{{NI}} 43-101 Technical Report and Mineral Resource Estimate for the Cariboo Gold Project, British Columbia, Canada},
  author = {{Osisko Gold Royalties Ltd}},
  year = {2020},
  keywords = {thesis_02}
}

@article{pang2022deep,
  title = {Deep {{Learning}} for {{Anomaly Detection}}: {{A Review}}},
  shorttitle = {Deep {{Learning}} for {{Anomaly Detection}}},
  author = {Pang, Guansong and Shen, Chunhua and Cao, Longbing and van den Hengel, Anton},
  year = {2022},
  month = mar,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {2},
  eprint = {2007.02500},
  primaryclass = {cs, stat},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3439950},
  urldate = {2024-04-16},
  abstract = {Anomaly detection, a.k.a. outlier detection or novelty detection, has been a lasting yet active research area in various research communities for several decades. There are still some unique problem complexities and challenges that require advanced approaches. In recent years, deep learning enabled anomaly detection, i.e., deep anomaly detection, has emerged as a critical direction. This paper surveys the research of deep anomaly detection with a comprehensive taxonomy, covering advancements in three high-level categories and 11 fine-grained categories of the methods. We review their key intuitions, objective functions, underlying assumptions, advantages and disadvantages, and discuss how they address the aforementioned challenges. We further discuss a set of possible future opportunities and new perspectives on addressing the challenges.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Pang et al\\pang2022deep.pdf;C\:\\Users\\benha\\Zotero\\storage\\5VKTMXJ2\\2007.html}
}

@article{pardo-iguzquiza2012varboot,
  title = {{{VARBOOT}}: {{A}} Spatial Bootstrap Program for Semivariogram Uncertainty Assessment},
  shorttitle = {{{VARBOOT}}},
  author = {{Pardo-Ig{\'u}zquiza}, Eulogio and Olea, Ricardo A.},
  year = {2012},
  month = apr,
  journal = {Computers \& Geosciences},
  volume = {41},
  pages = {188--198},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2011.09.002},
  urldate = {2024-03-26},
  abstract = {In applied geostatistics, the semivariogram is commonly estimated from experimental data, producing an empirical semivariogram for a specified number of discrete lags. In a second stage, a model defined by a few parameters is fitted to the empirical semivariogram. As the experimental data are usually few and sparsely located, there is considerable uncertainty about the calculated semivariogram values (uncertainty of the empirical semivariogram) and about the parameters of any model fitted to them (uncertainty of the estimated model parameters). In this paper, the uncertainty in the modeling of the empirical semivariogram is numerically assessed by the generalized bootstrap, which is an extension of the classic bootstrap procedure modified for spatially correlated data. A computer program is described and provided for the assessment of those uncertainties. In particular, the program provides for the empirical semivariogram: the standard errors, the bootstrap percentile confidence intervals, the complete variance--covariance matrix, standard deviation correlation matrix. A public domain, natural dataset is used to illustrate the performance of the program. A promising result is that, for any distance, the median of the bootstrap distribution for the empirical semivariogram approximates more closely the underlying semivariogram than the estimate derived from the empirical sample.},
  keywords = {Bootstrap percentile confidence interval,Correlated data,Model parameter uncertainty,Spatial covariance,Standard error},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Pardo-Igúzquiza_Olea\\pardo-iguzquiza2012varboot.pdf;C\:\\Users\\benha\\Zotero\\storage\\CKS5E4DN\\S0098300411003025.html}
}

@article{parker1991statistical,
  title = {Statistical Treatment of Outlier Data in Epithermal Gold Deposit Reserve Estimation},
  author = {Parker, {\relax HM}},
  year = {1991},
  journal = {Mathematical Geology},
  volume = {23},
  number = {2},
  pages = {175--199},
  publisher = {Springer},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Parker\parker1991statistical.pdf}
}

@techreport{parker2006,
  title = {Technical Report of the Rock Creek Property, Nome, Alaska, {{USA}}},
  author = {Parker, {\relax HM}},
  year = {2006},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Parker\parker2006.pdf}
}

@article{parrish1997geologist,
  title = {Geologist's Gordian Knot: {{To}} Cut or Not to Cut},
  author = {Parrish, I.S},
  year = {1997},
  journal = {Mining Engineering},
  volume = {49},
  pages = {45--49},
  keywords = {thesis_02},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Parrish\parrish1997geologist.pdf}
}

@techreport{pasofino2020,
  title = {{{DUGBE GOLD PROJECT}}, {{LIBERIA NI}} 43-101 {{TECHNICAL REPORT}}},
  author = {{Pasofino Gold Ltd.}},
  year = {2020}
}

@article{penunuri2016study,
  title = {A Study of the {{Classical Differential Evolution}} Control Parameters},
  author = {Pe{\~n}u{\~n}uri, F. and Cab, C. and Carvente, O. and {Zambrano-Arjona}, M. A. and Tapia, J. A.},
  year = {2016},
  month = feb,
  journal = {Swarm and Evolutionary Computation},
  volume = {26},
  pages = {86--96},
  issn = {2210-6502},
  doi = {10.1016/j.swevo.2015.08.003},
  urldate = {2023-12-06},
  abstract = {An extensive numerical study has been conducted to shed some light on the selection of parameters for the Classical Differential Evolution (DE/rand/1/bin) optimization method with the dither variant. It is well known that the crossover probability (Cr) has an active role in the convergence of the method. Our experiments show that even when the number of generations needed to achieve convergence as a function of the Cr parameter is of a stochastic nature, in some regions a reasonably well defined dependence of this number as a function of Cr can be observed. Motivated by this result, a self-adaptive DE methodology has been proposed. This new methodology applies the DE/rand/1/bin strategy itself to find a good value for the Cr parameter. Regarding the population size parameter, a phenomenological study involving the search space, the tolerance error, and the complexity of the function has been made. The proposed methodology has been applied to 10 of the most common test functions, giving the best success rate (100\% in all the studied examples) and in general a faster convergence than the classical DE/rand/1/bin strategy.},
  keywords = {Complexity,Control parameters,Differential evolution,Self-adaptive,Strategy},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Peñuñuri et al\\penunuri2016study.pdf;C\:\\Users\\benha\\Zotero\\storage\\NRLFUYS8\\S221065021500067X.html}
}

@article{pickands1975statistical,
  title = {Statistical Inference Using Extreme Order Statistics},
  author = {Pickands, James},
  year = {1975},
  journal = {Annals of statistics},
  volume = {3},
  number = {1},
  pages = {119--131},
  publisher = {Institute of Mathematical Statistics},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Pickands_others\pickands1975statistical.pdf}
}

@article{pimentel2014review,
  title = {A Review of Novelty Detection},
  author = {Pimentel, Marco AF and Clifton, David A. and Clifton, Lei and Tarassenko, Lionel},
  year = {2014},
  journal = {Signal Processing},
  volume = {99},
  pages = {215--249},
  publisher = {Elsevier},
  file = {D\:\\03 UofA\\04 Research\\02 PhD\\_zotfile\\Pimentel et al\\pimentel2014review.pdf;C\:\\Users\\benha\\Zotero\\storage\\UPWJB6M6\\S016516841300515X.html}
}

@phdthesis{pinto2020independent,
  title = {Independent {{Factor Simulation}} for {{Improved Multivariate Geostatistics}}},
  author = {Pinto, Felipe Cabral},
  year = {2020},
  langid = {english},
  school = {University of Alberta},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Pinto\pinto2020independent.pdf}
}

@article{piotrowski2017review,
  title = {Review of {{Differential Evolution}} Population Size},
  author = {Piotrowski, Adam P.},
  year = {2017},
  month = feb,
  journal = {Swarm and Evolutionary Computation},
  volume = {32},
  pages = {1--24},
  issn = {2210-6502},
  doi = {10.1016/j.swevo.2016.05.003},
  urldate = {2023-12-06},
  abstract = {Population size of Differential Evolution (DE) algorithms is often specified by user and remains fixed during run. During the first decade since the introduction of DE the opinion that its population size should be related to the problem dimensionality prevailed, later the approaches to DE population size setting diversified. In large number of recently introduced DE algorithms the population size is considered to be problem-independent and often fixed to 100 or 50 individuals, but alongside a number of DE variants with flexible population size have been proposed. The present paper briefly reviews the opinions regarding DE population size setting and verifies the impact of the population size on the performance of DE algorithms. Ten DE algorithms with fixed population size, each with at least five different population size settings, and four DE algorithms with flexible population size are tested on CEC2005 benchmarks and CEC2011 real-world problems. It is found that the inappropriate choice of the population size may severely hamper the performance of each DE algorithm. Although the best choice of the population size depends on the specific algorithm, number of allowed function calls and problem to be solved, some rough guidelines may be sketched. When the maximum number of function calls is set to classical values, i.e. those specified for CEC2005 and CEC2011 competitions, for low-dimensional problems (with dimensionality below 30) the population size equal to 100 individuals is suggested; population sizes smaller than 50 are rarely advised. For higher-dimensional artificial problems the population size should often depend on the problem dimensionality d and be set to 3d--5d. Unfortunately, setting proper population size for higher-dimensional real-world problems (d{$>$}40) turns out too problem and algorithm-dependent to give any general guide; 200 individuals may be a first guess, but many DE approaches would need a much different choice, ranging from 50 to 10d. However, quite clear relation between the population size and the convergence speed has been found, showing that the fewer function calls are available, the lower population sizes perform better. Based on the extensive experimental results the use of adaptive population size is highly recommended, especially for higher-dimensional and real-world problems. However, which specific algorithms with population size adaptation perform better depends on the number of function calls allowed.},
  keywords = {Adaptive control parameters,Differential Evolution,Evolutionary Algorithms,Metaheuristics,Population size},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Piotrowski\\piotrowski2017review.pdf;C\:\\Users\\benha\\Zotero\\storage\\9Z4QG6EA\\S2210650216300268.html}
}

@techreport{pretium2020,
  title = {Technical Report on the Brucejack Gold Mine, Northwest British Columbia},
  author = {{Pretium Resources Inc.}},
  year = {2020},
  keywords = {thesis_02}
}

@incollection{price2013differential,
  title = {Differential {{Evolution}}},
  booktitle = {Handbook of {{Optimization}}: {{From Classical}} to {{Modern Approach}}},
  author = {Price, Kenneth V.},
  editor = {Zelinka, Ivan and Sn{\'a}{\v s}el, V{\'a}clav and Abraham, Ajith},
  year = {2013},
  pages = {187--214},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-30504-7_8},
  urldate = {2024-03-15},
  abstract = {After an introduction that includes a discussion of the classic random walk, this paper presents a step-by-step development of the differential evolution (DE) global numerical optimization algorithm. Five fundamental DE strategies, each more complex than the last, are evaluated based on their conformance to invariance and symmetry principles, degree of control parameter dependence, computational efficiency and response to randomization. Optimal control parameter settings for the family of convex, quadratic functions are empirically derived.},
  isbn = {978-3-642-30504-7},
  langid = {english},
  keywords = {Differential Evolution,Differential Evolution Algorithm,Random Walk,Success Performance,Target Vector},
  file = {D:\03 UofA\06 Reading\_zotfile\Price\price2013differential.pdf}
}

@book{pyrcz2014geostatistical,
  title = {Geostatistical Reservoir Modeling},
  author = {Pyrcz, Michael J. and Deutsch, Clayton V.},
  year = {2014},
  publisher = {Oxford university press},
  file = {C:\Users\benha\Zotero\storage\3MK9UYA7\books.html}
}

@article{qu2018geostatistical,
  title = {Geostatistical {{Simulation}} with a {{Trend Using Gaussian Mixture Models}}},
  author = {Qu, Jianan and Deutsch, Clayton V.},
  year = {2018},
  month = jul,
  journal = {Natural Resources Research},
  volume = {27},
  number = {3},
  pages = {347--363},
  issn = {1573-8981},
  doi = {10.1007/s11053-017-9354-3},
  urldate = {2022-09-20},
  abstract = {Geostatistics applies statistics to quantitatively describe geological sites and assess the uncertainty due to incomplete sampling. Strong assumptions are required regarding the location independence of statistical parameters to construct numerical models with geostatistical tools. Most geological data exhibit large-scale deterministic trends together with short-scale variations. Such location dependence violates the common geostatistical assumption of stationarity. The trend-like deterministic features should be modeled prior to conventional geostatistical prediction and accounted for in subsequent geostatistical calculations. The challenge of using a trend in geostatistical simulation algorithms for the continuous variable is the subject of this paper. A stepwise conditional transformation with a Gaussian mixture model is considered to provide a stable and artifact-free numerical model. The complex features of the regionalized variable in the presence of a trend are removed in the forward transformation and restored in the back transformation. The Gaussian mixture model provides a seamless bin-free approach to transformation. Data from a copper deposit were used as an example. These data show an apparent trend unsuitable for conventional geostatistical algorithms. The result shows that the proposed algorithm leads to improved geostatistical models.},
  langid = {english},
  keywords = {Non-stationary regionalized variable,Sequential Gaussian simulation,Stepwise conditional transformation},
  file = {D:\03 UofA\06 Reading\_zotfile\Qu_Deutsch\qu2018geostatistical.pdf}
}

@article{qu2021anomaly,
  title = {Anomaly {{Detection}} in {{Hyperspectral Imagery Based}} on {{Gaussian Mixture Model}}},
  author = {Qu, Jiahui and Du, Qian and Li, Yunsong and Tian, Long and Xia, Haoming},
  year = {2021},
  month = nov,
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {59},
  number = {11},
  pages = {9504--9517},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2020.3038722},
  urldate = {2024-04-18},
  abstract = {Hyperspectral images (HSIs) with rich spectral information have been widely used in many fields. Anomaly detection is one of the most interesting and important applications. In this article, a novel Gaussian mixture model (GMM)-based anomaly detection (GMMD) method for HSI is proposed. The main contributions of this article are a new GMM-based extraction approach for extracting the anomaly pixels and an effective GMM-based weighting approach for fusing the extracted anomaly results. Specifically, based on the fact that the spectral values of anomaly pixels in some bands are different from those of background pixels, we propose a GMM-based anomaly extraction approach in which the HSI is characterized by the GMM and the anomaly pixels are extracted by a range prescribed by the GMM parameters. In order to fuse the extracted anomaly results, the GMM-based weighting method is introduced to adaptively construct the detection map. The detection map is rectified by using a guided filter to obtain the final anomaly detection map. Experimental results conducted on four hyperspectral data sets demonstrate the superior performance of the proposed GMMD method.},
  keywords = {Anomaly detection,anomaly extraction,band fusion,Correlation,Gaussian distribution,Gaussian mixture model,Gaussian mixture model (GMM),hyperspectral image (HSI),Hyperspectral imaging,Partitioning algorithms,Rivers},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Qu et al\\qu2021anomaly.pdf;C\:\\Users\\benha\\Zotero\\storage\\ZKRFD2JZ\\9277875.html}
}

@book{reiss2007statistical,
  title = {Statistical {{Analysis}} of {{Extreme Values}}: {{With Applications}} to {{Insurance}}, {{Finance}}, {{Hydrology}} and {{Other Fields}}},
  author = {Reiss, Rolf-Dieter and Thomas, Michael},
  year = {2007},
  publisher = {Springer Science \& Business Media},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Reiss_Thomas\reiss2007statistical.pdf}
}

@article{renard2011conditioning,
  title = {Conditioning {{Facies Simulations}} with {{Connectivity Data}}},
  author = {Renard, Philippe and Straubhaar, Julien and Caers, Jef and Mariethoz, Gr{\'e}goire},
  year = {2011},
  month = nov,
  journal = {Mathematical Geosciences},
  volume = {43},
  number = {8},
  pages = {879--903},
  issn = {1874-8961, 1874-8953},
  doi = {10.1007/s11004-011-9363-4},
  urldate = {2022-04-28},
  abstract = {When characterizing and simulating underground reservoirs for flow simulations, one of the key characteristics that needs to be reproduced accurately is its connectivity. More precisely, field observations frequently allow the identification of specific points in space that are connected. For example, in hydrogeology, tracer tests are frequently conducted that show which springs are connected to which sink-hole. Similarly well tests often allow connectivity information in a petroleum reservoir to be provided.},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Renard et al\renard2011conditioning.pdf}
}

@article{renard2013connectivity,
  title = {Connectivity Metrics for Subsurface Flow and Transport},
  author = {Renard, Philippe and Allard, Denis},
  year = {2013},
  month = jan,
  journal = {Advances in Water Resources},
  series = {35th {{Year Anniversary Issue}}},
  volume = {51},
  pages = {168--196},
  issn = {0309-1708},
  doi = {10.1016/j.advwatres.2011.12.001},
  urldate = {2024-02-14},
  abstract = {Understanding the role of connectivity for the characterization of heterogeneous porous aquifers or reservoirs is a very active and new field of research. In that framework, connectivity metrics are becoming important tools to describe a reservoir. In this paper, we provide a review of the various metrics that were proposed so far, and we classify them in four main groups. We define first the static connectivity metrics which depend only on the connectivity structure of the parameter fields (hydraulic conductivity or geological facies). By contrast, dynamic connectivity metrics are related to physical processes such as flow or transport. The dynamic metrics depend on the problem configuration and on the specific physics that is considered. Most dynamic connectivity metrics are directly expressed as a function of an upscaled physical parameter describing the overall behavior of the media. Another important distinction is that connectivity metrics can either be global or localized. The global metrics are not related to a specific location while the localized metrics relate to one or several specific points in the field. Using these metrics to characterize a given aquifer requires the possibility to measure dynamic connectivity metrics in the field, to relate them with static connectivity metrics, and to constrain models with those information. Some tools are already available for these different steps and reviewed here, but they are not yet routinely integrated in practical applications. This is why new steps should be added in hydrogeological studies to infer the connectivity structure and to better constrain the models. These steps must include specific field methodologies, interpretation techniques, and modeling tools to provide more realistic and more reliable forecasts in a broad range of applications.},
  keywords = {Connectivity,Effective permeability,Euler number,Heterogeneous media,Percolation,Review},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Renard_Allard\\renard2013connectivity.pdf;C\:\\Users\\benha\\Zotero\\storage\\RNC93ULY\\S0309170811002223.html}
}

@article{rios2013derivativefree,
  title = {Derivative-Free Optimization: A Review of Algorithms and Comparison of Software Implementations},
  shorttitle = {Derivative-Free Optimization},
  author = {Rios, Luis Miguel and Sahinidis, Nikolaos V.},
  year = {2013},
  month = jul,
  journal = {Journal of Global Optimization},
  volume = {56},
  number = {3},
  pages = {1247--1293},
  issn = {1573-2916},
  doi = {10.1007/s10898-012-9951-y},
  urldate = {2024-03-15},
  abstract = {This paper addresses the solution of bound-constrained optimization problems using algorithms that require only the availability of objective function values but no derivative information. We refer to these algorithms as derivative-free algorithms. Fueled by a growing number of applications in science and engineering, the development of derivative-free optimization algorithms has long been studied, and it has found renewed interest in recent time. Along with many derivative-free algorithms, many software implementations have also appeared. The paper presents a review of derivative-free algorithms, followed by a systematic comparison of 22 related implementations using a test set of 502 problems. The test bed includes convex and nonconvex problems, smooth as well as nonsmooth problems. The algorithms were tested under the same conditions and ranked under several criteria, including their ability to find near-global solutions for nonconvex problems, improve a given starting point, and refine a near-optimal solution. A total of 112,448 problem instances were solved. We find that the ability of all these solvers to obtain good solutions diminishes with increasing problem size. For the problems used in this study, TOMLAB/MULTIMIN, TOMLAB/GLCCLUSTER, MCS and TOMLAB/LGO are better, on average, than other derivative-free solvers in terms of solution quality within 2,500 function evaluations. These global solvers outperform local solvers even for convex problems. Finally, TOMLAB/OQNLP, NEWUOA, and TOMLAB/MULTIMIN show superior performance in terms of refining a near-optimal solution.},
  langid = {english},
  keywords = {Derivative-free algorithms,Direct search methods,Surrogate models},
  file = {D:\03 UofA\06 Reading\_zotfile\Rios_Sahinidis\rios2013derivativefree.pdf}
}

@article{rivoirard2013topcut,
  title = {A Top-Cut Model for Deposits with Heavy-Tailed Grade Distribution},
  author = {Rivoirard, Jacques and Demange, Claude and Freulon, Xavier and L{\'e}cureuil, Aur{\'e}lie and Bellot, Nicolas},
  year = {2013},
  journal = {Mathematical geosciences},
  volume = {45},
  number = {8},
  pages = {967--982},
  publisher = {Springer},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Rivoirard et al\rivoirard2013topcut.pdf}
}

@article{roberts1999novelty,
  title = {Novelty Detection Using Extreme Value Statistics},
  author = {Roberts, Stephen J},
  year = {1999},
  journal = {IEE Proceedings-Vision, Image and Signal Processing},
  volume = {146},
  number = {3},
  pages = {124--129},
  publisher = {IET},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Roberts\roberts1999novelty.pdf}
}

@article{roberts2000extreme,
  title = {Extreme Value Statistics for Novelty Detection in Biomedical Data Processing},
  author = {Roberts, Stephen J},
  year = {2000},
  journal = {IEE Proceedings-Science, Measurement and Technology},
  volume = {147},
  number = {6},
  pages = {363--367},
  publisher = {IET},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Roberts\roberts2000extreme.pdf}
}

@incollection{rojas1996backpropagation,
  title = {The {{Backpropagation Algorithm}}},
  booktitle = {Neural {{Networks}}: {{A Systematic Introduction}}},
  author = {Rojas, Ra{\'u}l},
  editor = {Rojas, Ra{\'u}l},
  year = {1996},
  pages = {149--182},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-61068-4_7},
  urldate = {2024-03-13},
  abstract = {We saw in the last chapter that multilayered networks are capable of computing a wider range of Boolean functions than networks with a single layer of computing units. However the computational effort needed for finding the correct combination of weights increases substantially when more parameters and more complicated topologies are considered. In this chapter we discuss a popular learning method capable of handling such large learning problems---the backpropagation algorithm. This numerical method was used by different research communities in different contexts, was discovered and rediscovered, until in 1985 it found its way into connectionist AI mainly through the work of the PDP group [382]. It has been one of the most studied and used algorithms for neural networks learning ever since.},
  isbn = {978-3-642-61068-4},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Rojas\rojas1996backpropagation.pdf}
}

@inproceedings{roscoe1996cutting,
  title = {Cutting Curves for Grade Estimation and Grade Control in Gold Mines},
  booktitle = {98th Annual General Meeting},
  author = {Roscoe, W.E},
  year = {1996},
  month = apr,
  publisher = {{Canadian Institute of Mining, Metallurgy and Petroleum}},
  keywords = {thesis_02},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Roscoe\roscoe1996cutting.pdf}
}

@book{rossi2013mineral,
  title = {Mineral Resource Estimation},
  author = {Rossi, Mario E. and Deutsch, Clayton V.},
  year = {2013},
  publisher = {Springer Science \& Business Media},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Rossi_Deutsch\\rossi2013mineral.pdf;C\:\\Users\\benha\\Zotero\\storage\\V3SMKWS5\\books.html}
}

@article{rousseeuw1999fast,
  title = {A {{Fast Algorithm}} for the {{Minimum Covariance Determinant Estimator}}},
  author = {Rousseeuw, Peter J. and Driessen, Katrien Van},
  year = {1999},
  month = aug,
  journal = {Technometrics},
  volume = {41},
  number = {3},
  pages = {212--223},
  publisher = {Taylor \& Francis},
  issn = {0040-1706},
  doi = {10.1080/00401706.1999.10485670},
  urldate = {2024-04-29},
  abstract = {The minimum covariance determinant (MCD) method of Rousseeuw is a highly robust estimator of multivariate location and scatter. Its objective is to find h observations (out of n) whose covariance matrix has the lowest determinant. Until now, applications of the MCD were hampered by the computation time of existing algorithms, which were limited to a few hundred objects in a few dimensions. We discuss two important applications of larger size, one about a production process at Philips with n = 677 objects and p = 9 variables, and a dataset from astronomy with n = 137,256 objects and p = 27 variables. To deal with such problems we have developed a new algorithm for the MCD, called FAST-MCD. The basic ideas are an inequality involving order statistics and determinants, and techniques which we call ``selective iteration'' and ``nested extensions.'' For small datasets, FAST-MCD typically finds the exact MCD, whereas for larger datasets it gives more accurate results than existing algorithms and is faster by orders of magnitude. Moreover, FASTMCD is able to detect an exact fit---that is, a hyperplane containing h or more observations. The new algorithm makes the MCD method available as a routine tool for analyzing multivariate data. We also propose the distance-distance plot (D-D plot), which displays MCD-based robust distances versus Mahalanobis distances, and illustrate it with some examples.},
  keywords = {Breakdown value,Multivariate location and scatter,Outlier detection,Regression,Robust estimation},
  file = {D:\03 UofA\06 Reading\_zotfile\Rousseeuw_Driessen\rousseeuw1999fast.pdf}
}

@article{rukhin2010statistical,
  title = {A {{Statistical Test Suite}} for {{Random}} and {{Pseudorandom Number Generators}} for {{Cryptographic Applications}}},
  author = {Rukhin, Andrew and Soto, Juan and Nechvatal, James and Barker, Elaine and Leigh, Stefan and Levenson, Mark and Banks, David and Heckert, Alan and Dray, James},
  year = {2010},
  pages = {131},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Rukhin et al\rukhin2010statistical.pdf}
}

@article{schlather2003dependence,
  title = {A Dependence Measure for Multivariate and Spatial Extreme Values: {{Properties}} and Inference},
  author = {Schlather, Martin and Tawn, Jonathan A},
  year = {2003},
  journal = {Biometrika},
  volume = {90},
  number = {1},
  pages = {139--156},
  publisher = {Oxford University Press},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Schlather_Tawn\schlather2003dependence.pdf}
}

@book{sen2013global,
  title = {Global {{Optimization Methods}} in {{Geophysical Inversion}}},
  author = {Sen, Mrinal K. and Stoffa, Paul L.},
  year = {2013},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511997570},
  urldate = {2024-04-11},
  abstract = {Providing an up-to-date overview of the most popular global optimization methods used in interpreting geophysical observations, this new edition includes a detailed description of the theoretical development underlying each method and a thorough explanation of the design, implementation and limitations of algorithms. New and expanded chapters provide details of recently developed methods, such as the neighborhood algorithm, particle swarm optimization, hybrid Monte Carlo and multi-chain MCMC methods. Other chapters include new examples of applications, from uncertainty in climate modeling to whole Earth studies. Several different examples of geophysical inversion, including joint inversion of disparate geophysical datasets, are provided to help readers design algorithms for their own applications. This is an authoritative and valuable text for researchers and graduate students in geophysics, inverse theory and exploration geoscience, and an important resource for professionals working in engineering and petroleum exploration.},
  isbn = {978-1-107-01190-8},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Sen_Stoffa\\sen2013global.pdf;C\:\\Users\\benha\\Zotero\\storage\\X9HZ8INL\\C2B23286E6BCC2177117431CB568101C.html}
}

@article{sharma2020activation,
  title = {{{ACTIVATION FUNCTIONS IN NEURAL NETWORKS}}},
  author = {Sharma, Siddharth and Sharma, Simone and Athaiya, Anidhya},
  year = {2020},
  month = may,
  journal = {International Journal of Engineering Applied Sciences and Technology},
  volume = {04},
  number = {12},
  pages = {310--316},
  issn = {24552143},
  doi = {10.33564/IJEAST.2020.v04i12.054},
  urldate = {2024-03-13},
  abstract = {Artificial Neural Networks are inspired from the human brain and the network of neurons present in the brain. The information is processed and passed on from one neuron to another through neuro synaptic junctions. Similarly, in artificial neural networks there are different layers of cells arranged and connected to each other. The output/information from the inner layers of the neural network are passed on to the next layers and finally to the outermost layer which gives the output. The input to the outer layer is provided nonlinearity to inner layers' output so that it can be further processed. In an Artificial Neural Network, activation functions are very important as they help in learning and making sense of non-linear and complicated mappings between the inputs and corresponding outputs.},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Sharma et al\sharma2020activation.pdf}
}

@inproceedings{siffer2017anomaly,
  title = {Anomaly Detection in Streams with Extreme Value Theory},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Siffer, Alban and Fouque, Pierre-Alain and Termier, Alexandre and Largouet, Christine},
  year = {2017},
  pages = {1067--1075},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Siffer et al\siffer2017anomaly.pdf}
}

@book{silva2014guide,
  title = {Guide to {{MPS Simulation}} with {{SNESIM}}},
  author = {Silva, Daniel A},
  year = {2014},
  series = {Centre for {{Computational Geostatistics}} ({{CCG}}) {{Guidebook Series}}},
  volume = {18},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Silva\silva2014guide.pdf}
}

@article{silva2017multiple,
  title = {Multiple Imputation Framework for Data Assignment in Truncated Pluri-{{Gaussian}} Simulation},
  author = {Silva, Diogo S. F. and Deutsch, Clayton V.},
  year = {2017},
  month = nov,
  journal = {Stochastic Environmental Research and Risk Assessment},
  volume = {31},
  number = {9},
  pages = {2251--2263},
  issn = {1436-3259},
  doi = {10.1007/s00477-016-1309-4},
  urldate = {2023-08-14},
  abstract = {Truncated pluri-Gaussian simulation (TPGS) is suitable for the simulation of categorical variables that show natural ordering as the TPGS technique can consider transition probabilities. The TPGS assumes that categorical variables are the result of the truncation of underlying latent variables. In practice, only the categorical variables are observed. This translates the practical application of TPGS into a missing data problem in which all latent variables are missing. Latent variables are required at data locations in order to condition categorical realizations to observed categorical data. The imputation of missing latent variables at data locations is often achieved by either assigning constant values or spatially simulating latent variables subject to categorical observations. Realizations of latent variables can be used to condition all model realizations. Using a single realization or a constant value to condition all realizations is the same as assuming that latent variables are known at the data locations and this assumption affects uncertainty near data locations. The techniques for imputation of latent variables in TPGS framework are investigated in this article and their impact on uncertainty of simulated categorical models and possible effects on factors affecting decision making are explored. It is shown that the use of single realization of latent variables leads to underestimation of uncertainty and overestimation of measured resources while the use constant values for latent variables may lead to considerable over or underestimation of measured resources. The results highlight the importance of multiple data imputation in the context of TPGS.},
  langid = {english},
  keywords = {Geomodeling,Geostatistics,Gibbs sampler,Missing data analysis,thesis_05},
  file = {D:\03 UofA\06 Reading\_zotfile\Silva_Deutsch\silva2017multiple.pdf}
}

@misc{silva2018enhanced,
  title = {Enhanced {{Geologic Modeling}} of {{Multiple Categorical Variables}}},
  author = {Silva, Diogo},
  year = 2018,
  journal = {ERA},
  doi = {10.7939/R30G3HD9R},
  urldate = {2023-08-15},
  abstract = {Widely spaced data sets from drilling are used in the mining and petroleum industries to model subsurface resources. These data sets have...},
  howpublished = {https://era.library.ualberta.ca/items/9ab8ab60-cb8f-4d7d-9551-3c602956b0ad},
  langid = {english},
  keywords = {thesis_05},
  file = {D:\03 UofA\06 Reading\_zotfile\Silva\silva2018enhanced.pdf}
}

@article{silva2018multivariate,
  title = {Multivariate Data Imputation Using {{Gaussian}} Mixture Models},
  author = {Silva, Diogo S. F. and Deutsch, Clayton V.},
  year = {2018},
  month = oct,
  journal = {Spatial Statistics},
  volume = {27},
  pages = {74--90},
  issn = {2211-6753},
  doi = {10.1016/j.spasta.2016.11.002},
  urldate = {2023-08-14},
  abstract = {Availability of high dimensional geological data has become common in the mining and petroleum industries. Data sets are often complex and require advanced multivariate geostatistical techniques. Multivariate data transformation is a common step of such advanced workflows and its application requires equally sampled (isotopic) data at all data locations. Samples with missing variables are common in geological data sets for many reasons. The missing data must be imputed (inferred) to permit the measured data to be used to their full extent. Imputation methods for geological data should address spatial structure and multivariate complexity. The published techniques that account for these considerations make strong assumptions regarding conditional distributions and are computationally demanding in presence of many data. A Gaussian mixture model fitted to the multivariate data is proposed in this paper to provide stability in fitting multivariate data and to significantly improve computational efficiency. The proposed approach is demonstrated using a lateritic Nickel data set. The proposed improvement is shown to decrease computational time by two orders of magnitude for the example while also consistently enhancing results in several performance tests.},
  keywords = {Geostatistics,Missing data analysis,Modeling,Semi-parametric,thesis_05},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Silva_Deutsch\\silva2018multivariate.pdf;C\:\\Users\\benha\\Zotero\\storage\\NMCSMPRH\\S2211675316301300.html}
}

@article{silva2021classification,
  title = {On the Classification and Treatment of Outliers in a Spatial Context: {{A Bayesian Updating}} Approach},
  shorttitle = {On the Classification and Treatment of Outliers in a Spatial Context},
  author = {Silva, Victor Miguel},
  year = {2021},
  month = jul,
  journal = {REM - International Engineering Journal},
  volume = {74},
  pages = {379--389},
  publisher = {Funda{\c c}{\~a}o Gorceix},
  issn = {2448-167X},
  doi = {10.1590/0370-44672021740003},
  urldate = {2022-01-17},
  abstract = {Abstract Checking and treating extreme values is commonplace in modelling workflows. The main methods to manage outliers may be categorized into graphical, Kriging- and simulation-based approaches. While graphical methods usually classify outliers from a global perspective, geostatistical methods evaluate outliers in a local context. Ordinary-Kriging based approaches are affected by conditional bias associated with the distribution tail(s), impacting on the correct classification of extreme values; the simulation method is based on the fact that geostatistical simulation is robust for outlier values. However, this approach ignores the interaction among outliers in the same neighborhood. The proposed approach considers that there are two values available at every sampled position, the sampled value and the conditional probability estimated from nearby data through cross-validation; the sampled value. Each value outside the user-defined threshold is classified as an outlier and is edited by merging the sampled and kriged value through Bayesian Updating. The proposed method is performed in normal-score units using Simple Kriging to (i) correctly estimate conditional distributions in the cross-validation step; (ii) avoid conditional bias; and (iii) minimize the outlier influence on experimental-variogram modelling. The proposed method is compared to three other widely used methods in a case study of a gold deposit. The proposed method substantially improved the local accuracy and reduced the number of misclassified blocks of a reference model.},
  langid = {english},
  keywords = {Bayesian updating,extreme values,geostatistics,outliers},
  file = {D\:\\03 UofA\\04 Research\\02 PhD\\_zotfile\\Silva\\silva2021classification.pdf;C\:\\Users\\benha\\Zotero\\storage\\ECSJQ6GY\\Q9gywB7zhHZHnpRHfcmq8bF.html}
}

@article{solow1985bootstrapping,
  title = {Bootstrapping Correlated Data},
  author = {Solow, Andrew R.},
  year = {1985},
  month = oct,
  journal = {Journal of the International Association for Mathematical Geology},
  volume = {17},
  number = {7},
  pages = {769--775},
  issn = {1573-8868},
  doi = {10.1007/BF01031616},
  urldate = {2024-04-15},
  langid = {english},
  keywords = {bootstrap,estimation variance,sampling distribution},
  file = {D:\03 UofA\06 Reading\_zotfile\Solow\solow1985bootstrapping.pdf}
}

@article{storn1997differential,
  title = {Differential {{Evolution}} -- {{A Simple}} and {{Efficient Heuristic}} for Global {{Optimization}} over {{Continuous Spaces}}},
  author = {Storn, Rainer and Price, Kenneth},
  year = {1997},
  month = dec,
  journal = {Journal of Global Optimization},
  volume = {11},
  number = {4},
  pages = {341--359},
  issn = {1573-2916},
  doi = {10.1023/A:1008202821328},
  urldate = {2024-04-30},
  abstract = {A new heuristic approach for minimizing possiblynonlinear and non-differentiable continuous spacefunctions is presented. By means of an extensivetestbed it is demonstrated that the new methodconverges faster and with more certainty than manyother acclaimed global optimization methods. The newmethod requires few control variables, is robust, easyto use, and lends itself very well to parallelcomputation.},
  langid = {english},
  keywords = {evolution strategy,genetic algorithm,global optimization,nonlinear optimization,Stochastic optimization},
  file = {D:\03 UofA\06 Reading\_zotfile\Storn_Price\storn1997differential.pdf}
}

@article{strebelle2002conditional,
  title = {Conditional {{Simulation}} of {{Complex Geological Structures Using Multiple-Point Statistics}}},
  author = {Strebelle, Sebastien},
  year = {2002},
  month = jan,
  journal = {Mathematical Geology},
  volume = {34},
  number = {1},
  pages = {1--21},
  issn = {1573-8868},
  doi = {10.1023/A:1014009426274},
  urldate = {2024-04-25},
  abstract = {In many earth sciences applications, the geological objects or structures to be reproduced are curvilinear, e.g., sand channels in a clastic reservoir. Their modeling requires multiple-point statistics involving jointly three or more points at a time, much beyond the traditional two-point variogram statistics. Actual data from the field being modeled, particularly if it is subsurface, are rarely enough to allow inference of such multiple-point statistics. The approach proposed in this paper consists of borrowing the required multiple-point statistics from training images depicting the expected patterns of geological heterogeneities. Several training images can be used, reflecting different scales of variability and styles of heterogeneities. The multiple-point statistics inferred from these training image(s) are exported to the geostatistical numerical model where they are anchored to the actual data, both hard and soft, in a sequential simulation mode. The algorithm and code developed are tested for the simulation of a fluvial hydrocarbon reservoir with meandering channels. The methodology proposed appears to be simple (multiple-point statistics are scanned directly from training images), general (any type of random geometry can be considered), and fast enough to handle large 3D simulation grids.},
  langid = {english},
  keywords = {geostatistics,random geometry,stochastic simulation,training image},
  file = {D:\03 UofA\06 Reading\_zotfile\Strebelle\strebelle2002conditional.pdf}
}

@article{tabatabaei2021robust,
  title = {Robust Outlier Detection in Geo-Spatial Data Based on {{LOLIMOT}} and {{KNN}} Search},
  author = {Tabatabaei, Mohammadreza and Kimiaefar, Roohollah and Hajian, Alireza and Akbari, Alireza},
  year = {2021},
  journal = {Earth Science Informatics},
  pages = {1--8},
  publisher = {Springer},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Tabatabaei et al\tabatabaei2021robust.pdf}
}

@incollection{tahmasebi2018multiple,
  title = {Multiple {{Point Statistics}}: {{A Review}}},
  shorttitle = {Multiple {{Point Statistics}}},
  booktitle = {Handbook of {{Mathematical Geosciences}}: {{Fifty Years}} of {{IAMG}}},
  author = {Tahmasebi, Pejman},
  editor = {Daya Sagar, B.S. and Cheng, Qiuming and Agterberg, Frits},
  year = {2018},
  pages = {613--643},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-78999-6_30},
  urldate = {2024-04-25},
  abstract = {Geostatistical modeling is one of the most important tools for building an ensemble of probable realizations in earth science. Among them, multiple-point statistics (MPS) has recently gone under a remarkable progress in handling complex and more realistic phenomenon that can produce large amount of the expected uncertainty and variability. Such progresses are mostly due to the recent increase in more advanced computational techniques/power. In this review chapter, the recent important developments in MPS are thoroughly reviewed. Furthermore, the advantages and disadvantages of each method are discussed as well. Finally, this chapter provides a brief review on the current challenges and paths that might be considered as future research.},
  isbn = {978-3-319-78999-6},
  langid = {english},
  keywords = {Conditioning Data Points,Raster Path,Single Normal Equation Simulation (SNESIM),SNESIM Algorithm,Tahmasebi},
  file = {D:\03 UofA\06 Reading\_zotfile\Tahmasebi\tahmasebi2018multiple.pdf}
}

@article{tamayomas2016testing,
  title = {Testing Geological Heterogeneity Representations for Enhanced Oil Recovery Techniques},
  author = {{Tamayo-Mas}, E. and Mustapha, H. and Dimitrakopoulos, R.},
  year = {2016},
  journal = {Journal of Petroleum Science and Engineering},
  volume = {146},
  pages = {222--240},
  issn = {0920-4105},
  doi = {10.1016/j.petrol.2016.04.027},
  abstract = {This paper analyzes the effects of geological heterogeneity representation in a producing reservoir, when different stochastic simulation methods are used, so as to assess the consequent effects on flow responses for different enhanced oil recovery (EOR) techniques employed. First, the spatial heterogeneity of a fluvial reservoir is simulated using three different stochastic methods: (1) the well-known two-point sequential Gaussian simulation (SGS), (2) a multiple-point filter-based algorithm (FILTERSIM), and (3) a new alternative high-order simulation method that uses high-order spatial statistics (HOSIM). Numerical results show that SGS suffers from the inability of describing the highly permeable channel network whereas FILTERSIM better reproduces this connectivity. By means of the recent HOSIM, a more appropriate description of the curvilinear high-permeability channels is obtained. Second, the realizations generated above represent permeability fields in EOR numerical simulations. In particular, four different methods are considered, namely: (1) surfactant, (2) polymer, (3) alkaline-surfactant-polymer and (4) foam flooding processes. The numerical results show that properly reproducing the main geological features of the reference images has a higher impact if surfactant or alkaline chemicals are injected rather than polymer or foam. This is due to the fact that these latter chemicals act by mitigating the effects of heterogeneities.},
  keywords = {Connectivity,Enhanced oil recovery,Geologic heterogeneity,Geostatistical algorithms},
  file = {D:\03 UofA\06 Reading\_zotfile\Tamayo-Mas et al\tamayomas2016testing.pdf}
}

@book{tarantola2005inverse,
  title = {Inverse Problem Theory and Methods for Model Parameter Estimation},
  author = {Tarantola, Albert},
  year = {2005},
  eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898717921},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898717921},
  file = {D:\03 UofA\06 Reading\_zotfile\Tarantola\tarantola2005inverse.pdf}
}

@techreport{tristar2021,
  title = {Mineral Resource Update for the Castelo de Sonhos Gold Project, Par{\'a} State, Brazil},
  author = {{TriStar Gold Inc.}},
  year = {2021},
  keywords = {thesis_02}
}

@book{tukey1977exploratory,
  title = {Exploratory Data Analysis},
  author = {Tukey, John W},
  year = {1977},
  volume = {2},
  publisher = {Reading, MA},
  keywords = {thesis_02},
  file = {D:\03 UofA\06 Reading\_zotfile\Tukey_others\tukey1977exploratory.pdf}
}

@article{unal2022evolutionary,
  title = {Evolutionary Design of Neural Network Architectures: A Review of Three Decades of Research},
  shorttitle = {Evolutionary Design of Neural Network Architectures},
  author = {{\"U}nal, Hamit Taner and Ba{\c s}{\c c}ift{\c c}i, Fatih},
  year = {2022},
  month = mar,
  journal = {Artificial Intelligence Review},
  volume = {55},
  number = {3},
  pages = {1723--1802},
  issn = {1573-7462},
  doi = {10.1007/s10462-021-10049-5},
  urldate = {2022-09-01},
  abstract = {We present a comprehensive review of the evolutionary design of neural network architectures. This work is motivated by the fact that the success of an Artificial Neural Network (ANN) highly depends on its architecture and among many approaches Evolutionary Computation, which is a set of global-search methods inspired by biological evolution has been proved to be an efficient approach for optimizing neural network structures. Initial attempts for automating architecture design by applying evolutionary approaches start in the late 1980s and have attracted significant interest until today. In this context, we examined the historical progress and analyzed all relevant scientific papers with a special emphasis on how evolutionary computation techniques were adopted and various encoding strategies proposed. We summarized key aspects of methodology, discussed common challenges, and investigated the works in chronological order by dividing the entire timeframe into three periods. The first period covers early works focusing on the optimization of simple ANN architectures with a variety of solutions proposed on chromosome representation. In the second period, the rise of more powerful methods and hybrid approaches were surveyed. In parallel with the recent advances, the last period covers the Deep Learning Era, in which research direction is shifted towards configuring advanced models of deep neural networks. Finally, we propose open problems for future research in the field of neural architecture search and provide insights for fully automated machine learning. Our aim is to provide a complete reference of works in this subject and guide researchers towards promising directions.},
  langid = {english},
  keywords = {Artificial intelligence,Artificial neural networks,Evolutionary computation,Machine learning,Optimization},
  file = {D:\03 UofA\06 Reading\_zotfile\Ünal_Başçiftçi\unal2022evolutionary.pdf}
}

@article{vincent2021mik,
  title = {{{MIK}} vs {{MG}} in {{Non-Gaussian Environments}}},
  author = {Vincent, Jeremy and Deutsch, Clayton},
  year = {2021},
  pages = {13},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Vincent_Deutsch\vincent2021mik.pdf}
}

@phdthesis{vincent2021multipleindicator,
  title = {Multiple-{{Indicator Kriging}} of {{Gaussian}} and {{Non-Gaussian Data}}},
  author = {Vincent, Jeremy D.},
  year = {2021},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Vincent\\vincent2021multipleindicator.pdf;C\:\\Users\\benha\\Zotero\\storage\\F8QA2H3Y\\60c554b1-02e4-40c0-b8c7-233eb3f0babe.html}
}

@incollection{wackernagel1988geostatistical,
  title = {Geostatistical {{Techniques}} for {{Interpreting Multivariate Spatial Information}}},
  booktitle = {Quantitative {{Analysis}} of {{Mineral}} and {{Energy Resources}}},
  author = {Wackernagel, Hans},
  editor = {Chung, C. F. and Fabbri, A. G. and {Sinding-Larsen}, R.},
  year = {1988},
  series = {{{NATO ASI Series}}},
  pages = {393--409},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-009-4029-1_24},
  urldate = {2021-11-04},
  abstract = {Several geostatistical techniques for exploring the structure of spatially distributed multivariate data are presented. The techniques are based on a combination of variogram modelling, principal component analysis and cokriging. Some possibilities to map the essential features of the multivariate spatial structure of the data are discussed. An example using geochemical data is given with an interpretation.},
  isbn = {978-94-009-4029-1},
  langid = {english},
  keywords = {Distance Class,Experimental Variogram,Geostatistical Technique,Spatial Component,Variogram Model},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Wackernagel\wackernagel1988geostatistical.pdf}
}

@article{wang2019progress,
  title = {Progress in Outlier Detection Techniques: {{A}} Survey},
  author = {Wang, Hongzhi and Bah, Mohamed Jaward and Hammad, Mohamed},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {107964--108000},
  publisher = {IEEE},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Wang et al\wang2019progress.pdf}
}

@techreport{Wilde2007,
  type = {{{CCG}} Annual Report 9},
  title = {Wide Array Declustering for Representative Distributions ({{The Ultimate DECLUS Program}})},
  author = {Wilde, B. J},
  year = {2007},
  address = {Edmonton AB},
  institution = {University of Alberta},
  keywords = {CCG,thesis_06}
}

@article{yan2020multivariate,
  title = {Multivariate Transformed {{Gaussian}} Processes},
  author = {Yan, Yuan and Jeong, Jaehong and Genton, Marc G.},
  year = {2020},
  journal = {Japanese Journal of Statistics and Data Science},
  volume = {3},
  number = {1},
  pages = {129--152},
  publisher = {Springer},
  file = {D\:\\03 UofA\\04 Research\\02 PhD\\_zotfile\\Yan et al\\yan2020multivariate.pdf;C\:\\Users\\benha\\Zotero\\storage\\VQFWCZQ8\\s42081-019-00068-6.html}
}

@book{yang2010nature,
  title = {Nature-Inspired Metaheuristic Algorithms},
  author = {Yang, Xin-She},
  year = {2010},
  publisher = {Luniver press},
  file = {D:\03 UofA\06 Reading\_zotfile\Yang\yang2010nature.pdf}
}

@book{yang2018nature,
  title = {Nature-Inspired Algorithms and Applied Optimization},
  author = {Yang, Xin-She},
  year = {2018},
  volume = {744},
  publisher = {Springer},
  file = {D:\03 UofA\06 Reading\_zotfile\Yang\yang2017nature.pdf}
}

@article{yao2020highorder,
  title = {High-{{Order Sequential Simulation}} via {{Statistical Learning}} in {{Reproducing Kernel Hilbert Space}}},
  author = {Yao, Lingqing and Dimitrakopoulos, Roussos and Gamache, Michel},
  year = {2020},
  month = jul,
  journal = {Mathematical Geosciences},
  volume = {52},
  number = {5},
  pages = {693--723},
  issn = {1874-8953},
  doi = {10.1007/s11004-019-09843-3},
  urldate = {2024-04-25},
  abstract = {The present work proposes a new high-order simulation framework based on statistical learning. The training data consist of the sample data together with a training image, and the learning target is the underlying random field model of spatial attributes of interest. The learning process attempts to find a model with expected high-order spatial statistics that coincide with those observed in the available data, while the learning problem is approached within the statistical learning framework in a reproducing kernel Hilbert space (RKHS). More specifically, the required RKHS is constructed via a spatial Legendre moment (SLM) reproducing kernel that systematically incorporates the high-order spatial statistics. The target distributions of the random field are mapped into the SLM-RKHS to start the learning process, where solutions of the random field model amount to solving a quadratic programming problem. Case studies with a known data set in different initial settings show that sequential simulation under the new framework reproduces the high-order spatial statistics of the available data and resolves the potential conflicts between the training image and the sample data. This is due to the characteristics of the spatial Legendre moment kernel and the generalization capability of the proposed statistical learning framework. A three-dimensional case study at a gold deposit shows practical aspects of the proposed method in real-life applications.},
  langid = {english},
  keywords = {High-order spatial statistics,Multipoint simulation,Reproducing kernel,Statistical learning,Stochastic simulation},
  file = {D:\03 UofA\06 Reading\_zotfile\Yao et al\yao2020highorder.pdf}
}

@article{yao2021learning,
  title = {Learning High-Order Spatial Statistics at Multiple Scales: {{A}} Kernel-Based Stochastic Simulation Algorithm and Its Implementation},
  shorttitle = {Learning High-Order Spatial Statistics at Multiple Scales},
  author = {Yao, Lingqing and Dimitrakopoulos, Roussos and Gamache, Michel},
  year = {2021},
  month = apr,
  journal = {Computers \& Geosciences},
  volume = {149},
  pages = {104702},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2021.104702},
  urldate = {2024-04-25},
  abstract = {This paper presents a learning-based stochastic simulation method that incorporates high-order spatial statistics at multiple scales from sources with different resolutions. Regarding the simulation of a certain spatial attribute, the high-order spatial information from different sources is encapsulated as aggregated kernel statistics in a spatial Legendre moment kernel space, and the probability distribution of the underlying random field model is derived by a statistical learning algorithm, which matches the high-order spatial statistics of the target model to the observed ones. In addition, a related software is developed as the SGeMS plugin. Case studies are conducted with a known data set and a gold deposit, demonstrating reproduction of high-order spatial statistics from the available data, as well as practical aspects in mining applications.},
  keywords = {Geostatistical simulation,High-order simulation software,High-order spatial statistics,Kernel,Statistical learning},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Yao et al\\yao2021learning.pdf;C\:\\Users\\benha\\Zotero\\storage\\TVJ3DYGT\\S0098300421000170.html}
}

@article{yao2021training,
  title = {Training {{Image Free High-Order Stochastic Simulation Based}} on {{Aggregated Kernel Statistics}}},
  author = {Yao, Lingqing and Dimitrakopoulos, Roussos and Gamache, Michel},
  year = {2021},
  month = oct,
  journal = {Mathematical Geosciences},
  volume = {53},
  number = {7},
  pages = {1469--1489},
  issn = {1874-8953},
  doi = {10.1007/s11004-021-09923-3},
  urldate = {2022-08-30},
  abstract = {A training image free, high-order sequential simulation method is proposed herein, which is based on the efficient inference of high-order spatial statistics from the available sample data. A statistical learning framework in kernel space is adopted to develop the proposed simulation method. Specifically, a new concept of aggregated kernel statistics is proposed to enable sparse data learning. The conditioning data in the proposed high-order sequential simulation method appear as data events corresponding to the attribute values associated with the so-called spatial templates of various geometric configurations. The replicates of the data events act as the training data in the learning framework for inference of the conditional probability distribution and generation of simulated values. These replicates are mapped into spatial Legendre moment kernel spaces, and the kernel statistics are computed thereafter, encapsulating the high-order spatial statistics from the available data. To utilize the incomplete information from the replicates, which partially match the spatial template of a given data event, the aggregated kernel statistics combine the ensemble of the elements in different kernel subspaces for statistical inference, embedding the high-order spatial statistics of the replicates associated with various spatial templates into the same kernel subspace. The aggregated kernel statistics are incorporated into a learning algorithm to obtain the target probability distribution in the underlying random field, while preserving in the simulations the high-order spatial statistics from the available data. The proposed method is tested using a synthetic dataset, showing the reproduction of the high-order spatial statistics of the sample data. The comparison with the corresponding high-order simulation method using TIs emphasizes the generalization capacity of the proposed method for sparse data learning.},
  langid = {english},
  keywords = {High-order sequential simulation,Kernel space,Spatial statistics,Statistical learning},
  file = {D:\03 UofA\06 Reading\_zotfile\Yao et al\yao2021training.pdf}
}

@article{zhou2014inverse,
  title = {Inverse Methods in Hydrogeology: {{Evolution}} and Recent Trends},
  shorttitle = {Inverse Methods in Hydrogeology},
  author = {Zhou, Haiyan and {G{\'o}mez-Hern{\'a}ndez}, J. Jaime and Li, Liangping},
  year = {2014},
  month = jan,
  journal = {Advances in Water Resources},
  volume = {63},
  pages = {22--37},
  issn = {0309-1708},
  doi = {10.1016/j.advwatres.2013.10.014},
  urldate = {2024-02-15},
  abstract = {Parameter identification is an essential step in constructing a groundwater model. The process of recognizing model parameter values by conditioning on observed data of the state variable is referred to as the inverse problem. A series of inverse methods has been proposed to solve the inverse problem, ranging from trial-and-error manual calibration to the current complex automatic data assimilation algorithms. This paper does not attempt to be another overview paper on inverse models, but rather to analyze and track the evolution of the inverse methods over the last decades, mostly within the realm of hydrogeology, revealing their transformation, motivation and recent trends. Issues confronted by the inverse problem, such as dealing with multiGaussianity and whether or not to preserve the prior statistics are discussed.},
  keywords = {Data assimilation,Groundwater modeling,Heterogeneity,Parameter identification,Uncertainty},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Zhou et al\\zhou2014inverse2.pdf;C\:\\Users\\benha\\Zotero\\storage\\W3HDII2X\\S0309170813002017.html}
}

@article{zimek2018there,
  title = {There and Back Again: {{Outlier}} Detection between Statistical Reasoning and Data Mining Algorithms},
  shorttitle = {There and Back Again},
  author = {Zimek, Arthur and Filzmoser, Peter},
  year = {2018},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {8},
  number = {6},
  pages = {e1280},
  issn = {1942-4795},
  doi = {10.1002/widm.1280},
  urldate = {2024-04-30},
  abstract = {Outlier detection has been a topic in statistics for centuries. Over mainly the last two decades, there has been also an increasing interest in the database and data mining community to develop scalable methods for outlier detection. Initially based on statistical reasoning, however, these methods soon lost the direct probabilistic interpretability of the derived outlier scores. Here, we detail from a joint point of view of data mining and statistics the roots and the path of development of statistical outlier detection and of database-related data mining methods for outlier detection. We discuss their inherent meaning, review approaches to again find a statistically meaningful interpretation of outlier scores, and sketch related current research topics. This article is categorized under: Algorithmic Development {$>$} Statistics Algorithmic Development {$>$} Scalable Statistical Methods Technologies {$>$} Machine Learning},
  copyright = {{\copyright} 2018 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {anomaly detection,outlier detection,outlier model,statistics and data mining},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Zimek_Filzmoser\\zimek2018there.pdf;C\:\\Users\\benha\\Zotero\\storage\\E94QWHSA\\widm.html}
}
