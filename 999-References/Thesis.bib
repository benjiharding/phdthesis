@book{armstrong2011plurigaussian,
  title = {Plurigaussian {{Simulations}} in {{Geosciences}}},
  author = {Armstrong, Margaret and Galli, Alain and Beucher, H{\'e}l{\`e}ne and Loc'h, Gaelle and Renard, Didier and Doligez, Brigitte and Eschard, R{\'e}mi and Geffroy, Francois},
  year = {2011},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-19607-2},
  urldate = {2024-02-14},
  isbn = {978-3-642-19606-5 978-3-642-19607-2},
  langid = {english},
  keywords = {Geostatistics,Mining,Petroleum,Simulations},
  file = {D:\03 UofA\06 Reading\_zotfile\Armstrong et al\armstrong2011plurigaussian.pdf}
}

@article{arroyo2020iterative,
  title = {Iterative Algorithms for Non-Conditional and Conditional Simulation of {{Gaussian}} Random Vectors},
  author = {Arroyo, Daisy and Emery, Xavier},
  year = {2020},
  month = oct,
  journal = {Stochastic Environmental Research and Risk Assessment},
  volume = {34},
  number = {10},
  pages = {1523--1541},
  issn = {1436-3259},
  doi = {10.1007/s00477-020-01875-0},
  urldate = {2024-02-13},
  abstract = {The conditional simulation of Gaussian random vectors is widely used in geostatistical~applications to quantify uncertainty in regionalized phenomena that have been observed at finitely many sampling locations. Two iterative algorithms are presented to deal with such a simulation. The first one is a variation of the propagative version of the Gibbs sampler aimed at simulating the random vector without any conditioning data. The novelty of the presented algorithm stems from the introduction of a relaxation parameter that, if adequately chosen, allows quickening the rates of convergence and mixing of the sampler. The second algorithm is meant to convert the non-conditional simulation into a conditional one, based on the successive over-relaxation method. Again, a relaxation parameter allows quickening the convergence in distribution to the desired conditional random vector. Both algorithms are applicable in a very general setting and avoid the pivoting, inversion, square rooting or decomposition of the variance-covariance matrix of the vector to be simulated, thus reduce the computation costs and memory requirements with respect to other discrete~geostatistical simulation approaches.},
  langid = {english},
  keywords = {Gauss-Seidel method,Gaussian random fields,Gibbs sampler,Mixing,Successive over-relaxation method},
  file = {D:\03 UofA\06 Reading\_zotfile\Arroyo_Emery\arroyo2020iterative.pdf}
}

@techreport{artemis2020,
  title = {Blackwater Gold Project British Columbia - {{NI}} 43-101 Technical Report on Pre-Feasibility Study},
  author = {{Artemis Gold Inc.}},
  year = {2020},
  keywords = {thesis_02}
}

@article{balkaya20173d,
  title = {{{3D}} Non-Linear Inversion of Magnetic Anomalies Caused by Prismatic Bodies Using Differential Evolution Algorithm},
  author = {Balkaya, {\c C}a{\u g}layan and Ekinci, Yunus Levent and G{\"o}kt{\"u}rkler, G{\"o}khan and Turan, Se{\c c}il},
  year = {2017},
  month = jan,
  journal = {Journal of Applied Geophysics},
  volume = {136},
  pages = {372--386},
  issn = {0926-9851},
  doi = {10.1016/j.jappgeo.2016.10.040},
  urldate = {2024-04-04},
  abstract = {3D non-linear inversion of total field magnetic anomalies caused by vertical-sided prismatic bodies has been achieved by differential evolution (DE), which is one of the population-based evolutionary algorithms. We have demonstrated the efficiency of the algorithm on both synthetic and field magnetic anomalies by estimating horizontal distances from the origin in both north and east directions, depths to the top and bottom of the bodies, inclination and declination angles of the magnetization, and intensity of magnetization of the causative bodies. In the synthetic anomaly case, we have considered both noise-free and noisy data sets due to two vertical-sided prismatic bodies in a non-magnetic medium. For the field case, airborne magnetic anomalies originated from intrusive granitoids at the eastern part of the Biga Peninsula (NW Turkey) which is composed of various kinds of sedimentary, metamorphic and igneous rocks, have been inverted and interpreted. Since the granitoids are the outcropped rocks in the field, the estimations for the top depths of two prisms representing the magnetic bodies were excluded during inversion studies. Estimated bottom depths are in good agreement with the ones obtained by a different approach based on 3D modelling of pseudogravity anomalies. Accuracy of the estimated parameters from both cases has been also investigated via probability density functions. Based on the tests in the present study, it can be concluded that DE is a useful tool for the parameter estimation of source bodies using magnetic anomalies.},
  keywords = {Differential evolution,Granitoids,Magnetic anomaly,Metaheuristic,Non-linear inversion,Prismatic bodies},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Balkaya et al\\balkaya20173d.pdf;C\:\\Users\\benha\\Zotero\\storage\\3T2CP34V\\S0926985116304347.html}
}

@techreport{banyan2020,
  title = {{{TECHNICAL REPORT ON THE AURMAC PROPERTY}}, {{MAYO MINING DISTRICT YUKON TERRITORY}}, {{CANADA}}},
  author = {{Banyan Gold Corp.}},
  year = {2020},
  keywords = {thesis_02}
}

@article{barnett2014projection,
  title = {Projection {{Pursuit Multivariate Transform}}},
  author = {Barnett, Ryan M. and Manchuk, John G. and Deutsch, Clayton V.},
  year = {2014},
  month = apr,
  journal = {Mathematical Geosciences},
  volume = {46},
  number = {3},
  pages = {337--359},
  issn = {1874-8953},
  doi = {10.1007/s11004-013-9497-7},
  urldate = {2022-09-20},
  abstract = {Transforming complex multivariate geological data to a Gaussian distribution is an important and challenging problem in geostatistics. A~variety of transforms are available for this goal, but struggle with high dimensional data sets. Projection pursuit density estimation (PPDE) is a well-established nonparametric method for estimating the joint density of multivariate data. A~central component of the PPDE algorithm transforms the original data toward a multivariate Gaussian distribution. The PPDE approach is modified to map complex data to a multivariate Gaussian distribution within a geostatistical modeling context. Traditional modeling may then take place on the transformed Gaussian data, with a back-transform used to return simulated variables to their original units. This approach is referred to as the projection pursuit multivariate transform (PPMT). The PPMT shows the potential to be an effective means for modeling high dimensional and complex geologic data. The PPMT algorithm is developed before discussing considerations and limitations. A~case study compares modeling results against more common techniques to demonstrate the value and place of the PPMT within geostatistics.},
  langid = {english},
  keywords = {Geostatistical Modeling,Kernel Density Estimation,Projection Index,Projection Pursuit,Radial Point Interpolation Method},
  file = {D:\03 UofA\06 Reading\_zotfile\Barnett et al\barnett2014projection.pdf}
}

@article{barnett2015multivariate,
  title = {Multivariate {{Imputation}} of {{Unequally Sampled Geological Variables}}},
  author = {Barnett, Ryan M. and Deutsch, Clayton V.},
  year = {2015},
  month = oct,
  journal = {Mathematical Geosciences},
  volume = {47},
  number = {7},
  pages = {791--817},
  issn = {1874-8953},
  doi = {10.1007/s11004-014-9580-8},
  urldate = {2023-08-14},
  abstract = {Unequally sampled data pose a practical and significant problem for geostatistical modeling. Multivariate transformations are frequently applied in modeling workflows to reproduce the multivariate relationships of geological data. Unfortunately, these transformations may only be applied to data observations that sample all of the variables. In the case of unequal sampling, practitioners must decide between excluding incomplete observations and imputing (inferring) the missing values. While imputation is recommended by missing data theorists, the use of deterministic methods such as regression is generally discouraged. Instead, techniques such as multiple imputation (MI) are advocated to increase the accuracy, decrease the bias, and capture the uncertainty of imputed values. As missing data theory has received little attention within geostatistical literature and practice, MI has not been adapted from its conventional form to be suitable for geological data. To address this, geostatistical algorithms are integrated within an MI framework to produce parametric and non-parametric methods. Synthetic and geometallurgical case studies are used to demonstrate the feasibility of each method, where techniques that use both spatial and colocated information are shown to outperform the alternatives.},
  langid = {english},
  keywords = {Geostatistics,Missing data analysis,Modeling,Statistics,thesis_05},
  file = {D:\03 UofA\06 Reading\_zotfile\Barnett_Deutsch\barnett2015multivariate.pdf}
}

@article{bilal2020differential,
  title = {Differential {{Evolution}}: {{A}} Review of More than Two Decades of Research},
  shorttitle = {Differential {{Evolution}}},
  author = {{Bilal} and Pant, Millie and Zaheer, Hira and {Garcia-Hernandez}, Laura and Abraham, Ajith},
  year = {2020},
  month = apr,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {90},
  pages = {103479},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2020.103479},
  urldate = {2024-03-15},
  abstract = {Since its inception in 1995, Differential Evolution (DE) has emerged as one of the most frequently used algorithms for solving complex optimization problems. Its flexibility and versatility have prompted several customized variants of DE for solving a variety of real life and test problems. The present study, surveys the near 25 years of existence of DE. In this extensive survey, 283 research articles have been covered and the journey of DE is shown through its basic aspects like population generation, mutation schemes, crossover schemes, variation in parameters and hybridized variants along with various successful applications of DE. This study also provides some key bibliometric indicators like highly cited papers having citations more than 500, publication trend since 1996, journal citations etc. The main aim of the present document is to serve as an extended summary of 25 years of existence of DE, intended for dissemination to interested parties. It is expected that the present survey would generate interest among the new users towards the philosophy of DE and would also guide the experience researchers.},
  keywords = {Crossover,Differential evolution,Meta-heuristics,Mutation,Selection},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Bilal et al\\bilal2020differential.pdf;C\:\\Users\\benha\\Zotero\\storage\\BHNXG4EA\\S095219762030004X.html}
}

@article{boisvert2007multiplepoint,
  title = {Multiple-{{Point Statistics}} for {{Training Image Selection}}},
  author = {Boisvert, Jeff B. and Pyrcz, Michael J. and Deutsch, Clayton V.},
  year = {2007},
  month = dec,
  journal = {Natural Resources Research},
  volume = {16},
  number = {4},
  pages = {313--321},
  issn = {1573-8981},
  doi = {10.1007/s11053-008-9058-9},
  urldate = {2022-05-26},
  abstract = {Selecting a training image (TI) that is representative of the target spatial phenomenon (reservoir, mineral deposit, soil type, etc.) is essential for an effective application of multiple-point statistics (MPS) simulation. It is often possible to narrow potential TIs to a general subset based on the available geological knowledge; however, this is largely subjective. A method is presented that compares the distribution of runs and the multiple-point density function from available exploration data and TIs. The difference in the MPS can be used to select the TI that is most representative of the data set. This tool may be applied to further narrow a suite of TIs for a more realistic model of spatial uncertainty. In addition, significant differences between the spatial statistics of local conditioning data and a TI may lead to artifacts in MPS. The utilization of this tool will identify contradictions between conditioning data and TIs. TI selection is demonstrated for a deepwater reservoir with 32 wells.},
  langid = {english},
  keywords = {Estimation,Geostatistics,Reserves,Runs,Uncertainty},
  file = {D:\03 UofA\06 Reading\_zotfile\Boisvert et al\boisvert2007multiplepoint.pdf}
}

@techreport{cardinal2019,
  title = {Namdini Gold Project Feasibility Study {{NI}} 43-101 Technical Report, Ghana, West Africa},
  author = {{Cardinal Resources}},
  year = {2019},
  keywords = {thesis_02}
}

@techreport{cartier2020,
  title = {{{NI}} 43-101 Technical Report and Mineral Resource Estimate for the Central, North and South Gold Corridors on the Chimo Mine Project, Qu{\'e}bec, Canada},
  author = {{Cartier Resources Inc.}},
  year = {2020},
  keywords = {thesis_02}
}

@misc{carvalho2017overview,
  title = {An {{Overview}} of {{Multiple Indicator Kriging}}},
  author = {Carvalho, Dhaniel and Deutsch, Clayton V},
  year = {2017},
  month = jan,
  urldate = {2022-10-03},
  howpublished = {https://geostatisticslessons.com/lessons/mikoverview},
  keywords = {thesis_02},
  file = {C:\Users\benha\Zotero\storage\J53QYQC4\mikoverview.html}
}

@techreport{cim2019,
  title = {{{CIM}} Estimation of Mineral Resources \& Mineral Reserves Best Practice Guidelines},
  author = {{CIM Mineral Resource \& Mineral Reserve Committee}},
  year = {2019},
  month = nov,
  institution = {{Canadian Institute of Mining, Metallurgy and Petroleum}},
  keywords = {thesis_02}
}

@book{conn2009introduction,
  title = {Introduction to Derivative-Free Optimization},
  author = {Conn, Andrew R and Scheinberg, Katya and Vicente, Luis N},
  year = {2009},
  publisher = {SIAM}
}

@article{davis1987production,
  title = {Production of Conditional Simulations via the {{LU}} Triangular Decomposition of the Covariance Matrix},
  author = {Davis, Michael W.},
  year = {1987},
  month = feb,
  journal = {Mathematical Geology},
  volume = {19},
  number = {2},
  pages = {91--98},
  issn = {1573-8868},
  doi = {10.1007/BF00898189},
  urldate = {2023-08-17},
  abstract = {This paper reviews the turning band method and fast Fourier transform method of producing a nonconditional simulation of a multinormal random function with a given covariance structure. A review of the two common methods of conditioning the simulation to honor the data shows that they are formally equivalent. Another method for directly pondering a conditional simulation based on the LU triangular decomposition of the covariance matrix is presented. Computational and implementation difficulties are discussed.},
  langid = {english},
  keywords = {conditional simulation,fast Fourier transform,geostatistics,kriging},
  file = {D:\03 UofA\06 Reading\_zotfile\Davis\davis1987productiona.pdf}
}

@phdthesis{deutsch1992annealing,
  title = {Annealing Techniques Applied to Reservoir Modeling and the Integration of Geological and Engineering (Well Test) Data},
  author = {Deutsch, Clayton Vernon},
  year = {1992},
  school = {stanford university},
  file = {D:\03 UofA\06 Reading\_zotfile\Deutsch\deutsch1992annealing.pdf}
}

@article{deutsch2010display,
  title = {Display of Cross Validation/Jackknife Results},
  author = {Deutsch, Clayton V},
  year = {2010},
  journal = {Centre for Computational Geostatistics Annual Report},
  volume = {12},
  number = {406},
  pages = {1--4},
  file = {D:\03 UofA\06 Reading\_zotfile\Deutsch\deutsch2010display.pdf}
}

@article{dubey2022activation,
  title = {Activation Functions in Deep Learning: {{A}} Comprehensive Survey and Benchmark},
  shorttitle = {Activation Functions in Deep Learning},
  author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
  year = {2022},
  month = sep,
  journal = {Neurocomputing},
  volume = {503},
  pages = {92--108},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.06.111},
  urldate = {2024-03-13},
  abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: https://github.com/shivram1987/ActivationFunctions.},
  keywords = {Activation Functions,Convolutional neural networks,Deep learning,Neural networks,Overview,Recurrent Neural Networks},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Dubey et al\\dubey2022activation.pdf;C\:\\Users\\benha\\Zotero\\storage\\ZZFULDLK\\S0925231222008426.html}
}

@book{eidsvik2015value,
  title = {Value of Information in the Earth Sciences: {{Integrating}} Spatial Modeling and Decision Analysis},
  author = {Eidsvik, Jo and Mukerji, Tapan and Bhattacharjya, Debarun},
  year = {2015},
  publisher = {Cambridge University Press}
}

@techreport{eldorado2020,
  title = {Technical Report Ki{\c s}lada{\u g} Gold Mine Turkey},
  author = {{Eldorado Gold Corporation}},
  year = {2020},
  keywords = {thesis_02}
}

@article{emery2014simulating,
  title = {Simulating {{Large Gaussian Random Vectors Subject}} to {{Inequality Constraints}} by {{Gibbs Sampling}}},
  author = {Emery, Xavier and Arroyo, Daisy and Pel{\'a}ez, Mar{\'i}a},
  year = {2014},
  month = apr,
  journal = {Mathematical Geosciences},
  volume = {46},
  number = {3},
  pages = {265--283},
  issn = {1874-8953},
  doi = {10.1007/s11004-013-9495-9},
  urldate = {2023-08-14},
  abstract = {The Gibbs sampler is an iterative algorithm used to simulate Gaussian random vectors subject to inequality constraints. This algorithm relies on the fact that the distribution of a vector component conditioned by the other components is Gaussian, the mean and variance of which are obtained by solving a kriging system. If the number of components is large, kriging is usually applied with a moving search neighborhood, but this practice can make the simulated vector not reproduce the target correlation matrix. To avoid these problems, variations of the Gibbs sampler are presented. The conditioning to inequality constraints on the vector components can be achieved by simulated annealing or by restricting the transition matrix of the iterative algorithm. Numerical experiments indicate that both approaches provide realizations that reproduce the correlation matrix of the Gaussian random vector, but some conditioning constraints may not be satisfied when using simulated annealing. On the contrary, the restriction of the transition matrix manages to satisfy all the constraints, although at the cost of a large number of iterations.},
  langid = {english},
  keywords = {Gaussian random field,Gibbs sampler,Kriging neighborhood,Markov chain,Restriction of transition matrix,Simulated annealing,thesis_05},
  file = {D:\03 UofA\06 Reading\_zotfile\Emery et al\emery2014simulating.pdf}
}

@book{everitt2010cambridge,
  title = {The Cambridge Dictionary of Statistics},
  author = {Everitt, B.S. and Skrondal, A.},
  year = {2010},
  publisher = {Cambridge University Press},
  isbn = {978-0-521-76699-9},
  lccn = {2010502891},
  keywords = {thesis_05},
  file = {D:\03 UofA\06 Reading\_zotfile\Everitt_Skrondal\everitt2010cambridge.pdf}
}

@techreport{fiore2021,
  title = {{{NI}} 43-101 Updated Technical Report on Resources and Reserves Pan Gold Project White Pine County, Nevada},
  author = {{Fiore Gold Ltd.}},
  year = {2021},
  keywords = {thesis_02}
}

@misc{fisher2019all,
  title = {All {{Models}} Are {{Wrong}}, but {{Many}} Are {{Useful}}: {{Learning}} a {{Variable}}'s {{Importance}} by {{Studying}} an {{Entire Class}} of {{Prediction Models Simultaneously}}},
  shorttitle = {All {{Models}} Are {{Wrong}}, but {{Many}} Are {{Useful}}},
  author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
  year = {2019},
  month = dec,
  number = {arXiv:1801.01489},
  eprint = {1801.01489},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.01489},
  urldate = {2023-08-10},
  abstract = {Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model \$f({\textbackslash}mathbf\{x\})={\textbackslash}mathbf\{x\}\^{}\{T\}{\textbackslash}beta\$ with a fixed coefficient vector \${\textbackslash}beta\$) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Fisher et al\\fisher2019all.pdf;C\:\\Users\\benha\\Zotero\\storage\\48WPPFJ8\\1801.html}
}

@article{geman1984stochastic,
  title = {Stochastic {{Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}},
  author = {Geman, Stuart and Geman, Donald},
  year = {1984},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-6},
  number = {6},
  pages = {721--741},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1984.4767596},
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  keywords = {Additive noise,Annealing,Bayesian methods,Deformable models,Degradation,Energy states,Gibbs distribution,image restoration,Image restoration,line process,MAP estimate,Markov random field,Markov random fields,relaxation,scene modeling,spatial degradation,Stochastic processes,Temperature distribution,thesis_05},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Geman_Geman\\geman1984stochastic.pdf;C\:\\Users\\benha\\Zotero\\storage\\YY8YSXS4\\4767596.html}
}

@article{georgioudakis2020comparative,
  title = {A {{Comparative Study}} of {{Differential Evolution Variants}} in {{Constrained Structural Optimization}}},
  author = {Georgioudakis, Manolis and Plevris, Vagelis},
  year = {2020},
  journal = {Frontiers in Built Environment},
  volume = {6},
  issn = {2297-3362},
  urldate = {2023-08-08},
  abstract = {Differential evolution (DE) is a population-based metaheuristic search algorithm that optimizes a problem by iteratively improving a candidate solution based on an evolutionary process. Such algorithms make few or no assumptions about the underlying optimization problem and can quickly explore very large design spaces. DE is arguably one of the most versatile and stable population-based search algorithms that exhibits robustness to multi-modal problems. In the field of structural engineering, most practical optimization problems are associated with one or several behavioral constraints. Constrained optimization problems are quite challenging to solve due to their complexity and high nonlinearity. In this work we examine the performance of several DE variants, namely the standard DE, the composite DE (CODE), the adaptive DE with optional external archive (JADE) and the self-adaptive DE (JDE and SADE), for handling constrained structural optimization problems associated with truss structures. The performance of each DE variant is evaluated by using five well-known benchmark structures in 2D and 3D. The evaluation is done on the basis of final optimum result and the rate of convergence. Valuable conclusions are obtained from the statistical analysis which can help a structural engineer in practice to choose the suitable algorithm for such kind of problems.},
  file = {D:\03 UofA\06 Reading\_zotfile\Georgioudakis_Plevris\georgioudakis2020comparative.pdf}
}

@incollection{gomez-hernandez1993joint,
  title = {Joint {{Sequential Simulation}} of {{MultiGaussian Fields}}},
  booktitle = {Geostatistics {{Tr{\'o}ia}} '92: {{Volume}} 1},
  author = {{G{\'o}mez-Hern{\'a}ndez}, J. Jaime and Journel, Andr{\'e} G.},
  editor = {Soares, Amilcar},
  year = {1993},
  series = {Quantitative {{Geology}} and {{Geostatistics}}},
  pages = {85--94},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-011-1739-5_8},
  urldate = {2023-08-17},
  abstract = {The sequential simulation algorithm can be used for the generation of conditional realizations from either a multiGaussian random function or any non-Gaussian random function as long as its conditional distributions can be derived. The multivariate probability density function (pdf) that fully describes a random function can be written as the product of a set of univariate conditional pdfs. Drawing realizations from the multivariate pdf amounts to drawing sequentially from that series of univariate conditional pdfs. Similarly, the joint multivariate pdf of several random functions can be written as the product of a series of univariate conditional pdfs. The key step consists of the derivation of the conditional pdfs. In the case of a multiGaussian fields, these univariate conditional pdfs are known to be Gaussian with mean and variance given by the solution of a set of normal equations also known as simple cokriging equations. Sequential simulation is preferred to other techniques, such as turning bands, because of its ease of use and extreme flexibility.},
  isbn = {978-94-011-1739-5},
  langid = {english},
  keywords = {Conditional Distribution,Conditioning Data,Exponential Type,Search Neighborhood,Sequential Simulation},
  file = {D:\03 UofA\06 Reading\_zotfile\Gómez-Hernández_Journel\gomez-hernandez1993joint.pdf}
}

@article{gomez-hernandez1998be,
  title = {To Be or Not to Be Multi-{{Gaussian}}? {{A}} Reflection on Stochastic Hydrogeology},
  shorttitle = {To Be or Not to Be Multi-{{Gaussian}}?},
  author = {{G{\'o}mez-Hern{\'a}ndez}, J. Jaime and Wen, Xian-Huan},
  year = {1998},
  month = feb,
  journal = {Advances in Water Resources},
  volume = {21},
  number = {1},
  pages = {47--61},
  issn = {0309-1708},
  doi = {10.1016/S0309-1708(96)00031-0},
  urldate = {2024-02-14},
  abstract = {The multivariate Gaussian random function model is commonly used in stochastic hydrogeology to model spatial variability of log-conductivity. The multi-Gaussian model is attractive because it is fully characterized by an expected value and a covariance function or matrix, hence its mathematical simplicity and easy inference. Field data may support a Gaussian univariate distribution for log hydraulic conductivity, but, in general, there are not enough field data to support a multi-Gaussian distribution. A univariate Gaussian distribution does not imply a multi-Gaussian model. In fact, many multivariate models can share the same Gaussian histogram and covariance function, yet differ by their patterns of spatial continuity at different threshold values. Hence the decision to use a multi-Gaussian model to represent the uncertainty associated with the spatial heterogeneity of log-conductivity is not databased. Of greatest concern is the fact that a multi-Gaussian model implies the minimal spatial correlation of extreme values, a feature critical for mass transport and a feature that may be in contradiction with some geological settings, e.g. channeling. The possibility for high conductivity values to be spatially correlated should not be discarded by adopting a congenial model just because data shortage prevents refuting it. In this study, three alternatives to a multi-Gaussian model, all sharing the same Gaussian histogram and the same covariance function, but with different continuity patterns for extreme values, were considered to model the spatial variability of log-conductivity. The three alternative models, plus the traditional multi-Gaussian model, are used to perform Monte Carlo analyses of groundwater travel times from a hypothetical nuclear repository to the ground surface through a synthetic formation similar to the Finnsj{\"o}n site in Sweden. The results show that the groundwater travel times predicted by the multi-Gaussian model could be ten times slower than those predicted by the other models. The probabilities of very short travel times could be severely underestimated using the multi-Gaussian model. Consequently, if field measured data are not sufficient to determine the higher-order moments necessary to validate the multi-Gaussian model --- which is the usual situation in practice --- other alternative models to the multi-Gaussian one ought to be considered.},
  keywords = {geostatistics,Heterogeneity,mass transport,Monte Carlo simulation,non-multi-Gaussian,risk analysis,stochastic simulation,travel time,uncertainty},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Gómez-Hernández_Wen\\gomez-hernandez1998be.pdf;C\:\\Users\\benha\\Zotero\\storage\\RK8KZZ6J\\S0309170896000310.html}
}

@article{goovaerts1992factorial,
  title = {Factorial Kriging Analysis: A Useful Tool for Exploring the Structure of Multivariate Spatial Soil Information},
  shorttitle = {Factorial Kriging Analysis},
  author = {Goovaerts, P.},
  year = {1992},
  journal = {Journal of Soil Science},
  volume = {43},
  number = {4},
  pages = {597--619},
  issn = {1365-2389},
  doi = {10.1111/j.1365-2389.1992.tb00163.x},
  urldate = {2021-10-16},
  abstract = {Most studies of relations between soil properties fail to take account of their regionalized nature because of the lack of appropriate methods. This paper describes a geostatistical technique, factorial kriging analysis, that bridges the gap between classical multivariate analysis and a univariate geostatistical approach. The basic feature of the method is the fitting of a linear model of coregionalization, i.e. all experimental simple and cross-variograms are modelled with a linear combination of basic variogram functions. A particular variance-covariance matrix, the coregionalization matrix, can then be associated with each spatial scale defined by the range of the basic variogram function. Each coregionalization matrix describes relationships between variables at a given spatial scale. A principal component analysis of these matrices produces a set of components, the regionalized factors, that reflect the main features of the multivariate information for each spatial scale and whose scores are estimated by cokriging. The technique is described and illustrated with three case studies based on a simulated data set and soil survey data. The results are compared with those of the principal component analysis of the variance-covariance matrix and the variogram matrices.},
  langid = {english},
  keywords = {thesis_05},
  file = {D\:\\03 UofA\\04 Research\\02 PhD\\_zotfile\\Goovaerts\\goovaerts1992factorial.pdf;C\:\\Users\\benha\\Zotero\\storage\\RI54W7CF\\j.1365-2389.1992.tb00163.html}
}

@book{goovaerts1997geostatistics,
  title = {Geostatistics for Natural Resources Evaluation},
  author = {Goovaerts, Pierre},
  year = {1997},
  publisher = {Oxford University Press on Demand},
  file = {C:\Users\benha\Zotero\storage\6WBXREIM\books.html}
}

@incollection{guardiano1993multivariate,
  title = {Multivariate {{Geostatistics}}: {{Beyond Bivariate Moments}}},
  shorttitle = {Multivariate {{Geostatistics}}},
  booktitle = {Geostatistics {{Tr{\'o}ia}} '92},
  author = {Guardiano, Felipe B. and Srivastava, R. Mohan},
  editor = {Gradstein, F. M. and Soares, Amilcar},
  year = {1993},
  volume = {5},
  pages = {133--144},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-011-1739-5_12},
  urldate = {2022-05-26},
  isbn = {978-0-7923-2157-6 978-94-011-1739-5},
  langid = {english},
  file = {C:\Users\benha\Zotero\storage\Z8UQKRWQ\Guardiano and Srivastava - 1993 - Multivariate Geostatistics Beyond Bivariate Momen.pdf}
}

@phdthesis{guthke2013non,
  title = {Non-Multi-{{Gaussian}} Spatial Structures: Process-Driven Natural Genesis, Manifestation, Modeling Approaches, and Influences on Dependent Processes},
  author = {Guthke, Philipp},
  year = {2013},
  file = {D:\03 UofA\06 Reading\_zotfile\Guthke\guthke2013non.pdf}
}

@article{hadavand2023spatial,
  title = {Spatial Multivariate Data Imputation Using Deep Learning and Lambda Distribution},
  author = {Hadavand, Mostafa and Deutsch, Clayton V.},
  year = {2023},
  month = aug,
  journal = {Computers \& Geosciences},
  volume = {177},
  pages = {105376},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2023.105376},
  urldate = {2024-02-14},
  abstract = {Artificial neural networks (ANNs) are often used to establish a mapping between an input data set and a corresponding output. There are many applications that rely on quantifying the conditional distribution of the output given the input data set. This is often referred to as aleatoric uncertainty associated with variability of the outcome due to inherently random effects. In this paper, deep learning is used to quantify moments of the conditional distribution of a missing variable based on homotopic multivariate observations. The lambda distribution is then used to parametrize the conditional distribution based on the provided moments. Geostatistical quantification of spatial continuity complements the multivariate conditional distribution through Bayesian updating to inform multiple data imputation that accounts for the uncertainty associated with the missing variable(s). Geological data are often incomplete, and data imputation is an essential step to avoid excluding heterotopic data. The proposed data imputation framework trains multi layer perceptron (MLP) neural networks to characterize multivariate relationships inferred from homotopic training data. A case study is conducted using geological data from a lateritic Nickle deposit to demonstrate application of the proposed methodology.},
  keywords = {Gaussian mixture model,Geostatistics,Simulation},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Hadavand_Deutsch\\hadavand2023spatial.pdf;C\:\\Users\\benha\\Zotero\\storage\\ZEKJPSLQ\\S0098300423000808.html}
}

@article{journel1974geostatistics,
  title = {Geostatistics for {{Conditional Simulation}} of {{Ore Bodies}}},
  author = {Journel, A. G.},
  year = {1974},
  month = aug,
  journal = {Economic Geology},
  volume = {69},
  number = {5},
  pages = {673--687},
  issn = {0361-0128},
  doi = {10.2113/gsecongeo.69.5.673},
  urldate = {2022-08-30},
  abstract = {Simulation techniques are frequently used to solve various problems of operational research for the mining industry and more generally in earth sciences (hydrogeology, gravimetry, meteorology, etc.). First, the model to be simulated is characterized; for example, the spatial dispersion of grades in an ore body. Then a simulation technique is devised, which must be operational, particularly in terms of computer time. The efficiency of the simulation produced is obviously linked to the capacity of the model to fit the main characteristics of the revealed reality. One of the most important of these characteristics, namely the spatial autocorrelation of variables, is often ignored by the models commonly presented in classical literature.The originality of conditional simulation derives: (1) from the fact that these simulations meet the particular spatial autocorrelation function (covariance or variogram) which characterizes the reality observed; (2) from the conditionalization of the experimental data, i.e., the simulated values at data locations equal the experimental values; and (3) from the possibility of working in real three-dimensional space. The simulation technique proposed (turning-bands method) consists of simulating on lines (one-dimensional space) and then turning these lines in three-dimensional space. This procedure avoids the well-known explosion of computer time and memories involved in classical procedures extended to several dimensions. The originality of using conditional simulation techniques with regard to spectral analysis techniques is presented in the third point.},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Journel\\journel1974geostatistics.pdf;C\:\\Users\\benha\\Zotero\\storage\\DGCFIMF5\\Geostatistics-for-Conditional-Simulation-of-Ore.html}
}

@article{journel1983nonparametric,
  title = {Nonparametric Estimation of Spatial Distributions},
  author = {Journel, A. G.},
  year = {1983},
  month = jun,
  journal = {Journal of the International Association for Mathematical Geology},
  volume = {15},
  number = {3},
  pages = {445--468},
  issn = {1573-8868},
  doi = {10.1007/BF01031292},
  urldate = {2021-10-22},
  abstract = {The indicator approach, whereby the data are used through their rank order, allows a nonparametric approach to the data bivariate distribution. Such rich structural information allows a nonparametric risk-qualified, estimation of local and global spatial distributions.},
  langid = {english},
  keywords = {thesis_02},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Journel\journel1983nonparametric.pdf}
}

@article{journel1989nongaussian,
  title = {Non-{{Gaussian}} Data Expansion in the {{Earth Sciences}}},
  author = {Journel, A. G. and Alabert, F.},
  year = {1989},
  journal = {Terra Nova},
  volume = {1},
  number = {2},
  pages = {123--134},
  issn = {1365-3121},
  doi = {10.1111/j.1365-3121.1989.tb00344.x},
  urldate = {2021-10-22},
  abstract = {A formalism is proposed to generate alternative equiprobable images of an underlying population spatial distribution. The resulting images honour data values at their locations and reflect important characteristics of the data such as patterns of spatial connectivity of extreme-values. The formalism capitalizes on a coding of all information available into bits (0-l), which are then processed all together accounting for their patterns of correlation in space. Such common coding allows accounting for qualitative information, possibly of an interpretative nature, to complement the usually sparse hard data available in Earth Sciences applications. The approach proposed, although of a probabilistic nature, does not call for any Gaussian-type modelling or hypothesis.},
  langid = {english},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Journel_Alabert\\journel1989nongaussian.pdf;C\:\\Users\\benha\\Zotero\\storage\\BCVDMGVT\\j.1365-3121.1989.tb00344.html}
}

@article{journel1993entropy,
  title = {Entropy and Spatial Disorder},
  author = {Journel, Andr{\'e} G. and Deutsch, Clayton V.},
  year = {1993},
  month = apr,
  journal = {Mathematical Geology},
  volume = {25},
  number = {3},
  pages = {329--355},
  issn = {1573-8868},
  doi = {10.1007/BF00901422},
  urldate = {2021-10-22},
  abstract = {The majority of geostatistical estimation and simulation algorithms rely on a covariance model as the sole characteristic of the spatial distribution of the attribute under study. The limitation to a single covariance implicitly calls for a multivariate Gaussian model for either the attribute itself or for its normal scores transform. The Gaussian model could be justified on the basis that it is both analytically simple and it is a maximum entropy model, i.e., a model that minimizes unwarranted structural properties. As a consequence, the Gaussian model also maximizes spatial disorder (beyond the imposed covariance) which can cause flow simulation results performed on multiple stochastic images to be very similar; thus, the space of response uncertainty could be too narrow entailing a misleading sense of safety. The ability of the sole covariance to adequately describe spatial distributions for flow studies, and the assumption that maximum spatial disorder amounts to either no additional information or a safe prior hypothesis are questioned. This paper attempts to clarify the link between entropy and spatial disorder and to provide, through a detailed case study, an appreciation for the impact of entropy of prior random function models on the resulting response distributions.},
  langid = {english},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Journel_Deutsch\journel1993entropy.pdf}
}

@inproceedings{lantuejoul2012simulation,
  title = {Simulation of a {{Gaussian}} Random Vector: A Propagative Version of the {{Gibbs}} Sampler},
  booktitle = {The 9th International Geostatistics Congress},
  author = {Lantu{\'e}joul, Christian and Desassis, Nicolas},
  year = {2012},
  pages = {174--181},
  file = {D:\03 UofA\06 Reading\_zotfile\Lantuéjoul_Desassis\lantuejoul2012simulation.pdf}
}

@article{lauzon2020calibration,
  title = {Calibration of Random Fields by a Sequential Spectral Turning Bands Method},
  author = {Lauzon, Dany and Marcotte, Denis},
  year = {2020},
  month = feb,
  journal = {Computers \& Geosciences},
  volume = {135},
  pages = {104390},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2019.104390},
  urldate = {2024-02-13},
  abstract = {A new algorithm for calibration of conditional realizations to measured or desired response functions is presented. The Sequential-Spectral Turning Bands Method (S-STBM) builds the field by choosing the phase of each new cosine function such that the observed field response functions become increasingly calibrated. The phase selection has little influence on the spatial correlation structure but can help to meet other objectives. Conditioning by kriging is used in the algorithm main loop to impose exact hard data reproduction. A first case study illustrates the performance of the algorithm for a cyclic and asymmetric field. S-STBM is shown to reproduce similarly or better the directional asymmetry than calibrated realizations obtained by FFTMA-SA. A training image (TI) with connected low values provides the second case study where the target is the reproduction of non-centered third-order spatial moments. A third case study shows the effectiveness of the S-STBM algorithm to calibrate a Gaussian field to tracer tests. Contrary to FFTMA-SA, S-STBM works on irregular grids. Its computational complexity of O(n) and small memory requirement makes it an attractive method for calibration.},
  keywords = {Conditional simulation,Constructive calibration,High-order statistics,Spectral turning bands},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Lauzon_Marcotte\\lauzon2020calibration.pdf;C\:\\Users\\benha\\Zotero\\storage\\WKMIAIZG\\S0098300419306752.html}
}

@article{lauzon2020sequential,
  title = {The Sequential Spectral Turning Band Simulator as an Alternative to {{Gibbs}} Sampler in Large Truncated- or Pluri- {{Gaussian}} Simulations},
  author = {Lauzon, Dany and Marcotte, Denis},
  year = {2020},
  month = nov,
  journal = {Stochastic Environmental Research and Risk Assessment},
  volume = {34},
  number = {11},
  pages = {1939--1951},
  issn = {1436-3259},
  doi = {10.1007/s00477-020-01850-9},
  urldate = {2024-02-13},
  abstract = {The Sequential Spectral Turning Bands Method (S-STBM) builds Gaussian random fields (GRF) calibrated to desired response functions. An interesting application of S-STBM concerns the simulation of GRF subject to inequality constraints. S-STBM works by choosing the phase of each cosine function of the STBM algorithm instead of perturbating nodes of the GRF many thousand times using conditional distributions as in Gibbs sampler. Each chosen phase increasingly constrains the nodes to the desired inequalities. A method based on the sequential Gaussian simulation is introduced to accelerate convergence at the end of the process. Examples shown compare S-STBM approach to Gibbs sampler. Orders of magnitude reduction in computation time is achieved with our spectral method. Furthermore, examples show that the phase selection has no significant influence on the spatial correlation. Our approach is easily generalized to pluriGaussian simulations. Compared to Gibbs sampler, S-STBM is not limited to small systems (no memory limitation) and its complexity of O(n) makes it an efficient tool to simulate large GRF subject to inequality constraints.},
  langid = {english},
  keywords = {Gibbs Sampler,Inequality constraints,PluriGaussian simulation,Sequential Gaussian simulation,Spectral simulation,Truncated Gaussian random field},
  file = {D:\03 UofA\06 Reading\_zotfile\Lauzon_Marcotte\lauzon2020sequential.pdf}
}

@article{lauzon2023joint,
  title = {Joint Hydrofacies-Hydraulic Conductivity Modeling Based on a Constructive Spectral Algorithm Constrained by Transient Head Data},
  author = {Lauzon, Dany and Marcotte, Denis},
  year = {2023},
  month = sep,
  journal = {Hydrogeology Journal},
  volume = {31},
  number = {6},
  pages = {1647--1664},
  issn = {1435-0157},
  doi = {10.1007/s10040-023-02638-1},
  urldate = {2024-02-15},
  abstract = {A constructive spectral method is presented to jointly calibrate hydrofacies and hydraulic conductivity to transient pressure heads. The method iteratively constructs Gaussian random fields to model the spatial correlation of hydraulic conductivity and hydrofacies using pluriGaussian simulation. Borehole conditioning is done quickly by replacing the slow Gibbs sampler method with an approach that is based on calibrating the underlying Gaussian fields that are subject to inequality constraints. Calibration to transient pressure heads is performed by shallow optimization of the phase vectors of the continuous spectral method. A parameterization technique makes it possible to reduce phase vector optimization from multivariate to univariate. The algorithm is tested on two-dimensional (2D) and 3D synthetic regional aquifers made of three hydrofacies. It reduced the objective function by one order of magnitude in one hundred iterations. The tests on the 2D aquifers indicated that the transient hydraulic heads alone cannot provide much information about hydrofacies. However, combining them with hydrofacies observations from boreholes results in improved hydrofacies identification compared to when only borehole data are used. Similar results were obtained in the 3D aquifer case, although the improvement in aquifer identification was less pronounced. The spectral method presented makes it possible to calibrate complex aquifers to transient heads using a limited number of calls to the flow simulator. Doing so helps to characterize sub-surface heterogeneity and assess the uncertainty and geological risks associated with groundwater flow.},
  langid = {english},
  keywords = {Data assimilation,Geostatistics,Inverse modeling,Parameter uncertainty assessment,Stochastic hydrogeology},
  file = {D:\03 UofA\06 Reading\_zotfile\Lauzon_Marcotte\lauzon2023joint.pdf}
}

@article{leuangthong2004minimum,
  title = {Minimum {{Acceptance Criteria}} for {{Geostatistical Realizations}}},
  author = {Leuangthong, Oy and McLennan, Jason A. and Deutsch, Clayton V.},
  year = {2004},
  month = sep,
  journal = {Natural Resources Research},
  volume = {13},
  number = {3},
  pages = {131--141},
  issn = {1573-8981},
  doi = {10.1023/B:NARR.0000046916.91703.bb},
  urldate = {2024-03-07},
  abstract = {Geostatistical simulation is being used increasingly for numerical modeling of natural phenomena. The development of simulation as an alternative to kriging is the result of improved characterization of heterogeneity and a model of joint uncertainty. The popularity of simulation has increased in both mining and petroleum industries. Simulation is widely available in commercial software. Many of these software packages, however, do not necessarily provide the tools for careful checking of the geostatistical realizations prior to their use in decision-making. Moreover, practitioners may not understand all that should be checked. There are some basic checks that should be performed on all geostatistical models. This paper identifies (1) the minimum criteria that should be met by all geostatistical simulation models, and (2) the checks required to verify that these minimum criteria are satisfied. All realizations should honor the input information including the geological interpretation, the data values at their locations, the data distribution, and the correlation structure, within ``acceptable'' statistical fluctuations. Moreover, the uncertainty measured by the differences between simulated realizations should be a reasonable measure of uncertainty. A number of different applications are shown to illustrate the various checks. These checks should be an integral part of any simulation modeling work flow.},
  langid = {english},
  keywords = {Model validation,simulation,verification},
  file = {D:\03 UofA\06 Reading\_zotfile\Leuangthong et al\leuangthong2004minimum.pdf}
}

@article{leuangthong2015dealinga,
  title = {Dealing with High-Grade Data in Resource Estimation},
  author = {Leuangthong, O. and Nowak, M.},
  year = {2015},
  journal = {Journal of the Southern African Institute of Mining and Metallurgy},
  volume = {115},
  number = {1},
  pages = {27--36},
  publisher = {{The Southern African Institute of Mining and Metallurgy}},
  keywords = {thesis_02},
  file = {D\:\\03 UofA\\04 Research\\02 PhD\\_zotfile\\Leuangthong_Nowak\\leuangthong2015dealing.pdf;C\:\\Users\\benha\\Zotero\\storage\\MIJ9JM8Z\\scielo.html}
}

@book{little2002statistical,
  title = {Statistical {{Analysis}} with {{Missing Data}}},
  author = {Little, Roderick J. A. and Rubin, Donald B.},
  year = {2002},
  edition = {1},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781119013563},
  urldate = {2023-08-14},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Little_Rubin\\little2002statistical.pdf;C\:\\Users\\benha\\Zotero\\storage\\T5FRK54J\\9781119013563.html}
}

@book{little2019statistical,
  title = {Statistical Analysis with Missing Data},
  author = {Little, Roderick J. A. and Rubin, Donald B.},
  year = {2019},
  series = {Wiley Series in Probability and Statistics},
  edition = {3},
  publisher = {Wiley},
  isbn = {0-470-52679-3 978-1-118-59601-2 978-1-118-59569-5 978-0-470-52679-8},
  file = {D:\03 UofA\06 Reading\_zotfile\Roderick J. A. Little\little2019statistical.pdf}
}

@article{madani2021enhanced,
  title = {Enhanced Conditional {{Co-Gibbs}} Sampling Algorithm for Data Imputation},
  author = {Madani, Nasser and Bazarbekov, Talgatbek},
  year = {2021},
  month = mar,
  journal = {Computers \& Geosciences},
  volume = {148},
  pages = {104655},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2020.104655},
  urldate = {2024-02-14},
  abstract = {The Gibbs sampler is an iterative algorithm for data imputation of a random vector at locations where values of the variable of interest are missing. In this algorithm, the simulated values converge to a Gaussian random vector distribution with zero mean and a given covariance matrix obtained by solving a simple kriging system through several iterations. In a bivariate dataset, if the principal variable for imputation depends on an auxiliary variable that is more abundant at the sample locations, this algorithm fails to produce the local and spatial cross-correlation structures. To overcome this impediment, a variant of the Gibbs sampler, the conditional Co-Gibbs sampler, has been proposed in this study, where simple kriging is replaced by three alternative cokriging paradigms: multicollocated cokriging, collocated cokriging, and homotopic cokriging. The algorithm was examined for an actual case study to statistically evaluate its performance. The results indicate that the conditional Co-Gibbs sampler with multicollocated cokriging outperformed the alternatives, including simple kriging where data imputation occurred as a consequence of ignoring the influence of the auxiliary variable, partially or totally. In addition, a computer software, provided as an open-source executable file, was used to implement the proposed algorithm for data imputation in bivariate cases.},
  keywords = {Algorithms,Data processing,Geology,Geostatistics,Spatial statistics},
  file = {D:\03 UofA\06 Reading\_zotfile\Madani_Bazarbekov\madani2021enhanced.pdf}
}

@techreport{matheron1982factorial,
  title = {Pour Une Analyse Krigeante Des Donn{\'e}es R{\'e}gionalis{\'e}es},
  author = {Matheron, Georges},
  year = {1982},
  number = {N-732},
  institution = {Ecole des Mines de Paris},
  urldate = {2021-10-28},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Matheron\matheron1982factorial.pdf}
}

@article{mclachlan2019finite,
  title = {Finite {{Mixture Models}}},
  author = {McLachlan, Geoffrey J. and Lee, Sharon X. and Rathnayake, Suren I.},
  year = {2019},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {6},
  number = {Volume 6, 2019},
  pages = {355--378},
  publisher = {Annual Reviews},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-031017-100325},
  urldate = {2024-04-03},
  abstract = {The important role of finite mixture models in the statistical analysis of data is underscored by the ever-increasing rate at which articles on mixture applications appear in the statistical and general scientific literature. The aim of this article is to provide an up-to-date account of the theory and methodological developments underlying the applications of finite mixture models. Because of their flexibility, mixture models are being increasingly exploited as a convenient, semiparametric way in which to model unknown distributional shapes. This is in addition to their obvious applications where there is group-structure in the data or where the aim is to explore the data for such structure, as in a cluster analysis. It has now been three decades since the publication of the monograph by McLachlan \&amp; Basford (1988) with an emphasis on the potential usefulness of mixture models for inference and clustering. Since then, mixture models have attracted the interest of many researchers and have found many new and interesting fields of application. Thus, the literature on mixture models has expanded enormously, and as a consequence, the bibliography here can only provide selected coverage.},
  langid = {english},
  file = {C:\Users\benha\Zotero\storage\4RTEK94L\annurev-statistics-031017-100325.html}
}

@techreport{medgold2021,
  title = {{{PRELIMINARY ECONOMIC ASSESSMENT AND NI}} 43-101 {{TECHNICAL REPORT FOR THE MEDGOLD TLAMINO PROJECT LICENCES}}, {{SERBIA}}},
  author = {{Medgold Resources Corp.}},
  year = {2021},
  keywords = {thesis_02}
}

@article{mohamed2014rdel,
  title = {{{RDEL}}: {{Restart Differential Evolution}} Algorithm with {{Local Search Mutation}} for Global Numerical Optimization},
  shorttitle = {{{RDEL}}},
  author = {Mohamed, Ali Wagdy},
  year = {2014},
  month = nov,
  journal = {Egyptian Informatics Journal},
  volume = {15},
  number = {3},
  pages = {175--188},
  issn = {1110-8665},
  doi = {10.1016/j.eij.2014.07.001},
  urldate = {2023-08-08},
  abstract = {In this paper, a novel version of Differential Evolution (DE) algorithm based on a couple of local search mutation and a restart mechanism for solving global numerical optimization problems over continuous space is presented. The proposed algorithm is named as Restart Differential Evolution algorithm with Local Search Mutation (RDEL). In RDEL, inspired by Particle Swarm Optimization (PSO), a novel local mutation rule based on the position of the best and the worst individuals among the entire population of a particular generation is introduced. The novel local mutation scheme is joined with the basic mutation rule through a linear decreasing function. The proposed local mutation scheme is proven to enhance local search tendency of the basic DE and speed up the convergence. Furthermore, a restart mechanism based on random mutation scheme and a modified Breeder Genetic Algorithm (BGA) mutation scheme is combined to avoid stagnation and/or premature convergence. Additionally, an exponent increased crossover probability rule and a uniform scaling factors of DE are introduced to promote the diversity of the population and to improve the search process, respectively. The performance of RDEL is investigated and compared with basic differential evolution, and state-of-the-art parameter adaptive differential evolution variants. It is discovered that the proposed modifications significantly improve the performance of DE in terms of quality of solution, efficiency and robustness.},
  langid = {english},
  keywords = {Differential evolution,Evolutionary computation,Global numerical optimization,Local search mutation,Restart mechanism},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Mohamed\\mohamed2014rdel.pdf;C\:\\Users\\benha\\Zotero\\storage\\4D5QJGVC\\S1110866514000279.html}
}

@article{mood1940distribution,
  title = {The {{Distribution Theory}} of {{Runs}}},
  author = {Mood, A. M.},
  year = {1940},
  journal = {The Annals of Mathematical Statistics},
  volume = {11},
  number = {4},
  eprint = {2235718},
  eprinttype = {jstor},
  pages = {367--392},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851},
  urldate = {2022-04-21},
  file = {D:\03 UofA\06 Reading\_zotfile\Mood\mood1940distribution.pdf}
}

@misc{mullner2011modern,
  title = {Modern Hierarchical, Agglomerative Clustering Algorithms},
  author = {M{\"u}llner, Daniel},
  year = {2011},
  month = sep,
  number = {arXiv:1109.2378},
  eprint = {1109.2378},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-12-11},
  abstract = {This paper presents algorithms for hierarchical, agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a "stepwise dendrogram", a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting efficiently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes.},
  archiveprefix = {arxiv},
  keywords = {62H30,Computer Science - Data Structures and Algorithms,I.5.3,Statistics - Machine Learning,thesis_06},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Müllner\\mullner2011modern.pdf;C\:\\Users\\benha\\Zotero\\storage\\RBT9X7UZ\\1109.html}
}

@techreport{ngm2020,
  title = {{{TECHNICAL REPORT ON THE CARLIN COMPLEX}}, {{EUREKA AND ELKO COUNTIES}}, {{STATE OF NEVADA}}, {{USA}}},
  author = {{Nevada Gold Mines LLC}},
  year = {2020},
  keywords = {thesis_02}
}

@inproceedings{nowak2008generalized,
  title = {Generalized Binary Search},
  booktitle = {2008 46th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}}},
  author = {Nowak, Robert},
  year = {2008},
  month = sep,
  pages = {568--574},
  doi = {10.1109/ALLERTON.2008.4797609},
  urldate = {2024-02-17},
  abstract = {This paper studies a generalization of the classic binary search problem of locating a desired value within a sorted list. The classic problem can be viewed as determining the correct one-dimensional, binary-valued threshold function from a finite class of such functions based on queries taking the form of point samples of the function. The classic problem is also equivalent to a simple binary encoding of the threshold location. This paper extends binary search to learning more general binary-valued functions. Specifically, if the set of target functions and queries satisfy certain geometrical relationships, then an algorithm, based on selecting a query that is maximally discriminating at each step, will determine the correct function in a number of steps that is logarithmic in the number of functions under consideration. Examples of classes satisfying the geometrical relationships include linear separators in multiple dimensions. Extensions to handle noise are also discussed. Possible applications include machine learning, channel coding, and sequential experimental design.},
  keywords = {Channel coding,Design for experiments,Feedback,Machine learning,Particle separators,Probability distribution,Sampling methods,Search problems,thesis_05,Uncertainty},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Nowak\\nowak2008generalized.pdf;C\:\\Users\\benha\\Zotero\\storage\\4QDUPKVN\\4797609.html}
}

@inproceedings{nowak2013suggestions,
  title = {Suggestions for Good Capping Practices from Historical Literature},
  booktitle = {Proceedings of the 23rd {{World Mining Congress}} 2013},
  author = {Nowak, M. and Leuangthong, O. and Srivastava, R. M.},
  year = {2013},
  publisher = {{Canadian Institute of Mining, Metallurgy and Petroleum Montreal}},
  keywords = {thesis_02},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Nowak et al\nowak2013suggestions.pdf}
}

@article{ortiz2002calculation,
  title = {Calculation of {{Uncertainty}} in the {{Variogram}}},
  author = {Ortiz, Juli{\'a}n and Deutsch, Clayton V.},
  year = {2002},
  month = feb,
  journal = {Mathematical Geology},
  volume = {34},
  number = {2},
  pages = {169--183},
  issn = {1573-8868},
  doi = {10.1023/A:1014412218427},
  urldate = {2024-03-26},
  abstract = {There are often limited data available in early stages of geostatistical modeling. This leads to considerable uncertainty in statistical parameters including the variogram. This article presents an approach to calculate the uncertainty in the variogram. A methodology to transfer this uncertainty through geostatistical simulation and decision making is also presented.},
  langid = {english},
  keywords = {decision making,multi-Gaussian,multipoint statistics},
  file = {D:\03 UofA\06 Reading\_zotfile\Ortiz_Deutsch\ortiz2002calculation.pdf}
}

@phdthesis{ortiz2003characterization,
  title = {Characterization of High Order Correlation for Enhanced Indicator Simulation.},
  author = {Ortiz, Juli{\'a}n},
  year = {2003},
  file = {D:\03 UofA\06 Reading\_zotfile\Ortiz\ortiz2003characterization2.pdf}
}

@techreport{osiko2020,
  title = {{{NI}} 43-101 Technical Report and Mineral Resource Estimate for the Cariboo Gold Project, British Columbia, Canada},
  author = {{Osisko Gold Royalties Ltd}},
  year = {2020},
  keywords = {thesis_02}
}

@article{pardo-iguzquiza2012varboot,
  title = {{{VARBOOT}}: {{A}} Spatial Bootstrap Program for Semivariogram Uncertainty Assessment},
  shorttitle = {{{VARBOOT}}},
  author = {{Pardo-Ig{\'u}zquiza}, Eulogio and Olea, Ricardo A.},
  year = {2012},
  month = apr,
  journal = {Computers \& Geosciences},
  volume = {41},
  pages = {188--198},
  issn = {0098-3004},
  doi = {10.1016/j.cageo.2011.09.002},
  urldate = {2024-03-26},
  abstract = {In applied geostatistics, the semivariogram is commonly estimated from experimental data, producing an empirical semivariogram for a specified number of discrete lags. In a second stage, a model defined by a few parameters is fitted to the empirical semivariogram. As the experimental data are usually few and sparsely located, there is considerable uncertainty about the calculated semivariogram values (uncertainty of the empirical semivariogram) and about the parameters of any model fitted to them (uncertainty of the estimated model parameters). In this paper, the uncertainty in the modeling of the empirical semivariogram is numerically assessed by the generalized bootstrap, which is an extension of the classic bootstrap procedure modified for spatially correlated data. A computer program is described and provided for the assessment of those uncertainties. In particular, the program provides for the empirical semivariogram: the standard errors, the bootstrap percentile confidence intervals, the complete variance--covariance matrix, standard deviation correlation matrix. A public domain, natural dataset is used to illustrate the performance of the program. A promising result is that, for any distance, the median of the bootstrap distribution for the empirical semivariogram approximates more closely the underlying semivariogram than the estimate derived from the empirical sample.},
  keywords = {Bootstrap percentile confidence interval,Correlated data,Model parameter uncertainty,Spatial covariance,Standard error},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Pardo-Igúzquiza_Olea\\pardo-iguzquiza2012varboot.pdf;C\:\\Users\\benha\\Zotero\\storage\\CKS5E4DN\\S0098300411003025.html}
}

@article{parrish1997,
  title = {Geologist's Gordian Knot: {{To}} Cut or Not to Cut},
  author = {Parrish, I.S},
  year = {1997},
  journal = {Mining Engineering},
  volume = {49},
  pages = {45--49},
  keywords = {thesis_02},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Parrish\parrish1997geologist.pdf}
}

@techreport{pasofino2020,
  title = {{{DUGBE GOLD PROJECT}}, {{LIBERIA NI}} 43-101 {{TECHNICAL REPORT}}},
  author = {{Pasofino Gold Ltd.}},
  year = {2020}
}

@phdthesis{pinto2020independent,
  title = {Independent {{Factor Simulation}} for {{Improved Multivariate Geostatistics}}},
  author = {Pinto, Felipe Cabral},
  year = {2020},
  langid = {english},
  school = {University of Alberta},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Pinto\pinto2020independent.pdf}
}

@article{piotrowski2017review,
  title = {Review of {{Differential Evolution}} Population Size},
  author = {Piotrowski, Adam P.},
  year = {2017},
  month = feb,
  journal = {Swarm and Evolutionary Computation},
  volume = {32},
  pages = {1--24},
  issn = {2210-6502},
  doi = {10.1016/j.swevo.2016.05.003},
  urldate = {2023-12-06},
  abstract = {Population size of Differential Evolution (DE) algorithms is often specified by user and remains fixed during run. During the first decade since the introduction of DE the opinion that its population size should be related to the problem dimensionality prevailed, later the approaches to DE population size setting diversified. In large number of recently introduced DE algorithms the population size is considered to be problem-independent and often fixed to 100 or 50 individuals, but alongside a number of DE variants with flexible population size have been proposed. The present paper briefly reviews the opinions regarding DE population size setting and verifies the impact of the population size on the performance of DE algorithms. Ten DE algorithms with fixed population size, each with at least five different population size settings, and four DE algorithms with flexible population size are tested on CEC2005 benchmarks and CEC2011 real-world problems. It is found that the inappropriate choice of the population size may severely hamper the performance of each DE algorithm. Although the best choice of the population size depends on the specific algorithm, number of allowed function calls and problem to be solved, some rough guidelines may be sketched. When the maximum number of function calls is set to classical values, i.e. those specified for CEC2005 and CEC2011 competitions, for low-dimensional problems (with dimensionality below 30) the population size equal to 100 individuals is suggested; population sizes smaller than 50 are rarely advised. For higher-dimensional artificial problems the population size should often depend on the problem dimensionality d and be set to 3d--5d. Unfortunately, setting proper population size for higher-dimensional real-world problems (d{$>$}40) turns out too problem and algorithm-dependent to give any general guide; 200 individuals may be a first guess, but many DE approaches would need a much different choice, ranging from 50 to 10d. However, quite clear relation between the population size and the convergence speed has been found, showing that the fewer function calls are available, the lower population sizes perform better. Based on the extensive experimental results the use of adaptive population size is highly recommended, especially for higher-dimensional and real-world problems. However, which specific algorithms with population size adaptation perform better depends on the number of function calls allowed.},
  keywords = {Adaptive control parameters,Differential Evolution,Evolutionary Algorithms,Metaheuristics,Population size},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Piotrowski\\piotrowski2017review.pdf;C\:\\Users\\benha\\Zotero\\storage\\9Z4QG6EA\\S2210650216300268.html}
}

@techreport{pretium2020,
  title = {Technical Report on the Brucejack Gold Mine, Northwest British Columbia},
  author = {{Pretium Resources Inc.}},
  year = {2020},
  keywords = {thesis_02}
}

@incollection{price2013differential,
  title = {Differential {{Evolution}}},
  booktitle = {Handbook of {{Optimization}}: {{From Classical}} to {{Modern Approach}}},
  author = {Price, Kenneth V.},
  editor = {Zelinka, Ivan and Sn{\'a}{\v s}el, V{\'a}clav and Abraham, Ajith},
  year = {2013},
  pages = {187--214},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-30504-7_8},
  urldate = {2024-03-15},
  abstract = {After an introduction that includes a discussion of the classic random walk, this paper presents a step-by-step development of the differential evolution (DE) global numerical optimization algorithm. Five fundamental DE strategies, each more complex than the last, are evaluated based on their conformance to invariance and symmetry principles, degree of control parameter dependence, computational efficiency and response to randomization. Optimal control parameter settings for the family of convex, quadratic functions are empirically derived.},
  isbn = {978-3-642-30504-7},
  langid = {english},
  keywords = {Differential Evolution,Differential Evolution Algorithm,Random Walk,Success Performance,Target Vector},
  file = {D:\03 UofA\06 Reading\_zotfile\Price\price2013differential.pdf}
}

@article{qu2018geostatistical,
  title = {Geostatistical {{Simulation}} with a {{Trend Using Gaussian Mixture Models}}},
  author = {Qu, Jianan and Deutsch, Clayton V.},
  year = {2018},
  month = jul,
  journal = {Natural Resources Research},
  volume = {27},
  number = {3},
  pages = {347--363},
  issn = {1573-8981},
  doi = {10.1007/s11053-017-9354-3},
  urldate = {2022-09-20},
  abstract = {Geostatistics applies statistics to quantitatively describe geological sites and assess the uncertainty due to incomplete sampling. Strong assumptions are required regarding the location independence of statistical parameters to construct numerical models with geostatistical tools. Most geological data exhibit large-scale deterministic trends together with short-scale variations. Such location dependence violates the common geostatistical assumption of stationarity. The trend-like deterministic features should be modeled prior to conventional geostatistical prediction and accounted for in subsequent geostatistical calculations. The challenge of using a trend in geostatistical simulation algorithms for the continuous variable is the subject of this paper. A stepwise conditional transformation with a Gaussian mixture model is considered to provide a stable and artifact-free numerical model. The complex features of the regionalized variable in the presence of a trend are removed in the forward transformation and restored in the back transformation. The Gaussian mixture model provides a seamless bin-free approach to transformation. Data from a copper deposit were used as an example. These data show an apparent trend unsuitable for conventional geostatistical algorithms. The result shows that the proposed algorithm leads to improved geostatistical models.},
  langid = {english},
  keywords = {Non-stationary regionalized variable,Sequential Gaussian simulation,Stepwise conditional transformation},
  file = {D:\03 UofA\06 Reading\_zotfile\Qu_Deutsch\qu2018geostatistical.pdf}
}

@article{renard2011conditioning,
  title = {Conditioning {{Facies Simulations}} with {{Connectivity Data}}},
  author = {Renard, Philippe and Straubhaar, Julien and Caers, Jef and Mariethoz, Gr{\'e}goire},
  year = {2011},
  month = nov,
  journal = {Mathematical Geosciences},
  volume = {43},
  number = {8},
  pages = {879--903},
  issn = {1874-8961, 1874-8953},
  doi = {10.1007/s11004-011-9363-4},
  urldate = {2022-04-28},
  abstract = {When characterizing and simulating underground reservoirs for flow simulations, one of the key characteristics that needs to be reproduced accurately is its connectivity. More precisely, field observations frequently allow the identification of specific points in space that are connected. For example, in hydrogeology, tracer tests are frequently conducted that show which springs are connected to which sink-hole. Similarly well tests often allow connectivity information in a petroleum reservoir to be provided.},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Renard et al\renard2011conditioning.pdf}
}

@article{renard2013connectivity,
  title = {Connectivity Metrics for Subsurface Flow and Transport},
  author = {Renard, Philippe and Allard, Denis},
  year = {2013},
  month = jan,
  journal = {Advances in Water Resources},
  series = {35th {{Year Anniversary Issue}}},
  volume = {51},
  pages = {168--196},
  issn = {0309-1708},
  doi = {10.1016/j.advwatres.2011.12.001},
  urldate = {2024-02-14},
  abstract = {Understanding the role of connectivity for the characterization of heterogeneous porous aquifers or reservoirs is a very active and new field of research. In that framework, connectivity metrics are becoming important tools to describe a reservoir. In this paper, we provide a review of the various metrics that were proposed so far, and we classify them in four main groups. We define first the static connectivity metrics which depend only on the connectivity structure of the parameter fields (hydraulic conductivity or geological facies). By contrast, dynamic connectivity metrics are related to physical processes such as flow or transport. The dynamic metrics depend on the problem configuration and on the specific physics that is considered. Most dynamic connectivity metrics are directly expressed as a function of an upscaled physical parameter describing the overall behavior of the media. Another important distinction is that connectivity metrics can either be global or localized. The global metrics are not related to a specific location while the localized metrics relate to one or several specific points in the field. Using these metrics to characterize a given aquifer requires the possibility to measure dynamic connectivity metrics in the field, to relate them with static connectivity metrics, and to constrain models with those information. Some tools are already available for these different steps and reviewed here, but they are not yet routinely integrated in practical applications. This is why new steps should be added in hydrogeological studies to infer the connectivity structure and to better constrain the models. These steps must include specific field methodologies, interpretation techniques, and modeling tools to provide more realistic and more reliable forecasts in a broad range of applications.},
  keywords = {Connectivity,Effective permeability,Euler number,Heterogeneous media,Percolation,Review},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Renard_Allard\\renard2013connectivity.pdf;C\:\\Users\\benha\\Zotero\\storage\\RNC93ULY\\S0309170811002223.html}
}

@article{rios2013derivativefree,
  title = {Derivative-Free Optimization: A Review of Algorithms and Comparison of Software Implementations},
  shorttitle = {Derivative-Free Optimization},
  author = {Rios, Luis Miguel and Sahinidis, Nikolaos V.},
  year = {2013},
  month = jul,
  journal = {Journal of Global Optimization},
  volume = {56},
  number = {3},
  pages = {1247--1293},
  issn = {1573-2916},
  doi = {10.1007/s10898-012-9951-y},
  urldate = {2024-03-15},
  abstract = {This paper addresses the solution of bound-constrained optimization problems using algorithms that require only the availability of objective function values but no derivative information. We refer to these algorithms as derivative-free algorithms. Fueled by a growing number of applications in science and engineering, the development of derivative-free optimization algorithms has long been studied, and it has found renewed interest in recent time. Along with many derivative-free algorithms, many software implementations have also appeared. The paper presents a review of derivative-free algorithms, followed by a systematic comparison of 22 related implementations using a test set of 502 problems. The test bed includes convex and nonconvex problems, smooth as well as nonsmooth problems. The algorithms were tested under the same conditions and ranked under several criteria, including their ability to find near-global solutions for nonconvex problems, improve a given starting point, and refine a near-optimal solution. A total of 112,448 problem instances were solved. We find that the ability of all these solvers to obtain good solutions diminishes with increasing problem size. For the problems used in this study, TOMLAB/MULTIMIN, TOMLAB/GLCCLUSTER, MCS and TOMLAB/LGO are better, on average, than other derivative-free solvers in terms of solution quality within 2,500 function evaluations. These global solvers outperform local solvers even for convex problems. Finally, TOMLAB/OQNLP, NEWUOA, and TOMLAB/MULTIMIN show superior performance in terms of refining a near-optimal solution.},
  langid = {english},
  keywords = {Derivative-free algorithms,Direct search methods,Surrogate models},
  file = {D:\03 UofA\06 Reading\_zotfile\Rios_Sahinidis\rios2013derivativefree.pdf}
}

@incollection{rojas1996backpropagation,
  title = {The {{Backpropagation Algorithm}}},
  booktitle = {Neural {{Networks}}: {{A Systematic Introduction}}},
  author = {Rojas, Ra{\'u}l},
  editor = {Rojas, Ra{\'u}l},
  year = {1996},
  pages = {149--182},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-61068-4_7},
  urldate = {2024-03-13},
  abstract = {We saw in the last chapter that multilayered networks are capable of computing a wider range of Boolean functions than networks with a single layer of computing units. However the computational effort needed for finding the correct combination of weights increases substantially when more parameters and more complicated topologies are considered. In this chapter we discuss a popular learning method capable of handling such large learning problems---the backpropagation algorithm. This numerical method was used by different research communities in different contexts, was discovered and rediscovered, until in 1985 it found its way into connectionist AI mainly through the work of the PDP group [382]. It has been one of the most studied and used algorithms for neural networks learning ever since.},
  isbn = {978-3-642-61068-4},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Rojas\rojas1996backpropagation.pdf}
}

@inproceedings{roscoe1996cutting,
  title = {Cutting Curves for Grade Estimation and Grade Control in Gold Mines},
  booktitle = {98th Annual General Meeting},
  author = {Roscoe, W.E},
  year = {1996},
  month = apr,
  publisher = {{Canadian Institute of Mining, Metallurgy and Petroleum}},
  keywords = {thesis_02},
  file = {D:\03 UofA\04 Research\02 PhD\_zotfile\Roscoe\roscoe1996cutting.pdf}
}

@book{rossi2013mineral,
  title = {Mineral Resource Estimation},
  author = {Rossi, Mario E. and Deutsch, Clayton V.},
  year = {2013},
  publisher = {Springer Science \& Business Media},
  file = {C:\Users\benha\Zotero\storage\V3SMKWS5\books.html}
}

@article{rukhin2010statistical,
  title = {A {{Statistical Test Suite}} for {{Random}} and {{Pseudorandom Number Generators}} for {{Cryptographic Applications}}},
  author = {Rukhin, Andrew and Soto, Juan and Nechvatal, James and Barker, Elaine and Leigh, Stefan and Levenson, Mark and Banks, David and Heckert, Alan and Dray, James},
  year = {2010},
  pages = {131},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Rukhin et al\rukhin2010statistical.pdf}
}

@article{sharma2020activation,
  title = {{{ACTIVATION FUNCTIONS IN NEURAL NETWORKS}}},
  author = {Sharma, Siddharth and Sharma, Simone and Athaiya, Anidhya},
  year = {2020},
  month = may,
  journal = {International Journal of Engineering Applied Sciences and Technology},
  volume = {04},
  number = {12},
  pages = {310--316},
  issn = {24552143},
  doi = {10.33564/IJEAST.2020.v04i12.054},
  urldate = {2024-03-13},
  abstract = {Artificial Neural Networks are inspired from the human brain and the network of neurons present in the brain. The information is processed and passed on from one neuron to another through neuro synaptic junctions. Similarly, in artificial neural networks there are different layers of cells arranged and connected to each other. The output/information from the inner layers of the neural network are passed on to the next layers and finally to the outermost layer which gives the output. The input to the outer layer is provided nonlinearity to inner layers' output so that it can be further processed. In an Artificial Neural Network, activation functions are very important as they help in learning and making sense of non-linear and complicated mappings between the inputs and corresponding outputs.},
  langid = {english},
  file = {D:\03 UofA\06 Reading\_zotfile\Sharma et al\sharma2020activation.pdf}
}

@article{silva2017multiple,
  title = {Multiple Imputation Framework for Data Assignment in Truncated Pluri-{{Gaussian}} Simulation},
  author = {Silva, Diogo S. F. and Deutsch, Clayton V.},
  year = {2017},
  month = nov,
  journal = {Stochastic Environmental Research and Risk Assessment},
  volume = {31},
  number = {9},
  pages = {2251--2263},
  issn = {1436-3259},
  doi = {10.1007/s00477-016-1309-4},
  urldate = {2023-08-14},
  abstract = {Truncated pluri-Gaussian simulation (TPGS) is suitable for the simulation of categorical variables that show natural ordering as the TPGS technique can consider transition probabilities. The TPGS assumes that categorical variables are the result of the truncation of underlying latent variables. In practice, only the categorical variables are observed. This translates the practical application of TPGS into a missing data problem in which all latent variables are missing. Latent variables are required at data locations in order to condition categorical realizations to observed categorical data. The imputation of missing latent variables at data locations is often achieved by either assigning constant values or spatially simulating latent variables subject to categorical observations. Realizations of latent variables can be used to condition all model realizations. Using a single realization or a constant value to condition all realizations is the same as assuming that latent variables are known at the data locations and this assumption affects uncertainty near data locations. The techniques for imputation of latent variables in TPGS framework are investigated in this article and their impact on uncertainty of simulated categorical models and possible effects on factors affecting decision making are explored. It is shown that the use of single realization of latent variables leads to underestimation of uncertainty and overestimation of measured resources while the use constant values for latent variables may lead to considerable over or underestimation of measured resources. The results highlight the importance of multiple data imputation in the context of TPGS.},
  langid = {english},
  keywords = {Geomodeling,Geostatistics,Gibbs sampler,Missing data analysis,thesis_05},
  file = {D:\03 UofA\06 Reading\_zotfile\Silva_Deutsch\silva2017multiple.pdf}
}

@misc{silva2018enhanced,
  title = {Enhanced {{Geologic Modeling}} of {{Multiple Categorical Variables}}},
  author = {Silva, Diogo},
  year = 2018,
  journal = {ERA},
  doi = {10.7939/R30G3HD9R},
  urldate = {2023-08-15},
  abstract = {Widely spaced data sets from drilling are used in the mining and petroleum industries to model subsurface resources. These data sets have...},
  howpublished = {https://era.library.ualberta.ca/items/9ab8ab60-cb8f-4d7d-9551-3c602956b0ad},
  langid = {english},
  keywords = {thesis_05},
  file = {D:\03 UofA\06 Reading\_zotfile\Silva\silva2018enhanced.pdf}
}

@article{silva2018multivariate,
  title = {Multivariate Data Imputation Using {{Gaussian}} Mixture Models},
  author = {Silva, Diogo S. F. and Deutsch, Clayton V.},
  year = {2018},
  month = oct,
  journal = {Spatial Statistics},
  volume = {27},
  pages = {74--90},
  issn = {2211-6753},
  doi = {10.1016/j.spasta.2016.11.002},
  urldate = {2023-08-14},
  abstract = {Availability of high dimensional geological data has become common in the mining and petroleum industries. Data sets are often complex and require advanced multivariate geostatistical techniques. Multivariate data transformation is a common step of such advanced workflows and its application requires equally sampled (isotopic) data at all data locations. Samples with missing variables are common in geological data sets for many reasons. The missing data must be imputed (inferred) to permit the measured data to be used to their full extent. Imputation methods for geological data should address spatial structure and multivariate complexity. The published techniques that account for these considerations make strong assumptions regarding conditional distributions and are computationally demanding in presence of many data. A Gaussian mixture model fitted to the multivariate data is proposed in this paper to provide stability in fitting multivariate data and to significantly improve computational efficiency. The proposed approach is demonstrated using a lateritic Nickel data set. The proposed improvement is shown to decrease computational time by two orders of magnitude for the example while also consistently enhancing results in several performance tests.},
  keywords = {Geostatistics,Missing data analysis,Modeling,Semi-parametric,thesis_05},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Silva_Deutsch\\silva2018multivariate.pdf;C\:\\Users\\benha\\Zotero\\storage\\NMCSMPRH\\S2211675316301300.html}
}

@techreport{tristar2021,
  title = {Mineral Resource Update for the Castelo de Sonhos Gold Project, Par{\'a} State, Brazil},
  author = {{TriStar Gold Inc.}},
  year = {2021},
  keywords = {thesis_02}
}

@book{tukey1977exploratory,
  title = {Exploratory Data Analysis},
  author = {Tukey, John W and others},
  year = {1977},
  volume = {2},
  publisher = {Reading, MA},
  keywords = {thesis_02},
  file = {D:\03 UofA\06 Reading\_zotfile\Tukey_others\tukey1977exploratory.pdf}
}

@phdthesis{vincent2021multipleindicator,
  title = {Multiple-{{Indicator Kriging}} of {{Gaussian}} and {{Non-Gaussian Data}}},
  author = {Vincent, Jeremy D.},
  year = {2021},
  file = {D\:\\03 UofA\\06 Reading\\_zotfile\\Vincent\\vincent2021multipleindicator.pdf;C\:\\Users\\benha\\Zotero\\storage\\F8QA2H3Y\\60c554b1-02e4-40c0-b8c7-233eb3f0babe.html}
}

@techreport{Wilde2007,
  type = {{{CCG}} Annual Report 9},
  title = {Wide Array Declustering for Representative Distributions ({{The Ultimate DECLUS Program}})},
  author = {Wilde, B. J},
  year = {2007},
  address = {Edmonton AB},
  institution = {University of Alberta},
  keywords = {CCG,thesis_06}
}
